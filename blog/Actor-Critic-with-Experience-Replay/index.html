<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Actor-Critic with Experience Replay | Mohamad H Danesh</title> <meta name="author" content="Mohamad H Danesh"> <meta name="description" content="A brief overview of the ACER RL algorithm is provided."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://modanesh.github.io/blog/Actor-Critic-with-Experience-Replay/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?f9552f799cecbba084ffb74abb67dbd7"></script> <script src="/assets/js/dark_mode.js?a380dd65f153c0fe7a7d70898aa6f5c6"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohamad H </span>Danesh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/100/">list 100</a> </li> <li class="nav-item "> <a class="nav-link" href="/get_in_touch/">get in touch</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Actor-Critic with Experience Replay</h1> <p class="post-meta">January 29, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="acer">ACER</h2> <p>It is an off-policy actor-critic model with experience replay, greatly increasing the sample efficiency and decreasing the data correlation. The reason for doing that is because ACER is off-policy and to control the stability of the off-policy estimator:</p> <ul> <li>It has multiple workers (as A2C);</li> <li>It uses replay buffer (as in DQN);</li> <li>It uses Retrace Q-value estimation;</li> <li>It truncates the importance weights with bias correction;</li> <li>It applies TRPO.</li> </ul> <p>Deep Q-learning methods are most sample efficient techniques. However, they have two important limitations. <strong>First</strong>, the deterministic nature of the optimal policy limits its use in adversarial domains. <strong>Second</strong>, finding the greedy action with respect to the Q function is costly for large action spaces. ACER matches SOTA of DQN with PER on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.</p> <p>In the calculation of the gradient of the policy wrt to theta (following equation), we can replace \(A\) with state-action value \(Q\), the discounted return \(R\), or the temporal difference residual, without introducing bias:</p> \[g = \mathbb{E}_{x_{0:\infty}, a_{0:\infty}} [\sum_{t \geq 0} A^{\pi} (x_t, a_t) \bigtriangledown_{\theta} log \pi_\theta (a_t, x_t)]\] <p>But since we use deep networks thus introducing additional approximation errors and biases. Comparing the expressions, we can have instead of \(A\), the discounted return will have higher variance and lower bias but the estimators using function approximation will have higher bias and lower variance. In ACER, they combine \(R\) with the current value function approximation to minimize bias while maintaining bounded variance.</p> <p>ACER is A3C’s off-policy counterpart. It uses parallel agents. It also uses a single neural net to estimate policy and value function. Gradient of policy with importance sampling:</p> \[\hat{g}^{imp} \left( \prod_{t=0}^{k} \rho_t \right) \sum_{t=0}^{k} \left( \sum_{i=0}^{k} \gamma^i r_{t+i} \right) \bigtriangledown_\theta log \pi_\theta (a_t, x_t)\] <p>It uses discounted rewards to reduce bias, but has a very high variance since it involves a product of many potentially unbounded importance weights. Truncating this product can prevent it from exploding. But this also adds bias. Then Degris suggested to use marginal value functions over the limiting distributions (The limiting distribution can be used on small, finite samples to approximate the true distribution of a random variable) to have this gradient expression:</p> \[g^{marg} = \mathbb{E}_{x_t \sim \beta , a_t \sim \mu} [\rho_t \bigtriangledown_{\theta} log \pi_\theta (a_t, x_t) Q^\pi (x_t, a_t)]\] <p>It depends on \(Q_\pi\) and not on \(Q_\mu\), consequently we must be able to estimate \(Q_\pi\). And instead of having a long product, it has marginal importance weight which lowers the variance. In ACER, they use Retrace to estimate \(Q_\pi\).</p> <p>To have off-policy samples (\(Q_\pi\)), there’s a need for importance sampling:</p> \[\Delta Q^{imp} (S_t, A_t) = \gamma^t \prod_{1 \leq \tau \leq t} \frac{\pi(A_tau, S_tau)}{\beta(A_tau, S_tau)}\delta_t\] <p>Importance sampling is a technique of estimating the expected value of \(f(x)\) where \(x\) has a data distribution \(p\). However, instead of sampling from \(p\), we calculate the result from sampling \(q\):</p> \[\mathbb{E}_p [f(x)] = \mathbb{E}_q (\frac{f(x)p(x)}{q(x)})\] <p>Retrace Q-value estimation method modifies \(\Delta Q\) to have importance weights truncated by no more than a constant \(c\):</p> \[\Delta Q^{ret} (S_t,A_t) = \gamma^t \prod_{1 \leq \tau \leq t} min(c, \frac{\pi(A_tau, S_tau)}{\beta(A_tau, S_tau)}\delta_t)\] <p>ACER uses \(Q^{ret}\) as the target to train the critic by minimizing the L2 error term: \((Q^{ret}(s,a)−Q(s,a))^2\).</p> <p>Finally, the multi-step estimator \(Q^{ret}\) has two benefits: to reduce bias in the policy gradient, and to enable faster learning of the critic, which further reduces bias.</p> <p>Truncate the importance weights with bias correction: To reduce the high variance of the policy gradient, ACER truncates the importance weights by a constant \(c\), plus a correction term.</p> <p>Furthermore, ACER adopts the idea of TRPO but with a small adjustment to make it more computationally efficient: rather than measuring the KL divergence between policies before and after one update, ACER maintains a running average of past policies and forces the updated policy to not deviate far from this average.</p> <p>Continuous case: It is not easy to integrate over \(Q\) to derive \(V\) in continuous action spaces. To overcome this issue, they propose a network architecture similar to dueling DQN which estimates both \(V_\pi\) and \(Q_\pi\) off-policy (SDNs). In addition to SDNs, they also construct (equation 14) novel target for estimating value function derived via the truncation and bias correction trick.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Mohamad H Danesh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?c76e72a42d2ffa684ad7f5449780d241"></script> <script defer src="/assets/js/common.js?df804ae9138b077a31fe21acfee94852"></script> <script defer src="/assets/js/copy_code.js?c65216eed90a75b0c06f8d6a5559bc0f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143609193-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-143609193-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>