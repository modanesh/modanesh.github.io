<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>RL Course by David Silver Notes | Mohamad H Danesh</title> <meta name="author" content="Mohamad H Danesh"> <meta name="description" content="After being excited about RL for more than a year, I should have a concise and satisfying answer to the question, 'What is reinforcement learning?' Here it is gathered briefly."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://modanesh.github.io/blog/2018/RL-Course-by-David-Silver-Notes/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?f9552f799cecbba084ffb74abb67dbd7"></script> <script src="/assets/js/dark_mode.js?a380dd65f153c0fe7a7d70898aa6f5c6"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohamad H </span>Danesh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/100/">list 100</a> </li> <li class="nav-item "> <a class="nav-link" href="/get_in_touch/">get in touch</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">RL Course by David Silver Notes</h1> <p class="post-meta">December 14, 2018</p> <p class="post-tags"> <a href="/blog/2018"> <i class="fas fa-calendar fa-sm"></i> 2018 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="lecture-1-introduction-to-reinforcement-learning">Lecture 1: Introduction to Reinforcement Learning</h3> <ul> <li> <p>Planning: rules of the game are given, perfect model inside agent’s head, plan ahead to find optimal policy(look ahead search or tree search).</p> </li> <li> <p>In RL environment is unknown, in planning environment is known.</p> </li> <li> <p>Types of RL agents:</p> <ul> <li> <p>Policy based</p> </li> <li> <p>Value function based</p> </li> <li> <p>Actor critic(combines policy and value function)</p> </li> </ul> </li> <li> <p>Agent’s model is a representation of the environment in the agent’s head.</p> </li> <li> <p>Agent is our brain, is the algorithm we come up with.</p> </li> <li> <p>To get the maximum expected reward, risk is already included.</p> </li> <li> <p>Value function: goodness/badness of states, so can be used to select between actions.</p> </li> <li> <p>Fully observability: agent sees the environment state.</p> </li> <li> <p>Data are not iid.</p> </li> <li> <p>Reward is delayed.</p> </li> <li> <p>There is only a reward signal, no supervision.</p> </li> </ul> <p>Reinforcement learning is the science of decision making.</p> <h3 id="lecture-2-markov-decision-process">Lecture 2: Markov Decision Process</h3> <ul> <li> <p>Partial ordering over policies:</p> \[\pi' \geq \pi \Longleftarrow v_\pi' (s) \geq v_\pi (s), \forall s\] </li> <li> <p>Action-Value function Q is the same as value function, but also takes action as input:</p> \[Q_\pi (s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] = \mathbb{E}_\pi [R_{t+1} + \gamma * Q_\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a]\] </li> <li> <p>Policy is a distribution over actions given states: \(\pi (a | s) = P [ A_t = a | S_t = s]\)</p> <p>Policies are stationary: \(A_t \approx \pi (\cdot | S_t), \forall t &gt; 0\)</p> </li> <li> <p>The returns are random samples, the value function is an expectation over there random samples.</p> </li> <li> <p>Value function: \(v_\pi (s) = \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi [R_{t+1} + \gamma * v_\pi (S_{t+1}) | S_t = s]\)</p> <p>It also can be expressed using matrices: \(v_\pi = R_\pi + \gamma * \rho_\pi * v\)</p> </li> <li> <p>Total discounted reward from state \(t\): \(G_t = R_{t+1} + \gamma * R_{t+2} + \gamma^2 * R_{t+3} ... + \gamma^{T-1} * R_T\)</p> </li> <li> <p>Why having discount factor?</p> <ul> <li> <p>Uncertainty in the future.</p> </li> <li> <p>Mathematically convenient.</p> </li> <li> <p>Avoids cycles.</p> </li> <li> <p>Immediate rewards worth more that delayed rewards.</p> </li> </ul> </li> <li> <p>Reward function: \(R = \mathbb{E} [R_{t+1} | S_t = s, A_t = a]\)</p> </li> <li> <p>Sample episodes from a MDP: different iterations over different states in each episode.</p> </li> <li> <p>State transition probability: \(\rho_{ss'} = P [S_{t+1} = s' | S_t = s, A_t = a]\)</p> </li> <li> <p>Showing an ad on a website is actually a bandit problem.</p> </li> <li> <p>Almost all RL problems can be modeled using MDPs.</p> </li> </ul> <h3 id="lecture-3-planning-by-dynamic-programming">Lecture 3: Planning by Dynamic Programming</h3> <ul> <li> <p>Value Iteration:</p> <ul> <li> <p>Goal: find optimal policy \(\pi\)</p> </li> <li> <p>Start with an arbitrary value function: \(v_1\) and apply Bellman backup to get to \(v_2\) and finally \(v^*\).</p> </li> <li> <p>Unlike policy iteration, there is no explicit policy.</p> </li> <li> <p>Intermediate value functions may not correspond to any policy.</p> </li> <li> <p>Will converge.</p> </li> <li> <p>Intuition: start with final reward and work backwards.</p> </li> </ul> </li> <li> <p>Policy Iteration:</p> <ul> <li> <p>Given a policy \(\pi\) (any policy works):</p> </li> <li> <p>Evaluate the policy:</p> \[V_\pi (s) = \mathbb{E} [ R_{t+1} + \gamma * R_{t+2} + ... | S_t = s ]\] </li> <li> <p>Improve the policy: \(\pi' = greedy ( V_\pi )\)</p> </li> <li> <p>Will converge.</p> </li> </ul> </li> <li> <p>Policy Evaluation:</p> <ul> <li> <p>Start with an arbitrary value function: \(v_1\) and apply Bellman backup to get to \(v_2\) and finally \(v_\pi\).</p> </li> <li> <p>Use both synchronous and asynchronous backups.</p> </li> <li> <p>Will converge.</p> </li> <li> \[V^{k+1} = R^\pi + \gamma * P^\pi * V^k\] </li> </ul> </li> <li> <p>Input: \(MDP(S, A, P, R, \gamma)\) - Output: optimal value function \(V^*\) and optimal policy \(\pi^*\)</p> </li> <li> <p>In planning full knowledge of MDP is given. We want to solve the MPD, i.e. finding a policy.</p> </li> <li> <p>MDPs satisfy both DP properties.</p> </li> <li> <p>Dynamic programming applies to problems that have:</p> <ul> <li> <p>Optimal substructure.</p> </li> <li> <p>Overlapping subproblems.</p> </li> </ul> </li> <li> <p>Dynamic Programming: method for solving complex problems. Breaking them down into subproblems. Solve subproblems and combine solutions.</p> <ul> <li> <p>Dynamic: sequential component to the problem.</p> </li> <li> <p>Programming: a mapping, e.g. a policy.</p> </li> </ul> </li> </ul> <h3 id="lecture-4-model-free-prediction">Lecture 4: Model-Free Prediction</h3> <ul> <li> <p>Bootstrapping: update involves estimated rewards.</p> </li> <li> <p>Monte-Carlo Learning:</p> <ul> <li> <p>In a nutshell, goes all the way through the trajectory and estimates the value by looking at the sample returns.</p> </li> <li> <p>Useful only for episodic(terminating) settings, and applies once an episode is complete. No bootstrapping.</p> </li> <li> <p>High variance, zero bias.</p> </li> <li> <p>Not exploit Markov property. In partially observed environments, MC is a better choice.</p> </li> </ul> </li> <li> <p>Temporal-Difference Learning:</p> <ul> <li> <p>Looks one step ahead, and then estimates the return.</p> </li> <li> <p>Uses bootstrapping to learn, so learns from incomplete episodes. Does online learning.</p> </li> <li> <p>Low variance, some bias.</p> </li> <li> <p>Exploits Markov property. Implicitly building MDP structure and solving for that MDP structure(refer to the AB example). More efficient in Markov environments.</p> </li> <li> <p>\(TD(\lambda)\): looks into \(\lambda\) steps of the future to update. Also \(\lambda\) can be defined as averaging over all n-step returns.</p> </li> </ul> </li> </ul> <h3 id="lecture-5-model-free-control">Lecture 5: Model Free Control</h3> <ul> <li> <p>On-policy Learning:</p> <ul> <li> <p>Learn about policy \(\pi\) from experience sampled from \(\pi\).</p> </li> <li> <p>\(\epsilon\)-greedy Exploration:</p> <ul> <li> <p>With probability = \(1 - \epsilon\) choose the greedy action. With probability = \(\epsilon\) choose a random action.</p> </li> <li> <p>Guarantees to have improvements.</p> </li> </ul> </li> <li> <p>Greedy in the Limit of Infinite Exploration(GLIE):</p> <ul> <li>All state-action pairs are explored infinitely many times.</li> </ul> </li> <li> <p>Policy converges to the greedy policy.</p> </li> <li> <p>SARSA:</p> <ul> <li> \[Q(S, A) \leftarrow Q(S, A) + \alpha * (R + \gamma * Q(S', A') - Q(S, A))\] </li> <li>Converges to the optimal action-value function, under some conditions.</li> </ul> </li> </ul> </li> <li> <p>Off-policy Learning:</p> <ul> <li> <p>Learn about policy \(\pi\) from experience sampled from \(\mu\).</p> </li> <li> <p>Evaluate target policy \(\pi (a | s)\) to compute \(v_\pi (s)\) or \(q_\pi (s, a)\) while following the behavior policy \(\mu (a | s)\).</p> </li> <li> <p>Can learn from observing human behaviors or other agents.</p> </li> <li> <p>Learn optimal policy while following exploratory policy.</p> </li> <li> <p>Monte-carlo learning doesn’t work in off-policy learning, because of really high variance. Over many steps, the target policy and the behavior policy never match enough to be useful.</p> </li> <li> <p>Using TD leaning, it works best. Because policies only need to be similar over one step.</p> </li> <li> <p>Q-Learning:</p> <ul> <li> <p>\(S'\) is gathered using the behavior policy.</p> </li> <li> \[Q(S, A) \leftarrow Q(S, A) + \alpha * (R + \gamma * Q(S, A') - Q(S, A))\] </li> <li>Converges to the optimal action-value function.</li> </ul> </li> </ul> </li> </ul> <h3 id="lecture-6-value-function-approximation">Lecture 6: Value Function Approximation</h3> <ul> <li> <p>Large scale RL problems because of the huge state spaces become non-tabular.</p> </li> <li> <p>So far, we had \(V(s)\) or \(Q(s, a)\). But with large scale problems, calculating them takes too much memory and time. Solution: estimate the value function with function approximation:</p> <p>\(v' (s, w) ≈ v_\pi (s)\) or \(q' (s, a, w) ≈ q_\pi (s, a)\)</p> <p>Generalize from seen states to unseen ones. Update parameter \(w\) using MC or TD learning.</p> </li> <li> <p>Incremental Methods:</p> <ul> <li> <p>Table lookup is a special case of linear value function approximation.</p> </li> <li> <p>Target value function for MC is the return \(G_t\) and for TD is the \lambda-return \(G^\lambda _t\)</p> </li> <li> <p>Gradient descent is simple and appealing.</p> </li> </ul> </li> <li> <p>Batch Methods:</p> <ul> <li> <p>GD is not sample efficient.</p> </li> <li> <p>Least squares algorithms find parameter vector \(w\) minimizing sum squared error.</p> </li> <li> <p>Example: experience replay in DQN.</p> </li> </ul> </li> </ul> <h3 id="lecture-7-policy-gradient-methods">Lecture 7: Policy Gradient Methods</h3> <ul> <li> <p>Policy gradient methods optimize the policy directly, instead of the value function.</p> </li> <li> <p>Parametrize the policy: \(\pi_\theta (s, a) = P [a | s, \theta]\)</p> </li> <li> <p>Point of using policy gradient methods is to being able to scale.</p> </li> <li> <p>Nash’s equilibrium is the game theoretic notion of optimality.</p> </li> <li> <p>Finite Difference Policy Gradient:</p> <ul> <li> <p>For each dimension in \(\theta\) parameters, perturbing \(\theta\) by small amount \(\epsilon\). (look at the formula)</p> </li> <li> <p>Uses \(n\) evaluations to compute policy gradient in \(n\) dimension.</p> </li> <li> <p>Simple, noisy, inefficient.</p> </li> <li> <p>Works for arbitrary policies.</p> </li> </ul> </li> <li> <p>Monte-Carlo Policy Gradient:</p> <ul> <li> <p>Score function is \(∇_\theta log \pi_\theta (s, a)\)</p> </li> <li> <p>In continuous action spaces, Gaussian policy should be used.</p> </li> <li> <p>REINFORCE: update parameters by stochastic gradient ascent.</p> </li> <li> <p>Has high variance.</p> </li> </ul> </li> <li> <p>Actor-Critic Policy Gradient:</p> <ul> <li> <p>Use a critic to estimate the action-value function.</p> </li> <li> <p>Critic: updates action-value function parameters \(w\).</p> </li> <li> <p>Actor: updates policy parameters \(\theta\) in direction suggested by critic.</p> </li> <li> <p>Approximating the policy gradient introduces bias.</p> </li> <li> <p>Using baseline function \(B\) to reduce variance. Value function \(V\) could be a good baseline.</p> </li> <li> <p>Using advantage function to reduce the variance: \(Q_\pi (s, a) - V_\pi (s)\)</p> </li> <li> <p>Critic estimates the advantage function.</p> </li> </ul> </li> </ul> <h3 id="lecture-8-integrating-learning-and-planning">Lecture 8: Integrating Learning and Planning</h3> <ul> <li> <p>So far, the course covered model-free RL.</p> </li> <li> <p>Last lecture: learn policy directly from experience; Previous lectures: learn value function directly from experience; This lecture: learn model directly from experience.</p> </li> <li> <p>Use <b>planning</b> to construct a value function or policy.</p> </li> <li>Model-Based RL: <ul> <li>Plan value function (and/or policy) from model.</li> <li>Advantages: <ul> <li>Can learn model by supervised learning methods.</li> <li>Car reason about model uncertainty.</li> </ul> </li> <li>Disadvantages: <ul> <li>First learn a model, then construct a value function: two sources of approximation error.</li> </ul> </li> <li>Model is a parametrized representation of the MDP, i.e. representing state transitions and rewards.</li> <li>Learning \(s, a \rightarrow r\) is a regression problem.</li> <li>Learning \(s, a \rightarrow s'\) is a density estimation problem.</li> <li>Planning the MDP = Solving the MDP, i.e. figure out what’s the best thing to do.</li> <li>Planning algorithms: value iteration, policy iteration, tree search, …</li> </ul> </li> <li>Integrated Architectures: <ul> <li>Put together the best parts of model-free and model-based RL.</li> <li>Two sources of experience: real: sampled from environment (true MDP); simulated: sampled from model (approximate MDP).</li> <li>Dyna: <ul> <li>Learn a model from real experience.</li> <li>Learn and plan value function from real and simulated experience.</li> </ul> </li> </ul> </li> <li>Simulation-Based Search: <ul> <li>Forward search: select best action by lookahead. Doesn’t explore the entire state space. Builds a search tree with current s as the root. Uses a model of MDP to look ahead. Doesn’t solve the whole MDP, just sub-MDP starting from now.</li> <li>Simulation-based search is a forward search paradigm using sample-based planning.</li> <li>Simulates episodes of experience from now with the model.</li> <li>Applies model-free RL to simulated episodes.</li> </ul> </li> </ul> <h3 id="lecture-9-exploration-and-exploitation">Lecture 9: Exploration and Exploitation</h3> <ul> <li> <p>Three methods of exploration and exploitation:</p> <ul> <li> <p>Random exploration: e.g. \(\epsilon\)-greedy</p> </li> <li> <p>Optimism in the face of uncertainty: estimate uncertainty on value, prefer to explore states/actions with highest uncertainty.</p> </li> <li> <p>Information state space: consider agent’s information as part of its state, lookahead to see how information helps reward.</p> </li> </ul> </li> <li> <p>Types of exploration:</p> <ul> <li> <p>State-action exploration: e.g. pick different action \(A\) each time in state \(S\).</p> </li> <li> <p>Parameter exploration: parameterize policy \(\pi (A | S , u)\), e.g. pick different parameters and try for a while.</p> </li> </ul> </li> <li> <p>Multi-Armed Bandit:</p> <ul> <li> <p>One-step decision making problem.</p> </li> <li> <p>No state space, no transition function.</p> </li> <li> <p>\(R (r) = P [ R = r | A = a ]\) is an unknown probability distribution over rewards.</p> </li> <li> <p>Regret is a function of gaps and the counts.</p> </li> <li> <p>\epsilon-greedy has linear total regret. To resolve this, pick a decay schedule for \(\epsilon_1, \epsilon_2\), … . However, it’s not possible to use because \(V^*\) is needed to calculate the gaps.</p> </li> <li> <p>One can transform multi-armed bandit problem into a sequential decision making problem.</p> </li> <li> <p>Define an MDP over information states.</p> </li> <li> <p>At each step, information state \(S'\) summarizes all information accumulated so far.</p> </li> <li> <p>MDP can then be solved by RL.</p> </li> </ul> </li> <li> <p>Contextual Bandits:</p> <ul> <li> <p>\(S = P [ S ]\) is an unknown distribution over states(or “contexts”).</p> </li> <li> <p>\(R (r) = P [ R = r | S = s, A = a ]\) is an unknown probability distribution over rewards.</p> </li> </ul> </li> <li> <p>MDPs:</p> <ul> <li> <p>For unknown or poorly estimated states, replace reward function with \(r_max\). Means to be very optimistic about uncertain states.</p> </li> <li> <p>Augmented MDP: includes information state so that \(S' = (S , I)\).</p> </li> </ul> </li> </ul> <h3 id="lecture-10-classic-games">Lecture 10: Classic Games</h3> <ul> <li> <p>Nash equilibrium is a joint policy for all players, so a way for others to pick actions such that every single player is playing the best response to all other players, i.e. no player would choose from Nash.</p> </li> <li> <p>Single-Agent and Self-Play Reinforcement Learning: Nash equilibrium is fixed-point of self-play RL. Experience is generated by playing games between agents. Each agent learns best response to other players. One player’s policy determines another player’s environment.</p> </li> <li> <p>Two-Player Zero-Sum Games: A two-player game has two (alternating) players: \(R_1 + R_2 = 0\).</p> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Bayesian-GAN/">Ablation Study of the Bayesian GAN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Gauss-from-Uniform/">Generating Gaussian Samples From A Uniform Distribution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/C51-QRDQN-IQN/">Distributional Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Actor-Critic-with-Experience-Replay/">Actor-Critic with Experience Replay</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Mohamad H Danesh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?c76e72a42d2ffa684ad7f5449780d241"></script> <script defer src="/assets/js/common.js?df804ae9138b077a31fe21acfee94852"></script> <script defer src="/assets/js/copy_code.js?c65216eed90a75b0c06f8d6a5559bc0f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143609193-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-143609193-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>