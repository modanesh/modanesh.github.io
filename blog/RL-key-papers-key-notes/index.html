<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reinforcement Learning Key Papers Keynotes | Mohamad H Danesh</title> <meta name="author" content="Mohamad H Danesh"> <meta name="description" content="Keynotes from teh RL Key Papers of Spinning Up by OpenAI."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://modanesh.github.io/blog/RL-key-papers-key-notes/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?f9552f799cecbba084ffb74abb67dbd7"></script> <script src="/assets/js/dark_mode.js?a380dd65f153c0fe7a7d70898aa6f5c6"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohamad H </span>Danesh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/100/">list 100</a> </li> <li class="nav-item "> <a class="nav-link" href="/get_in_touch/">get in touch</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning Key Papers Keynotes</h1> <p class="post-meta">December 1, 2019</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li> <p><a href="https://openreview.net/forum?id=B1DmUzWAW&amp;noteId=B1DmUzWAW" rel="external nofollow noopener" target="_blank">A Simple Neural Attentive Meta-Learner</a>, algorithm: SNAIL</p> <ul> <li> <p>Uses a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.</p> </li> <li> <p>Rather than training the learner on a single task (with the goal of generalizing to unseen samples from a similar data distribution) a meta-learner is trained on a distribution of similar tasks, with the goal of learning a strategy that generalizes to related but unseen tasks from a similar task distribution.</p> </li> <li> <p>Combines temporal convolutions, which enable the meta-learner to aggregate contextual information from past experience, with causal attention, which allow it to pinpoint specific pieces of information within that context.</p> </li> <li> <p>Soft attention treats the context as an unordered key-value store which it can query based on the content of each element. However, the lack of positional dependence can also be undesirable, especially in reinforcement learning, where the observations, actions, and rewards are intrinsically sequential.</p> </li> <li> <p>Despite their individual shortcomings, temporal convolutions and attention complement each other: while the former provide high-bandwidth access at the expense of finite context size, the latter provide pinpoint access over an infinitely large context.</p> </li> <li> <p>By interleaving TC layers with causal attention layers, SNAIL can have high-bandwidth access over its past experience without constraints on the amount of experience it can effectively use.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.03400" rel="external nofollow noopener" target="_blank">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>, algorithm: MAML</p> <ul> <li> <p>Proposes an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning.</p> </li> <li> <p>Aims to train models that can achieve rapid adaptation, a problem setting that is often formalized as few-shot learning.</p> </li> <li> <p>Makes no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \(\theta\), and that the loss function is smooth enough in \(\theta\) that we can use gradient-based learning techniques.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.05763" rel="external nofollow noopener" target="_blank">Learning to Reinforcement Learn</a></p> <ul> <li> <p>Emerges a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure.</p> </li> <li> <p>Here, the tasks that make up the training series are interrelated RL problems, for example, a series of bandit problems varying only in their parameterization. Rather than presenting target outputs as auxiliary inputs, the agent receives inputs indicating the action output on the previous step and, critically, the quantity of reward resulting from that action.</p> </li> <li> <p>At the start of a new episode, a new MDP task \(m \approx D\) and an initial state for this task are sampled, and the internal state of the agent (i.e., the pattern of activation over its recurrent units) is reset. The agent then executes its action-selection strategy in this environment for a certain number of discrete time-steps. At each step \(t\) an action \(a_t \in A\) is executed as a function of the whole history \(H_t = {x_0, a_0, r_0, . . . , x_{t-1}, a_{t-1}, r_{t-1}, x_t}\) of the agent interacting in the MDP \(m\) during the current episode. The network weights are trained to maximize the sum of observed rewards over all steps and episodes.</p> </li> <li> <p>After training, the agent’s policy is fixed (i.e. the weights are frozen, but the activations are changing due to input from the environment and the hidden state of the recurrent layer), and it is evaluated on a set of MDPs that are drawn either from the same distribution \(D\) or slight modifications of that distribution (to test the generalization capacity of the agent). The internal state is reset at the beginning of the evaluation of any new episode.</p> </li> <li> <p>Since the policy learned by the agent is history-dependent (as it makes uses of a recurrent network), when exposed to any new MDP environment, it is able to adapt and deploy a strategy that optimizes rewards for that task.</p> </li> <li> <p>All reinforcement learning was conducted using the Advantage Actor-Critic algorithm.</p> </li> <li> <p>reward and last action are additional inputs to the LSTM. For non-bandit environments, observation is also fed into the LSTM either as a one-hot or passed through an encoder model [3-layer encoder: two convolutional layers (first layer: 16 8x8 filters applied with stride 4, second layer: 32 4x4 filters with stride 2) followed by a fully connected layer with 256 units and then a ReLU non-linearity]. For bandit experiments, current time step is also fed in as input.</p> </li> <li> <p>Deep meta-RL involves a combination of three ingredients: (1) Use of a deep RL algorithm to train a recurrent neural network, (2) a training set that includes a series of interrelated tasks, (3) network input that includes the action selected and reward received in the previous time interval.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.02779" rel="external nofollow noopener" target="_blank">RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning</a>, algorithm: RL^2</p> <ul> <li> <p>Benefits from their prior knowledge about the world.</p> </li> <li> <p>The algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (“slow”) RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the “fast” RL algorithm on the current (previously unseen) MDP.</p> </li> <li> <p>Bayesian reinforcement learning provides a solid framework for incorporating prior knowledge into the learning process.</p> </li> <li> <p>Views the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms.</p> </li> <li> <p>Since the underlying MDP changes across trials, as long as different strategies are required for different MDPs, the agent must act differently according to its belief over which MDP it is currently in. Hence, the agent is forced to integrate all the information it has received, including past actions, rewards, and termination flags, and adapt its strategy continually. Hence, we have set up an end-to-end optimization process, where the agent is encouraged to learn a “fast” reinforcement learning algorithm.</p> </li> <li> <p>it receives the tuple \((s, a, r, d)\) as input, which is embedded using a function \(\phi(s, a, r, d)\) and provided as input to an RNN. To alleviate the difficulty of training RNNs due to vanishing and exploding gradients, they use Gated Recurrent Units (GRUs) which have been demonstrated to have good empirical performance. The output of the GRU is fed to a fully connected layer followed by a softmax function, which forms the distribution over actions.</p> </li> <li> <p>Supervised learning vs reinforcement learning: agent must not only learn to exploit existing information, but also learn to explore, a problem that is usually not a factor in supervised learning.</p> </li> <li> <p>The “fast” RL algorithm is a computation whose state is stored in the RNN activations, and the RNN’s weights are learned by a general-purpose “slow” reinforcement learning algorithm.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1705.08439" rel="external nofollow noopener" target="_blank">Thinking Fast and Slow with Deep Learning and Tree Search</a>, algorithm: ExIt</p> <ul> <li> <p>Decomposes the problem into separate planning and generalization tasks.</p> </li> <li> <p>Planning new policies is performed by tree search, while a deep neural network generalizes those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans.</p> </li> <li> <p>According to dual-process theory, human reasoning consists of two different kinds of thinking. System 1 is a fast, unconscious and automatic mode of thought, also known as intuition or heuristic process. System 2, an evolutionarily recent process unique to humans, is a slow, conscious, explicit and rule-based mode of reasoning.</p> </li> <li> <p>In deep RL algorithms such as REINFORCE and DQN, neural networks make action selections with no lookahead; this is analogous to System 1. Unlike human intuition, their training does not benefit from a ‘System 2’ to suggest strong policies. In this paper, they present Expert Iteration (EXIT), which uses a Tree Search as an analogue of System 2; this assists the training of the neural network.</p> </li> <li> <p>In Imitation Learning (IL), we attempt to solve the MDP by mimicking an expert policy \(\pi^*\) that has been provided. Experts can arise from observing humans completing a task, or, in the context of structured prediction, calculated from labelled training data. The policy we learn through this mimicry is referred to as the apprentice policy.</p> </li> <li> <p>Compared to IL techniques, Expert Iteration (EXIT) is enriched by an expert improvement step. Improving the expert player and then resolving the Imitation Learning problem allows us to exploit the fast convergence properties of Imitation Learning even in contexts where no strong player was originally known, including when learning tabula rasa.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1712.01815" rel="external nofollow noopener" target="_blank">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a>, algorithm: AlphaZero</p> <ul> <li> <p>AlphaGo Zero algorithm achieved superhuman performance in the game of Go, by representing Go knowledge using deep convolutional neural networks, trained solely by reinforcement learning from games of self-play.</p> </li> <li> <p>AlphaGo Zero estimates and optimizes the probability of winning, assuming binary win/loss outcomes. AlphaZero instead estimates and optimizes the expected outcome, taking account of draws or potentially other outcomes.</p> </li> <li> <p>The rules of Go are invariant to rotation and reflection. This fact was exploited in AlphaGo and AlphaGo Zero in two ways. First, training data was augmented by generating 8 symmetries for each position. Second, during MCTS, board positions were transformed using a randomly selected rotation or reflection before being evaluated by the neural network, so that the Monte- Carlo evaluation is averaged over different biases. The rules of chess and shogi are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does not transform the board position during MCTS.</p> </li> <li> <p>In AlphaGo Zero, self-play games were generated by the best player from all previous iterations. After each iteration of training, the performance of the new player was measured against the best player; if it won by a margin of 55% then it replaced the best player and self-play games were subsequently generated by this new player. In contrast, AlphaZero simply maintains a single neural network that is updated continually, rather than waiting for an iteration to complete. Self-play games are generated by using the latest parameters for this neural network, omitting the evaluation step and the selection of best player.</p> </li> <li> <p>AlphaGo Zero tuned the hyper-parameter of its search by Bayesian optimization. In Alp- haZero we reuse the same hyper-parameters for all games without game-specific tuning.</p> </li> <li> <p>Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialized parameters, using 5,000 first-generation TPUs to generate self-play games and 64 second-generation TPUs to train the neural networks.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1809.01999" rel="external nofollow noopener" target="_blank">Recurrent World Models Facilitate Policy Evolution</a>, algorithm: World Models</p> <ul> <li> <p>A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model’s extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments.</p> </li> <li> <p>Trains the agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment.</p> </li> <li> <p>Model M will be a large RNN that learns to predict the future given the past in an unsupervised manner. M’s internal representations of memories of past observations and actions are perceived and exploited by another NN called the controller (C) which learns through RL to perform some task without a teacher. A small and simple C limits C’s credit assignment problem to a comparatively small search space, without sacrificing the capacity and expressiveness of the large and complex M.</p> </li> <li> <p>To overcome the problem of an agent exploiting imperfections of the generated environments, they adjust a temperature parameter of M to control the amount of uncertainty of the generated environments. They train C inside of a noisier and more uncertain version of its generated environment, and demonstrate that this approach helps prevent C from taking advantage of the imperfections of M.</p> </li> <li> <p>M is only an approximate probabilistic model of the environment, it will occasionally generate trajectories that do not follow the laws governing the actual environment.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1809.05214" rel="external nofollow noopener" target="_blank">Model-Based Reinforcement Learning via Meta-Policy Optimization</a>, algorithm: MB-MPO</p> <ul> <li> <p>Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step.</p> </li> <li> <p>Learning dynamics models can be done in a sample efficient way since they are trained with standard supervised learning techniques, allowing the use of off-policy data.</p> </li> <li> <p>Accurate dynamics models can often be far more complex than good policies.</p> </li> <li> <p>Model-bias: Model-based approaches tend to rely on accurate (learned) dynamics models to solve a task. If the dynamics model is not sufficiently precise, the policy optimization is prone to overfit on the deficiencies of the model, leading to suboptimal behavior or even to catastrophic failures.</p> </li> <li> <p>Learning an ensemble of dynamics models and framing the policy optimization step as a meta-learning problem. Meta-learning, in the context of RL, aims to learn a policy that adapts fast to new tasks or environments.</p> </li> <li> <p>Using the models as learned simulators, MB-MPO learns a policy that can be quickly adapted to any of the fitted dynamics models with one gradient step.</p> </li> <li> <p>MB-MPO learns a robust policy in the regions where the models agree, and an adaptive one where the models yield substantially different predictions.</p> </li> <li> <p>Current meta-learning algorithms can be classified in three categories. One approach involves training a recurrent or memory-augmented network that ingests a training dataset and outputs the parameters of a learner model. Another set of methods feeds the dataset followed by the test data into a recurrent model that outputs the predictions for the test inputs. The last category embeds the structure of optimization problems into the meta-learning algorithm.</p> </li> <li> <p>While model-free RL does not explicitly model state transitions, model-based RL methods learn the transition distribution, also known as dynamics model, from the observed transitions.</p> </li> <li> <p>MB-MPO frames model-based RL as meta-learning a policy on a distribution of dynamic models, advocates to maximize the policy adaptation, instead of robustness, when models disagree.</p> </li> <li> <p>First, they initialize the models and the policy with different random weights. Then, they proceed to the data collection step. In the first iteration, a uniform random controller is used to collect data from the real-world, which is stored in a buffer D. At subsequent iterations, trajectories from the real-world are collected with the adapted policies \(\{\pi_{\theta_1'} , ..., \pi_{\theta_K '} \}\), and then aggregated with the trajectories from previous iterations. The models are trained with the aggregated real-environment samples.</p> </li> <li> <p>The algorithm proceeds by imagining trajectories from each the ensemble of models \(\{f_{\phi_1} , ..., f_{\phi_K} \}\) using the policy \(\pi_\theta\) . These trajectories are are used to perform the inner adaptation policy gradient step, yielding the adapted policies \(\{\pi_{\theta_1'} , ..., \pi_{\theta_K'} \}\). Finally, they generate imaginary trajectories using the adapted policies \(\pi_{\theta_k'}\) and models \(f_{\phi_k}\) , and optimize the policy towards the meta-objective.</p> </li> </ul> </li> <li> <p><a href="https://openreview.net/forum?id=SJJinbWRZ&amp;noteId=SJJinbWRZ" rel="external nofollow noopener" target="_blank">Model-Ensemble Trust-Region Policy Optimization</a>, algorithm: ME-TRPO</p> <ul> <li> <p>Uses an ensemble of models to maintain the model uncertainty and regularize the learning process to overcome instability in training which is caused by the learned policy that tends to exploit regions where insufficient data is available for the model to be learned.</p> </li> <li> <p>Shows that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time.</p> </li> <li> <p>The standard approach for model-based reinforcement learning alternates between model learning and policy optimization. In the model learning stage, samples are collected from interaction with the environment, and supervised learning is used to fit a dynamics model to the observations. In the policy optimization stage, the learned model is used to search for an improved policy.</p> </li> <li> <p>During model learning, they differentiate the neural networks by varying their weight initialization and training input sequences. Then, during policy learning, they regularize the policy updates by combining the gradients from the imagined stochastic roll-outs.</p> </li> <li> <p>Standard model-based techniques require differentiating through the model over many time steps, a procedure known as backpropagation through time (BPTT). It is well-known in the literature that BPTT can lead to exploding and vanishing gradients.</p> </li> <li> <p>Proposes to use likelihood ratio methods instead of BPTT to estimate the gradient, which only make use of the model as a simulator rather than for direct gradient computation. In particular, they use Trust Region Policy Optimization (TRPO), which imposes a trust region constraint on the policy to further stabilize learning.</p> </li> <li> <p>The reward function is known but the transition function is unknown.</p> </li> <li> <p>ME-TRPO combines three modifications to the vanilla approach. First, they fit a set of dynamics models \(\{f_{\phi_1} , . . . , f_{\phi_K}\}\) (termed a model ensemble) using the same real world data. These models are trained via standard supervised learning, and they only differ by the initial weights and the order in which mini-batches are sampled. Second, they use Trust Region Policy Optimization (TRPO) to optimize the policy over the model ensemble. Third, they use the model ensemble to monitor the policy’s performance on validation data, and stops the current iteration when the policy stops improving.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1807.01675" rel="external nofollow noopener" target="_blank">Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</a>, algorithm: STEVE</p> <ul> <li> <p>By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors.</p> </li> <li> <p>Interpolates between many different horizon lengths, favoring those whose estimates have lower uncertainty, and thus lower error. To compute the interpolated target, they replace both the model and Q-function with ensembles, approximating the uncertainty of an estimate by computing its variance under samples from the ensemble.</p> </li> <li> <p>Uses DDPG as the base learning algorithm, but their technique is generally applicable to other methods that use TD objectives.</p> </li> <li> <p>Complex environments require much smaller rollout horizon H, which limits the effectiveness of the approach.</p> </li> <li> <p>From a single rollout of \(H\) timesteps, they can compute \(H+1\) distinct candidate targets by considering rollouts of various horizon lengths: \(T^{MVE_0},T^{MVE_1},T^{MVE_2},...,T^{MVE_H}\). Standard TD learning uses \(T^{MVE_0}\) as the target, while MVE uses \(T^{MVE_H}\) as the target. They propose interpolating all of the candidate targets to produce a target which is better than any individual.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1803.00101" rel="external nofollow noopener" target="_blank">Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning</a>, algorithm: MVE</p> <ul> <li> <p>‌Model-based value expansion controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, they improve value estimation, which, in turn, reduces the sample complexity of learning.</p> </li> <li> <p>Model-based (MB) methods can quickly arrive at near-optimal control with learned models under fairly restricted dynamics classes. In settings with nonlinear dynamics, fundamental issues arise with the MB approach: complex dynamics demand high-capacity models, which in turn are prone to overfitting in precisely those low-data regimes where they are most needed.</p> </li> <li> <p>Reduces sample complexity while supporting complex non-linear dynamics by combining MB and MF learning techniques through disciplined model use for value estimation.</p> </li> <li> <p>Model-based value expansion (MVE): a hybrid algorithm that uses a dynamics model to simulate the short-term horizon H and Q-learning to estimate the long-term value beyond the simulation horizon. This improves Q-learning by providing higher-quality target values for training.</p> </li> <li> <p>MVE offers a single, simple, and adjustable notion of model trust (H), and fully utilizes the model to that extent.</p> </li> <li> <p>MVE also demonstrates that state dynamics prediction enables on-policy imagination via the TD-k trick starting from off-policy data.</p> </li> <li> <p>To deal with sparse reward signals, it is important to consider exploration with the model, not just refinement of value estimates.</p> </li> <li> <p>MVE forms TD targets by combining a short term value estimate formed by unrolling the model dynamics and a long term value estimate using the learned \(Q^\pi_\theta−\) function.</p> </li> <li> <p>Replaces the standard Q-learning target with an improved target, computed by rolling the learned model out for \(H\) steps.</p> </li> <li> <p>Relies on task-specific tuning of the rollout horizon \(H\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1708.02596" rel="external nofollow noopener" target="_blank">Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</a>, algorithm: MBMF</p> <ul> <li> <p>Medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks.</p> </li> <li> <p>Uses deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods.</p> </li> <li> <p>Although such model-based methods are drastically more sample efficient and more flexible than task-specific policies learned with model-free reinforcement learning, their asymptotic performance is usually worse than model-free learners due to model bias. To address this issue, they use their model-based algorithm to initialize a model-free learner.</p> </li> <li> <p>The learned model-based controller provides good rollouts, which enable supervised initialization of a policy that can then be fine-tuned with model-free algorithms, such as policy gradients.</p> </li> <li> <p>Section IV - A: how to learn the dynamics function which is \(f(s_t, a_t)\) and outputs the next state \(s_{t+1}\). This function can be difficult to learn when the states \(s_t\) and \(s_{t+1}\) are too similar and the action has seemingly little effect on the output; this difficulty becomes more pronounced as the time between states \(\Delta t\) becomes smaller and the state differences do not indicate the underlying dynamics well. They overcome this issue by instead learning a dynamics function that predicts the change in state st over the time step duration of \(\Delta t\). Thus, the predicted next state is as follows: \(s_{t+1} = s_t + f(s_t , a_t)\). Note that increasing this \(\Delta t\) increases the information available from each data point, and can help with not only dynamics learning but also with planning using the learned dynamics model.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.06203" rel="external nofollow noopener" target="_blank">Imagination-Augmented Agents for Deep Reinforcement Learning</a>, algorithm: I2A</p> <ul> <li> <p>Combines model-free and model-based aspects.</p> </li> <li> <p>I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks.</p> </li> <li> <p>Model-free approaches usually require large amounts of training data and the resulting policies do not readily generalize to novel tasks in the same environment, as they lack the behavioral flexibility constitutive of general intelligence.</p> </li> <li> <p>Uses approximate environment models by “learning to interpret” their imperfect predictions.</p> </li> <li> <p>Allows the agent to benefit from model-based imagination without the pitfalls of conventional model-based planning.</p> </li> <li> <p>A key issue addressed by I2As is that a learned model cannot be assumed to be perfect; it might sometimes make erroneous or nonsensical predictions. They therefore do not want to rely solely on predicted rewards (or values predicted from predicted states), as is often done in classical planning.</p> </li> <li> <p>It takes information about present and imagines possible futures, and chooses the one with the highest reward.</p> </li> <li> <p>Offloads all uncertainty estimation and model use into an implicit neural network training process, inheriting the inefficiency of model-free methods.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1806.01822" rel="external nofollow noopener" target="_blank">Relational Recurrent Neural Networks</a>, algorithm: RMC</p> <ul> <li> <p>Proposes that it is fruitful to consider memory interactions along with storage and retrieval.</p> </li> <li> <p>Hypothesizes that such a bias may allow a model to better understand how memories are related, and hence may give it a better capacity for relational reasoning over time.</p> </li> <li> <p>RMC uses multi-head dot product attention to allow memories to interact with each other.</p> </li> <li> <p>Relational reasoning is the process of understanding the ways in which entities are connected and using this understanding to accomplish some higher order goal.</p> </li> <li> <p>Applies attention between memories at a single time step, and not across all previous representations computed from all previous observations.</p> </li> <li> <p>Employs multi-head dot product attention (MHDPA), where each memory will attend over all of the other memories, and will update its content based on the attended information.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1702.08360" rel="external nofollow noopener" target="_blank">Neural Map: Structured Memory for Deep Reinforcement Learning</a>, algorithm: Neural Map</p> <ul> <li> <p>Neural map is a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with.</p> </li> <li> <p>Neural networks that utilized external memories can be distinguished along two main axis: memories with write operators and those without. Writeless external memory systems, often referred to as “Memory Networks” typically fix which memories are stored. For example, at each time step, the memory network would store the past M states seen in an environment. What is learnt by the network is therefore how to access or read from this fixed memory pool, rather than what contents to store within it.</p> </li> <li> <p>The memory network introduces two main disadvantages. The first disadvantage is that a potentially significant amount of redundant information could be stored. The second disadvantage is that a domain expert must choose what to store in the memory, e.g. for the DRL agent, the expert must set M to a value that is larger than the time horizon of the currently considered task.</p> </li> <li> <p>On the other hand, external neural memories having write operations are potentially far more efficient, since they can learn to store salient information for unbounded time steps and ignore any other useless information, without explicitly needing any a priori knowledge on what to store.</p> </li> <li> <p>Neural Map uses an adaptable write operation and so its size and computational cost does not grow with the time horizon of the environment as it does with memory networks. Also, it imposes a particular inductive bias on the write operation so that it is 1) well suited to 3D environments where navigation is a core component of successful behaviours, and 2) uses a sparse write operation that prevents frequent overwriting of memory locations that can occur with NTMs and DNCs.</p> </li> <li> <p>Combination of REINFORCE with value function baseline is commonly termed the “Actor-Critic” algorithm.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01988" rel="external nofollow noopener" target="_blank">Neural Episodic Control</a>, algorithm: NEC</p> <ul> <li> <p>Agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function.</p> </li> <li> <p>Why learning is slow:</p> <ul> <li> <p>Stochastic gradient descent optimisation requires the use of small learning rates.</p> </li> <li> <p>Environments with a sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero. This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by an unknown number.</p> </li> <li> <p>Reward signal propagation by value-bootstrapping techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment.</p> </li> </ul> </li> <li> <p>NEC is able to rapidly latch onto highly successful strategies as soon as they are experienced, instead of waiting for many steps of optimisation (e.g., stochastic gradient descent) as is the case with DQN and A3C.</p> </li> <li> <p>The semi-tabular representation is an append-only memory that binds slow-changing keys to fast updating values and uses a context-based lookup on the keys to retrieve useful values during action selection by the agent.</p> </li> <li> <p>Values retrieved from the memory can be updated much faster than the rest of the deep neural network.</p> </li> <li> <p>The architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, they elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to where the memory is wiped at the end of each episode).</p> </li> <li> <p>Differentiable Neural Dictionary(DND): For each action \(a \in A\), NEC has a simple memory module \(Ma = (Ka, Va)\), where \(Ka\) and \(Va\) are dynamically sized arrays of vectors, each containing the same number of vectors. The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs.</p> </li> <li> <p>The pixel state s is processed by a convolutional neural network to produce a key h. The key h is then used to lookup a value from the DND, yielding weights wi in the process for each element of the memory arrays. Finally, the output is a weighted sum of the values in the DND. The values in the DND, in the case of an NEC agent, are the Q values corresponding to the state that originally resulted in the corresponding key-value pair to be written to the memory. Thus this architecture produces an estimate of \(Q(s, a)\) for a single given action a.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.04460" rel="external nofollow noopener" target="_blank">Model-Free Episodic Control</a>, algorithm: MFEC</p> <ul> <li> <p>Addresses the question of how to emulate such fast learning abilities in a machine—without any domain-specific prior knowledge.</p> </li> <li> <p>QEC (s, a): Each entry contains the highest return ever obtained by taking action a from state s. It estimates the highest potential return for a given state and action, based upon the states, rewards and actions seen.</p> </li> <li> <p>Tabular RL methods suffer from two key deficiencies: firstly, for large problems they consume a large amount of memory, and secondly, they lack a way to generalise across similar states. To address the first problem, they limit the size of the table by removing the least recently updated entry once a maximum size has been reached. Such forgetting of older, less frequently accessed memories also occurs in the brain. In large scale RL problems (such as real life) novel states are common; the real world, in general, also has this property. They address the problem of what to do in novel states and how to generalise values across common experiences by taking QEC to be a non-parametric nearest-neighbours model. For states that have never been visited, QEC is approximated by averaging the value of the k nearest states.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1805.08296" rel="external nofollow noopener" target="_blank">Data-Efficient Hierarchical Reinforcement Learning</a>, algorithm: HIRO</p> <ul> <li> <p>To address efficiency, proposes to use off-policy experience for both higher- and lower-level training. This allows HIRO to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms.</p> </li> <li> <p>HIRO: a hierarchical two-layer structure, with a lower-level policy \(\mu_{lo}\) and a higher-level policy \(\mu_{hi}\).</p> </li> <li> <p>The higher-level policy operates at a coarser layer of abstraction and sets goals to the lower-level policy, which correspond directly to states that the lower-level policy attempts to reach.</p> </li> <li> <p>At step t, the higher-level policy produces a goal gt, indicating its desire for the lower-level agent to take actions that yield it an observation \(st+c\) that is close to \(st + gt\) .</p> </li> <li> <p>HRL methods to be applicable to real-world settings, they must be sample-efficient, and off-policy algorithms (often based on some variant of Q-function learning) generally exhibit substantially better sample efficiency than on-policy actor-critic or policy gradient variants.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01161" rel="external nofollow noopener" target="_blank">FeUdal Networks for Hierarchical Reinforcement Learning</a>, algorithm: Feudal Networks</p> <ul> <li> <p>Employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment.</p> </li> <li> <p>key contributions:</p> <ul> <li> <p>A consistent, end-to-end differentiable model that embodies and generalizes the principles of feudal reinforcement learning(FRL).</p> </li> <li> <p>A novel, approximate transition policy gradient update for training the Manager, which exploits the semantic meaning of the goals it produces.</p> </li> <li> <p>The use of goals that are directional rather than absolute in nature.</p> </li> <li> <p>A novel RNN design for the Manager – a dilated LSTM – which extends the longevity of the recurrent state memories and allows gradients to flow through large hops in time, enabling effective back-propagation through hundreds of steps.</p> </li> </ul> </li> <li> <p>The top level produces a meaningful and explicit goal for the bottom level to achieve. Sub-goals emerge as directions in the latent state- space and are naturally diverse.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.04695" rel="external nofollow noopener" target="_blank">Strategic Attentive Writer for Learning Macro-Actions</a>, algorithm: STRAW</p> <ul> <li> <p>Proposes a new deep recurrent neural network architecture, dubbed STRategic Attentive Writer (STRAW), that is capable of learning macro-actions in a reinforcement learning setting.</p> </li> <li> <p>Macro-actions enable both structured exploration and economic computation.</p> </li> <li> <p>STRAW maintains a multi-step action plan. STRAW periodically updates the plan based on observations and commits to the plan between the replanning decision points.</p> </li> <li> <p>One observation can generate a whole sequence of outputs if it is informative enough.</p> </li> <li> <p>Facilitates structured exploration in reinforcement learning – as the network learns meaningful action patterns it can use them to make longer exploratory steps in the state space.</p> </li> <li> <p>Since the model does not need to process observations while it is committed its action plan, it learns to allocate computation to key moments thereby freeing up resources when the plan is being followed.</p> </li> <li> <p>Macro-action is a particular, simpler instance of options, where the action sequence (or a distribution over them) is decided at the time the macro-action is initiated.</p> </li> <li> <p>STRAW learns macro-actions and a policy over them in an end-to-end fashion from only the environment’s reward signal and without resorting to explicit pseudo-rewards or hand-crafted subgoals.</p> </li> <li> <p>STRAW is a deep recurrent neural network with two modules. The first module translates environment observations into an action-plan – a state variable which represents an explicit stochastic plan of future actions. STRAW generates macro-actions by committing to the action-plan and following it without updating for a number of steps. The second module maintains commitment-plan – a state variable that determines at which step the network terminates a macro-action and updates the action-plan.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.01495" rel="external nofollow noopener" target="_blank">Hindsight Experience Replay</a>, algorithm: Hindsight Experience Replay (HER)</p> <ul> <li> <p>The necessity of cost engineering limits the applicability of RL in the real world because it requires both RL expertise and domain-specific knowledge.</p> </li> <li> <p>The approach is based on training universal policies which takes as input not only the current state, but also a goal state.</p> </li> <li> <p>The pivotal idea behind HER is to replay each episode with a different goal than the one the agent was trying to achieve.</p> </li> <li> <p>Shows that training an agent to perform multiple tasks can be easier than training it to perform only one task.</p> </li> <li> <p>HER: after experiencing some episode \(s_0, s_1, ..., s_T\) they store in the replay buffer every transition \(s_t \rightarrow s_{t + 1}\) not only with the original goal used for this episode but also with a subset of other goals.</p> </li> <li> <p>Not only does HER learn with extremely sparse rewards, in their experiments it also performs better with sparse rewards than with shaped ones. These results are indicative of the practical challenges with reward shaping, and that shaped rewards would often constitute a compromise on the metric they truly care about (such as binary success/failure).</p> </li> </ul> </li> <li> <p><a href="https://openreview.net/forum?id=rk07ZXZRb&amp;noteId=rk07ZXZRb" rel="external nofollow noopener" target="_blank">Learning an Embedding Space for Transferable Robot Skills</a></p> <ul> <li> <p>Allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks.</p> </li> <li> <p>Recent RL advances learn solutions from scratch for every task. Not only this is inefficient and constrains the difficulty of the tasks that can be solved, but also it limits the versatility and adaptivity of the systems that can be built.</p> </li> <li> <p>The main contribution of is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients.</p> </li> <li> <p>Desires an embedding space, in which solutions to different, potentially orthogonal, tasks can be represented.</p> </li> <li> <p>Aims to learn a skill embedding space, in which different embedding vectors that are “close” to each other in the embedding space correspond to distinct solutions to the same task.</p> </li> <li> <p>Given the state and action trace of an executed skill, it should be possible to identify the embedding vector that gave rise to the solution: i.e. derive a new skill by re-combining a diversified library of existing skills.</p> </li> <li> <p>For the policy to learn a diverse set of skills instead of just T separate solutions (one per task), they endow it with a task-conditional latent variable z. With this latent variable, which they also refer to as “skill embedding”, the policy is able to represent a distribution over skills for each task and to share these across tasks.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.07907" rel="external nofollow noopener" target="_blank">Mutual Alignment Transfer Learning</a>, algorithm: MATL</p> <ul> <li> <p>Harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa.</p> </li> <li> <p>Real world applications of RL present a significant challenge to the reinforcement learning paradigm as it is constrained to learn from comparatively expensive and slow task executions.</p> </li> <li> <p>As policies trained via reinforcement learning will learn to exploit the specific characteristics of a system – optimizing for mastery instead of generality – a policy can overfit to the simulation.</p> </li> <li> <p>Guides the exploration for both systems towards mutually aligned state distributions via auxiliary rewards.</p> </li> <li> <p>Employs an adversarial approach to train policies with additional rewards based on confusing a discriminator with respect to the originating system for state sequences visited by the agents. By guiding the target agent on the robot towards states that the potentially more proficient source agent visits in simulation, they can accelerate training.</p> </li> <li> <p>Also, the agent in simulation will be driven to explore better trajectories from states visited by the real-world policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1701.08734" rel="external nofollow noopener" target="_blank">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</a>, algorithm: PathNet</p> <ul> <li> <p>It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.</p> </li> <li> <p>Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropagation algorithm.</p> </li> <li> <p>Fixes the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning.</p> </li> <li> <p>Uses genetic algorithms to select a population of pathways through the neural network for replication and mutation.</p> </li> <li> <p>PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.03300" rel="external nofollow noopener" target="_blank">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a>, algorithm: IU Agent</p> <ul> <li> <p>Learns to solve many tasks simultaneously and faster than agents that target a single task at-a-time comparing with DDPG.</p> </li> <li> <p>The architecture enables the agent to attend to one task on-policy, while unintentionally learning to solve many other tasks off-policy. Due to the fact that multiple policies are being learned at once they must necessarily be learning off-policy.</p> </li> <li> <p>Consists of two neural networks. The actor neural network has multiple-heads representing different policies with shared lower-level representations. The critic network represents several state-action value functions, sharing a common representation for the observations.</p> </li> <li> <p>Refers to the task whose behavior the agent follows during training as the intentional task, and to the remaining tasks as unintentional.</p> </li> <li> <p>The experiments demonstrate that when acting according to the policy associated with one of the hardest tasks, they are able to learn all other tasks off-policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.05397" rel="external nofollow noopener" target="_blank">Reinforcement Learning with Unsupervised Auxiliary Tasks</a>, algorithm: UNREAL</p> <ul> <li> <p>Hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented.</p> </li> <li> <p>Predicts and controls features of the sensorimotor stream, by treating them as pseudo- rewards for reinforcement learning.</p> </li> <li> <p>Uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards.</p> </li> <li> <p>Both the auxiliary control and auxiliary prediction tasks share the convolutional neural network and LSTM that the base agent uses to act.</p> </li> <li> <p>The auxiliary control tasks are defined as additional pseudo-reward functions in the environment the agent is interacting with.</p> </li> <li> <p>Changes in the perceptual stream often correspond to important events in an environment. They train agents that learn a separate policy for maximally changing the pixels in each cell of an n × n non-overlapping grid placed over the input image. They refer to these auxiliary tasks as pixel control.</p> </li> <li> <p>The policy or value networks of an agent learn to extract task- relevant high-level features of the environment, they can be useful quantities for the agent to learn to control. Hence, the activation of any hidden unit of the agent’s neural network can itself be an auxiliary reward. They train agents that learn a separate policy for maximally activating each of the units in a specific hidden layer. They refer to these tasks as feature control.</p> </li> <li> <p>Reward prediction auxiliary task: predicting the onset of immediate reward given some historical context.</p> </li> <li> <p>The auxiliary tasks are trained on very recent sequences of experience that are stored and randomly sampled; these sequences may be prioritised(in this case according to immediate rewards).</p> </li> <li> <p>The auxiliary control tasks (pixel changes and simple network features) are shown to enable the A3C agent to learn to achieve the scalar reward faster in domains where the action-space is discrete.</p> </li> </ul> </li> <li> <p><a href="http://proceedings.mlr.press/v37/schaul15.pdf" rel="external nofollow noopener" target="_blank">Universal Value Function Approximators</a>, algorithm: UVFA</p> <ul> <li> <p>Introduces universal value function approximators (UVFAs) \(V (s, g; \theta)\) that generalise not just over states s but also over goals g.</p> </li> <li> <p>The goal space often contains just as much structure as the state space.</p> </li> <li> <p>By universal, they mean that the value function can generalise to any goal g in a set G of possible goals.</p> </li> <li> <p>UVFAs can exploit two kinds of structure between goals: similarity encoded a priori in the goal representations g, and the structure in the induced value functions discovered bottom-up.</p> </li> <li> <p>The complexity of UVFA learning does not depend on the number of demons but on the inherent domain complexity.</p> </li> <li> <p>Introduces a novel factorization approach that decomposes the regression into two stages. They view the data as a sparse table of values that contains one row for each observed state \(s\) and one column for each observed goal \(g\), and find a low-rank factorization of the table into state embeddings \(\phi(s)\) and goal embeddings \(\psi(g)\).</p> </li> <li> <p>Provides two algorithms for learning UVFAs directly from rewards. The first algorithm maintains a finite Horde of general value functions \(V_g(s)\), and uses these values to seed the table and hence learn a UVFA \(V (s, g; \theta)\) that generalizes to previously unseen goals. The second algorithm bootstraps directly from the value of the UVFA at successor states.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.04671" rel="external nofollow noopener" target="_blank">Progressive Neural Networks</a>, algorithm: Progressive Networks</p> <ul> <li> <p>Progressive networks: immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features.</p> </li> <li> <p>Progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task.</p> </li> <li> <p>Solves K independent tasks at the end of training.</p> </li> <li> <p>Accelerates learning via transfer when possible.</p> </li> <li> <p>Avoids catastrophic forgetting.</p> </li> <li> <p>Makes no assumptions about the relationship between tasks, which may in practice be orthogonal or even adversarial.</p> </li> <li> <p>Each column is trained to solve a particular Markov Decision Process (MDP), the k-th column thus defines a policy \(\pi(k)(a | s)\) taking as input a state s given by the environment, and generating probabilities over actions.</p> </li> <li> <p>A downside of the approach is the growth in number of parameters with the number of tasks.</p> </li> <li> <p>Studies in detail which features and at which depth transfer actually occurs. They explored two related methods: an intuitive, but slow method based on a perturbation analysis (APS), and a faster analytical method derived from the Fisher Information (AFS).</p> </li> <li> <p>APS: To evaluate the degree to which source columns contribute to the target task, they inject Gaussian noise at isolated points in the architecture (e.g. a given layer of a single column) and measure the impact of this perturbation on performance.</p> </li> <li> <p>AFS: By using the Fisher Information matrix, they get a local approximation to the perturbation sensitivity.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1807.10299" rel="external nofollow noopener" target="_blank">Variational Option Discovery Algorithms</a>, algorithm: VALOR</p> <ul> <li> <p>Highlights a tight connection between variational option discovery methods and variational autoencoders, and introduces Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection.</p> </li> <li> <p>In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories.</p> </li> <li> <p>Proposes a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent’s performance is strong enough (as measured by the decoder) on the current set of contexts.</p> </li> <li> <p>Shows that Variational Intrinsic Control (VIC) and the Diversity is All You Need (DIAYN) are specific instances of this template which decode from states instead of complete trajectories.</p> </li> <li> <p>VALOR can attain qualitatively different behavior of VIC and DIAYN because of its trajectory-centric approach, and DIAYN learns more quickly because of its denser reward signal.</p> </li> <li> <p>Learns a policy \pi where action distributions are conditioned on both the current state \(s_t\) and a context \(c\) which is sampled at the start of an episode and kept fixed throughout. The context should uniquely specify a particular mode of behavior (also called a skill). But instead of using reward functions to ground contexts to trajectories, they want the meaning of a context to be arbitrarily assigned (‘discovered’) during training.</p> </li> <li> <p>VALOR is a variational option discovery method with two key decisions about the decoder:</p> <ul> <li> <p>The decoder never sees actions. Their conception of “interesting” behaviors requires that the agent attempt to interact with the environment to achieve some change in state. If the decoder was permitted to see raw actions, the agent could signal the context directly through its actions and ignore the environment. Limiting the decoder in this way forces the agent to manipulate the environment to communicate with the decoder.</p> </li> <li> <p>Unlike in DIAYN, the decoder does not decompose as a sum of per-timestep computations.</p> </li> </ul> </li> <li> <p>VALOR has a recurrent architecture for the decoder, using a bidirectional LSTM to make sure that both the beginning and end of a trajectory are equally important.</p> </li> <li> <p>Starts training with small K (where learning is easy), and gradually increase it over time as the decoder gets stronger.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1802.06070" rel="external nofollow noopener" target="_blank">Diversity is All You Need: Learning Skills without a Reward Function</a>, algorithm: DIAYN</p> <ul> <li> <p>Learns useful skills without a reward function, by maximizing an information theoretic objective using a maximum entropy policy.</p> </li> <li> <p>Unsupervised discovery of skills can serve as an effective pre-training mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.</p> </li> <li> <p>A skill is a latent-conditioned policy that alters that state of the environment in a consistent way.</p> </li> <li> <p>Uses discriminability between skills as an objective.</p> </li> <li> <p>Learns skills that not only are distinguishable, but also are as diverse as possible.</p> </li> <li> <p>Proposes a simple method for using learned skills for hierarchical RL and find this methods solves challenging tasks.</p> </li> <li> <p>Because skills are learned without a priori knowledge of the task, the learned skills can be used for many different tasks.</p> </li> <li> <p>First, for skills to be useful, they want the skill to dictate the states that the agent visits. Different skills should visit different states, and hence be distinguishable. Second, they want to use states, not actions, to distinguish skills, because actions that do not affect the environment are not visible to an outside observer. For example, an outside observer cannot tell how much force a robotic arm applies when grasping a cup if the cup does not move. Finally, they encourage exploration and incentivize the skills to be as diverse as possible by learning skills that act as randomly as possible.</p> </li> <li> <p>Performs option discovery by optimizing a variational lower bound for an objective function designed to maximize mutual information between context and every state in a trajectory, while minimizing mutual information between actions and contexts conditioned on states, and maximizing entropy of the mixture policy over contexts.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.07507" rel="external nofollow noopener" target="_blank">Variational Intrinsic Control</a>, algorithm: VIC</p> <ul> <li> <p>Introduces two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly.</p> </li> <li> <p>Addresses the question of what intrinsic options are available to an agent in a given state?</p> </li> <li> <p>The objective of empowerment: long-term goal of the agent is to get to a state with a maximal set of available intrinsic options.</p> </li> <li> <p>The primary goal of empowerment is not to understand or predict the observations but to control the environment.</p> </li> <li> <p>Learns to represent the intrinsic control space of an agent.</p> </li> <li> <p>Data likelihood and empowerment are both information measures: likelihood measures the amount of information needed to describe data and empowerment measures the mutual information between action choices and final states.</p> </li> <li> <p>VIC is an option discovery technique based on optimizing a variational lower bound on the mutual information between the context and the final state in a trajectory, conditioned on the initial state.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1810.12894" rel="external nofollow noopener" target="_blank">Exploration by Random Network Distillation</a>, algorithm: RND</p> <ul> <li> <p>Introduces exploration bonus that is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network .</p> </li> <li> <p>Flexibly combines intrinsic and extrinsic rewards.</p> </li> <li> <p>Can be used with any policy optimization algorithm.</p> </li> <li> <p>Efficient to compute as it requires only a single forward pass of a neural network on a batch of experience.</p> </li> <li> <p>Predicts the output of a fixed randomly initialized neural network on the current observation.</p> </li> <li> <p>Introduces a modification of Proximal Policy Optimization (PPO) that uses two value heads for the two reward streams, to combine the exploration bonus with the extrinsic rewards. This allows the use of different discount rates for the different rewards, and combining episodic and non-episodic returns.</p> </li> <li> <p>In a tabular setting with a finite number of states one can define it to be a decreasing function of the visitation count \(nt(s)\) of the state s. In non-tabular cases it is not straightforward to produce counts, as most states will be visited at most once. One possible generalization of counts to non-tabular settings is pseudo-counts (Bellemare et al., 2016) which uses changes in state density estimates as an exploration bonus.</p> </li> <li> <p>RND involves two neural networks: a fixed and randomly initialized target network which sets the prediction problem, and a predictor network trained on data collected by the agent.</p> </li> <li> <p>Prediction errors can be attributed to a number of factors:</p> <ul> <li> <p>Amount of training data.</p> </li> <li> <p>Stochasticity.</p> </li> <li> <p>Model misspecification.</p> </li> <li> <p>Learning dynamics.</p> </li> </ul> </li> <li> <p>The distillation error could be seen as a quantification of uncertainty in predicting the constant zero function.</p> </li> <li> <p>In order to keep the rewards on a consistent scale they normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of the intrinsic returns.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1808.04355" rel="external nofollow noopener" target="_blank">Large-Scale Study of Curiosity-Driven Learning</a></p> <ul> <li> <p>Curiosity is a type of intrinsic reward function which uses prediction error as reward signal.</p> </li> <li> <p>Examples of intrinsic reward include “curiosity” which uses prediction error as reward signal, and “visitation counts” which discourages the agent from revisiting the same states. The idea is that these intrinsic rewards will bridge the gaps between sparse extrinsic rewards.</p> </li> <li> <p>Studies agents driven purely by intrinsic rewards.</p> </li> <li> <p>Shows that encoding observations via a random network turns out to be a simple, yet effective technique for modeling curiosity across many popular RL benchmarks. This might suggest that many popular RL video game test-beds are not as visually sophisticated as commonly thought.</p> </li> <li> <p>If the agent itself is the source of stochasticity in the environment, it can reward itself without making any actual progress.</p> </li> <li> <p>One important point is that the use of an end of episode signal, sometimes called a ‘done’, can often leak information about the true reward function. If they don’t remove the ‘done’ signal, many of the Atari games become too simple. For example, a simple strategy of giving +1 artificial reward at every time-step when the agent is alive and 0 on death is sufficient to obtain a high score in some games.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1705.05363" rel="external nofollow noopener" target="_blank">Curiosity-driven Exploration by Self-supervised Prediction</a>, algorithm: Intrinsic Curiosity Module (ICM)</p> <ul> <li> <p>Formulates curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.</p> </li> <li> <p>Motivation/curiosity have been used to explain the need to explore the environment and discover novel states.</p> </li> <li> <p>Measuring “novelty” requires a statistical model of the distribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state \(s_{t+1}\) given the current state \(s_t\) and the action \(a_t\) executed at time t.</p> </li> <li> <p>Predicts those changes in the environment that could possibly be due to the actions of their agent or affect the agent, and ignore the rest.</p> </li> <li> <p>Curiosity helps an agent explore its environment in the quest for new knowledge. Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios.</p> </li> <li> <p>Agent is composed of two subsystems: a reward generator that outputs a curiosity-driven intrinsic reward signal and a policy that outputs a sequence of actions to maximize that reward signal.</p> </li> <li> <p>Making predictions in the raw sensory space (e.g. when st corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because it is unclear if predicting pixels is even the right objective to optimize.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01260" rel="external nofollow noopener" target="_blank">EX2: Exploration with Exemplar Models for Deep Reinforcement Learning</a>, algorithm: EX2</p> <ul> <li> <p>When the reward signals are rare and sparse, function approximation methods such as deep RL struggle to acquire meaningful policies.</p> </li> <li> <p>Intuitively, novel states are easier to distinguish against other states seen during training.</p> </li> <li> <p>Estimate novelty by considering how easy it is for a discriminatively trained classifier to distinguish a given state from other states seen previously.</p> </li> <li> <p>Trains exemplar models for each state that distinguish that state from all other observed states.</p> </li> <li> <p>Given a dataset \(X = \{x_1, ...x_n\}\), an exemplar model consists of a set of \(n\) classifiers or discriminators \(\{D_{x_1} , ....D_{x_n}\}\), one for each data point. Each individual discriminator \(D_{x_i}\) is trained to distinguish a single positive data point xi, the “exemplar,” from the other points in the dataset X.</p> </li> <li> <p>In GANs, the generator plays an adversarial game with the discriminator by attempting to produce indistinguishable samples in order to fool the discriminator. However, in this work, the generator is rewarded for helping the discriminator rather than fooling it, so their algorithm plays a cooperative game instead of an adversarial one.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.04717" rel="external nofollow noopener" target="_blank">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a>, algorithm: Hash-based Counts</p> <ul> <li> <p>It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once.</p> </li> <li> <p>States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory.</p> </li> <li> <p>Important aspects of a good hash function are, first, having appropriate granularity, and second, encoding information relevant to solving the MDP.</p> </li> <li> <p>The sample complexity can grow exponentially(with state space size) in tasks with sparse rewards.</p> </li> <li> <p>Discretizes the state space with a hash function and apply a bonus based on the state-visitation count.</p> </li> <li> <p>The agent is trained with rewards \((r + r+)\), while performance is evaluated as the sum of rewards without bonuses.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01310" rel="external nofollow noopener" target="_blank">Count-Based Exploration with Neural Density Models</a>, algorithm: PixelCNN-based Pseudocounts</p> <ul> <li> <p>Considers two questions left open by CTS-based Pseudocounts work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration?</p> </li> <li> <p>Answers the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count.</p> </li> <li> <p>The mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings.</p> </li> <li> <p>Trains the density model completely online on the sequence of experienced states.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.01868" rel="external nofollow noopener" target="_blank">Unifying Count-Based Exploration and Intrinsic Motivation</a>, algorithm: CTS-based Pseudocounts</p> <ul> <li> <p>In a tabular setting, agent’s uncertainty over the environment’s reward and transition functions can be quantified using confidence intervals derived from Chernoff bounds, or inferred from a posterior over the environment parameters.</p> </li> <li> <p>Count-based exploration methods directly use visit counts to guide an agent’s behaviour towards reducing uncertainty.</p> </li> <li> <p>In spite of their pleasant theoretical guarantees, count-based methods have not played a role in the contemporary successes of reinforcement learning. The issue is that visit counts are not directly useful in large domains, where states are rarely visited more than once.</p> </li> <li> <p>Pseudo-count estimates the uncertainty of an agent’s knowledge.</p> </li> <li> <p>Derives the pseudo-count from a density model over the state space to generalize count-based exploration to non-tabular reinforcement learning.</p> </li> <li> <p>The model should be learning-positive, i.e. the probability assigned to a state x should increase with training.</p> </li> <li> <p>It should be trained on- line, using each sample exactly once.</p> </li> <li> <p>The effective model step-size should decay at a rate of \(n^{−1}\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1605.09674" rel="external nofollow noopener" target="_blank">VIME: Variational Information Maximizing Exploration</a>, algorithm: VIME</p> <ul> <li> <p>Maximizes information gain about the agent’s belief of environment dynamics.</p> </li> <li> <p>Modifies the MDP reward function.</p> </li> <li> <p>Agents are encouraged to take actions that result in states they deem surprising, i.e., states that cause large updates to the dynamics model distribution.</p> </li> <li> <p>Variational inference is used to approximate the posterior distribution of a Bayesian neural network that represents the environment dynamics.</p> </li> <li> <p>Using information gain in this learned dynamics model as intrinsic rewards allows the agent to optimize for both external reward and intrinsic surprise simultaneously.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.03864" rel="external nofollow noopener" target="_blank">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, algorithm: ES</p> <ul> <li> <p>Needs to communicate scalars, making it possible to scale to over a thousand parallel workers.</p> </li> <li> <p>It is invariant to action frequency and delayed rewards.</p> </li> <li> <p>Tolerant of extremely long horizons.</p> </li> <li> <p>Does not need temporal discounting or value function approximation.</p> </li> <li> <p>Uses of virtual batch normalization and other reparameterizations of the neural network policy greatly improve the reliability of evolution strategies.</p> </li> <li> <p>The data efficiency of evolution strategies was surprisingly good.</p> </li> <li> <p>Exhibits better exploration behaviour than policy gradient methods like TRPO.</p> </li> <li> <p>Black-box optimization methods have several highly attractive properties: indifference to the distribution of rewards (sparse or dense), no need for backpropagating gradients, and tolerance of potentially arbitrarily long time horizons.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1704.06440" rel="external nofollow noopener" target="_blank">Equivalence Between Policy Gradients and Soft Q-Learning</a></p> <ul> <li> <p>Q-learning methods are not well-understood why they work, since empirically, the Q-values they estimate are very inaccurate.</p> </li> <li> <p>“Soft” (entropy-regularized) Q-learning is exactly equivalent to a policy gradient method.</p> </li> <li> <p>In both cases, if the return following an action at is high, then that action is reinforced: in policy gradient methods, the probability \(\pi (a_t |s_t)\) is increased; whereas in Q-learning methods, the Q-value \(Q (s_t, a_t)\) is increased.</p> </li> <li> <p>Problem setting described in the paper is the bandit problem.</p> </li> </ul> </li> <li> <p><a href="http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning" rel="external nofollow noopener" target="_blank">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning</a>, algorithm: IPG</p> <ul> <li> <p>Examines approaches to merging on- and off-policy updates for deep reinforcement learning.</p> </li> <li> <p>On-policy learning: One of the simplest ways to learn a neural network policy is to collect a batch of behavior wherein the policy is used to act in the world, and then compute and apply a policy gradient update from this data. This is referred to as on-policy learning because all of the updates are made using data that was collected from the trajectory distribution induced by the current policy of the agent.</p> <ul> <li>Drawbacks: Data inefficient, because they only look at each data point once.</li> </ul> </li> <li> <p>Off-policy learning: Such methods reuse samples by storing them in a memory replay buffer and train a value function or Q-function with off-policy updates.</p> <ul> <li>Drawbacks: This improves data efficiency, but often at a cost in stability and ease of use.</li> </ul> </li> <li> <p>Mixes likelihood ratio gradient with \(\hat{Q}\), which provides unbiased but high-variance gradient estimation, and deterministic gradient through an off-policy fitted critic \(Q_w\), which provides low-variance but biased gradients.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1704.04651" rel="external nofollow noopener" target="_blank">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a>, algorithm: Reactor</p> <ul> <li> <p>First contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting.</p> </li> <li> <p>Introduces the β-leave-one-out policy gradient algorithm, which improves the trade-off between variance and bias by using action values as a baseline.</p> </li> <li> <p>Exploits the temporal locality of neighboring observations for more efficient replay prioritization.</p> </li> <li> <p>Combines the sample-efficiency of off-policy experience replay with the time-efficiency of asynchronous algorithms.</p> </li> <li> <p>The Reactor architecture represents both a policy \(\pi (a|x)\) and action-value function \(Q(x, a)\).</p> </li> <li> <p>Temporal differences are temporally correlated, with correlation decaying on average with the time-difference between two transitions.</p> </li> <li> <p>More samples are made in areas of high estimated priorities, and in the absence of weighting this would lead to overestimation of unassigned priorities.</p> </li> <li> <p>An important aspect of the architecture: an acting thread receives observations, submits actions to the environment, and stores transitions in memory, while a learning thread re-samples sequences of experiences from memory and trains on them.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.01626" rel="external nofollow noopener" target="_blank">Combining Policy Gradient and Q-learning</a>, algorithm: PGQL</p> <ul> <li> <p>Combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer.</p> </li> <li> <p>Considers model-free RL, where the state-transition function is not known or learned.</p> </li> <li> <p>In policy gradient techniques the policy is represented explicitly and they improve the policy by updating the parameters in the direction of the gradient of the performance.</p> </li> <li> <p>The actor refers to the policy and the critic to the estimate of the action-value function.</p> </li> <li> <p>Combines two updates to the policy, the regularized policy gradient update, and the Q-learning update.</p> </li> <li> <p>Mix some ratio of on- and off-policy gradients or update steps in order to update a policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.01891" rel="external nofollow noopener" target="_blank">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a>, algorithm: Trust-PCL</p> <ul> <li> <p>Under entropy regularization, the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path.</p> </li> <li> <p>By alternatively augmenting the maximum reward objective with a relative entropy regularizer, the optimal policy and values still satisfy a certain set of pathwise consistencies along any sampled trajectory.</p> </li> <li> <p>Maximizes entropy regularized expected reward while maintaining natural proximity to the previous policy.</p> </li> <li> <p>Entropy regularization helps improve exploration, while the relative entropy improves stability and allows for a faster learning rate. This combination is a key novelty.</p> </li> <li> <p>It is beneficial to learn the parameter \(\phi\) at least as fast as \(\theta\), and accordingly, given a mini-batch of episodes they perform a single gradient update on \(\theta\) and possibly multiple gradient updates on \(\phi\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1702.08892" rel="external nofollow noopener" target="_blank">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a>, algorithm: PCM</p> <ul> <li> <p>Identifies a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence.</p> </li> <li> <p>Use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning.</p> </li> <li> <p>Attempts to minimize the squared soft consistency error over a set of sub-trajectories \(E\).</p> </li> <li> <p>Given a fixed rollout parameter d, at each iteration, PCL samples a batch of on-policy trajectories and computes the corresponding parameter updates for each sub-trajectory of length d. Then PCL exploits off-policy trajectories by maintaining a replay buffer and applying additional updates based on a batch of episodes sampled from the buffer at each iteration. So, PCL is applicable to both on-policy and off-policy trajectories.</p> </li> <li> <p>Unified PCL optimizes the same objective as PCL but differs by combining the policy and value function into a single model.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1802.10031" rel="external nofollow noopener" target="_blank">The Mirage of Action-Dependent Baselines in Reinforcement Learning</a></p> <ul> <li> <p>Demonstrates that the state-action-dependent(like Q-Prop or Stein Control Variates) don’t result in variance reduction over a state-dependent baseline in commonly tested benchmark domains.</p> </li> <li> <p>Having bias leads to instability and sensitivity to hyperparameters.</p> </li> <li> <p>The variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline.</p> </li> <li> <p>“We emphasize that without the open-source code accompanying, this work would not be possible. Releasing the code has allowed us to present a new view on their work and to identify interesting implementation decisions for further study that the original authors may not have been aware of.”</p> </li> <li> <p>Shows that prior works actually introduce bias into the policy gradient due to subtle implementation decisions:</p> <ul> <li> <p>Applies an adaptive normalization to only some of the estimator terms, which introduces a bias.</p> </li> <li> <p>Poorly fit value function.</p> </li> <li> <p>Fitting the baseline to the current batch of data and then using the updated baseline to form the estimator results in a biased gradient.</p> </li> </ul> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1710.11198" rel="external nofollow noopener" target="_blank">Action-depedent Control Variates for Policy Optimization via Stein’s Identity</a>, algorithm: Stein Control Variates</p> <ul> <li> <p>The idea of the control variate method is to subtract a Monte Carlo gradient estimator by a baseline function that analytically has zero expectation.</p> </li> <li> <p>Constructs a class of Stein control variate that allows to use arbitrary baseline functions that depend on both actions and states.</p> </li> <li> <p>Does not change the expectation but can decrease the variance significantly when it is chosen properly to cancel out the variance of Q.</p> </li> <li> <p>The gradient in is taken w.r.t. the action a, no w.r.t the parameter \theta.</p> </li> <li> <p>Connect \(\bigtriangledown_a log \pi(a|s)\) to \(\bigtriangledown_\theta log \pi(a|s)\) in order to apply Stein’s identity as a control variate for policy gradient. This is possible when the policy is reparameterizable in that \(a \approx \pi_\theta(a|s)\) can be viewed as generated by \(a = f_\theta(s,\xi)\) where \(\xi\) is a random noise drawn from some distribution independently of \(\theta\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.02247" rel="external nofollow noopener" target="_blank">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a>, algorithm: Q-Prop</p> <ul> <li> <p>Combines the stability of policy gradients with the efficiency of off-policy RL.</p> </li> <li> <p>Uses a Taylor expansion of the off-policy critic as a control variate.</p> </li> <li> <p>Reduces the variance of gradient estimator without adding bias.</p> </li> <li> <p>It can be easily incorporated into any policy gradient algorithm.</p> </li> <li> <p>A common choice is to estimate the value function of the state \(V_\theta(s_t)\) to use as the baseline, which provides an estimate of advantage function \(A_\theta(s_t , a_t)\).</p> </li> <li> <p>Constructs a new estimator that in practice exhibits improved sample efficiency through the inclusion of off-policy samples while preserving the stability of on-policy Monte Carlo policy gradient.</p> </li> <li> <p>A more general baseline function that linearly depends on the actions.</p> </li> <li> <p>An off-policy Q critic is trained but is used as a control variate to reduce on-policy gradient variance.</p> </li> <li> <p>Uses only on-policy samples for estimating the policy gradient.</p> </li> </ul> </li> <li> <p><a href="https://openreview.net/forum?id=ByG_3s09KX" rel="external nofollow noopener" target="_blank">Dopamine: A Research Framework for Deep Reinforcement Learning</a></p> <ul> <li> <p>A new research framework for deep RL that aims to support some of the RL goals diversities.</p> </li> <li> <p>Open-source, TensorFlow-based, and provides compact yet reliable implementations of some state-of-the-art deep RL agents.</p> </li> <li> <p>DQN is architecture research.</p> </li> <li> <p>Double DQN, distributional methods, prioritized experience replay are algorithmic research.</p> </li> <li> <p>Rainbow is comprehensive study.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1806.06923" rel="external nofollow noopener" target="_blank">Implicit Quantile Networks for Distributional Reinforcement Learning</a>, algorithm: IQN</p> <ul> <li> <p>Reparameterizes a distribution over the sample space, causing to yield an implicitly defined return distribution and give rise to a large class of risk-sensitive policies.</p> </li> <li> <p>Learns the full quantile function, a continuous map from probabilities to returns.</p> </li> <li> <p>The approximation error for the distribution is controlled by the size of the network itself, and the amount of training.</p> </li> <li> <p>Provides improved data efficiency with increasing number of samples per training update.</p> </li> <li> <p>Expands the class of policies to more fully take advantage of the learned distribution.</p> </li> <li> <p>IQN is a type of <a href="http://proceedings.mlr.press/v37/schaul15.pdf" rel="external nofollow noopener" target="_blank">universal value function approximator (UVFA)</a>.</p> </li> <li> <p>Samples TD errors decorrelated and the estimated action-values go from being the true mean of a mixture of n Diracs to a sample mean of the implicit distribution.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1710.10044" rel="external nofollow noopener" target="_blank">Distributional Reinforcement Learning with Quantile Regression</a>, algorithm: QR-DQN</p> <ul> <li> <p>Sampling probabilistically, randomness in the observed long-term return will be induced.</p> </li> <li> <p>The distribution over returns is modeled explicitly instead of only estimating the mean, which means learning the value distribution instead of the value function.</p> </li> <li> <p>Assigns fixed, uniform probabilities to N adjustable locations.</p> </li> <li> <p>Uses quantile regression to stochastically adjust the distributions’ locations so as to minimize the Wasserstein distance to a target distribution(adapts return quantiles to minimize the Wasserstein distance between the Bellman updated and current return distributions).</p> </li> <li> <p>The distribution over returns plays the central role and replaces the value function.</p> </li> <li> <p>Transposes parametrization of C51 approach by considering fixed probabilities but variable locations.</p> </li> <li> <p>Distributional Reinforcement Learning with Quantile Regression</p> </li> <li> <p>Uses a similar neural network architecture as DQN, changing the output layer to be of size \(\|A\| × N\), where \(N\) is a hyper-parameter giving the number of quantile targets.</p> </li> <li> <p>Replaces the Huber loss used by DQN with a quantile Huber loss.</p> </li> <li> <p>Replaces RMSProp with ADAM.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.06887" rel="external nofollow noopener" target="_blank">A Distributional Perspective on Reinforcement Learning</a>, algorithm: C51</p> <ul> <li> <p>Studies the random return Z whose expectation is the value Q.</p> </li> <li> <p>The distributional Bellman equation states that the distribution of Z is characterized by the interaction of three random variables: the reward R, the next state-action (X′, A′), and its random return Z(X′,A′).</p> </li> <li> <p>Instead of trying to minimize the error between the expected values, it tries to minimize a distributional error, which is a distance between full distributions.</p> </li> <li> <p>Proves that the distributional Bellman operator is a contraction in a maximal form of the Wasserstein metric between probability distributions.</p> </li> <li> <p>The Wasserstein metric, viewed as a loss, cannot generally be minimized using stochastic gradient methods.</p> </li> <li> <p>Approximates the distribution at each state by attaching variable(parametrized) probabilities to fixed locations.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1802.09477" rel="external nofollow noopener" target="_blank">Addressing Function Approximation Error in Actor-Critic Methods</a>, algorithm: TD3</p> <ul> <li> <p>Tackles the problem of overestimation bias and the accumulation of error in temporal difference method, by using double Q-learning. Double DQN doesn’t work.</p> </li> <li> <p>Target networks are critical for variance reduction by reducing the accumulation of errors.</p> </li> <li> <p>Delaying policy updates until the value estimate has converged, to overcome the the coupling of value and policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1509.02971" rel="external nofollow noopener" target="_blank">Continuous Control With Deep Reinforcement Learning</a>, algorithm: DDPG</p> <ul> <li> <p>Off-policy actor-critic, model-free, deterministic policy gradient on continuous action space.</p> </li> <li> <p>Actor maps states to actions, instead of outputting the probability distribution across a discrete action space.</p> </li> <li> <p>Next-state Q values are calculated with the target value network and target policy network.</p> </li> <li> <p>For discrete action spaces, exploration is selecting a random action(e.g. epsilon-greedy) by a given probability. For continuous action spaces, exploration is adding some noise to the action itself(using Ornstein-Uhlenbeck process).</p> </li> <li> <p>Soft target update: solve the Q update divergence. The target values are constrained to change slowly, improving the stability of learning.</p> </li> <li> <p>Having both a target \(\mu′\) and \(Q′\) was required to have stable targets \(y_i\) in order to consistently train the critic without divergence, which slows learning. However, in practice this was greatly outweighed by the stability of learning.</p> </li> </ul> </li> <li> <p><a href="http://proceedings.mlr.press/v32/silver14.pdf" rel="external nofollow noopener" target="_blank">Deterministic Policy Gradient Algorithms</a>, algorithm: DPG</p> <ul> <li> <p>Deterministic policy gradient can be estimated more efficiently than the usual stochastic policy gradient.</p> </li> <li> <p>Uses an off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory behaviour policy to address the exploration concerns.</p> </li> <li> <p>Proves that deterministic policy gradient exists and has a simple model-free form.</p> </li> <li> <p>Stochastic vs Deterministic policy gradient:</p> <ul> <li> <p>stochastic case: policy gradient integrates over both state and action spaces.</p> </li> <li> <p>deterministic case: policy gradient integrates over the state space.</p> </li> </ul> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1801.01290" rel="external nofollow noopener" target="_blank">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, algorithm: SAC</p> <ul> <li> <p>Entropy is a quantity which basically determines how random a random variable should be.</p> </li> <li> <p>Includes entropy regularization in the RL problem: policy function, value function, Q function.</p> </li> <li> <p>Maximum entropy RL alters the RL objective, though the original objective can be recovered using a temperature parameter.</p> </li> <li> <p>The maximum entropy formulation provides a substantial improvement in exploration and robustness.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.01224" rel="external nofollow noopener" target="_blank">Sample Efficient Actor-Critic with Experience Replay</a>, algorithm: ACER</p> <ul> <li> <p>Introduces truncated importance sampling with bias correction, stochastic dueling network architectures, and efficient trust region policy optimization.</p> </li> <li> <p>Matches the state-of-the-art performance of deep Q-networks with prioritized replay on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.</p> </li> <li> <p>ACER may be understood as the off-policy counterpart of the A3C method.</p> </li> <li> <p>Retrace and off- policy correction, SDNs, and trust region are critical: removing any one of them leads to a clear deterioration of the performance.</p> </li> <li> <p>Mix some ratio of on- and off-policy gradients or update steps in order to update a policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1708.05144" rel="external nofollow noopener" target="_blank">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a>, algorithm: ACKTR</p> <ul> <li> <p>Optimizes both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region.</p> </li> <li> <p>K-FAC: approximates the curvature by Kronecker factorization, reducing the computation complexity of the parameter updates.</p> </li> <li> <p>Learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs.</p> </li> <li> <p>ACKTR substantially improves both sample efficiency and the final performance of the agent in the Atari environments and the MuJoCo tasks compared to the state-of-the-art on-policy actor-critic method A2C and the famous trust region optimizer TRPO.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.02286" rel="external nofollow noopener" target="_blank">Emergence of Locomotion Behaviours in Rich Environments</a>, algorithm: PPO-Penalty</p> <ul> <li> <p>Investigates learning complex behavior in rich environments.</p> </li> <li> <p>Locomotion: behaviors that are known for their sensitivity to the choice of reward.</p> </li> <li> <p>Environments include a wide range of obstacles with varying levels of difficulty causing the implicit curriculum learning to the agent.</p> </li> <li> <p>Uses TRPO, PPO and parallelism(like A3C).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.06347" rel="external nofollow noopener" target="_blank">Proximal Policy Optimization Algorithms</a>, algorithm: PPO-Clip, PPO-Penalty</p> <ul> <li> <p>Compares to TRPO, simpler to implement, more general, and better sample complexity.</p> </li> <li> <p>Algorithm:</p> <ul> <li> <p>Each iteration, each of N parallel actors collect T timesteps of data.</p> </li> <li> <p>Then they construct the surrogate loss on these NT timesteps of data.</p> </li> <li> <p>Optimize it with minibatch SGD for K epochs.</p> </li> </ul> </li> <li> <p>PPO is a robust learning algorithm that requires little hyper-parameter tuning.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1506.02438" rel="external nofollow noopener" target="_blank">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>, algorithm: GAE</p> <ul> <li> <p>Uses value functions to estimate how the policy should be improved.</p> </li> <li> <p>Reduces variance while maintaining a tolerable level of bias, called Generalized Advantage Estimator(GAE).</p> </li> <li> <p>Trains value functions with trust region optimization.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1502.05477" rel="external nofollow noopener" target="_blank">Trust Region Policy Optimization</a>, algorithm: TRPO</p> <ul> <li> <p>Uses the expected advantage value of function A to improve the current policy.</p> </li> <li> <p>Guarantees to increase the policy performance, or leave it constant when the expected advantage is zero everywhere.</p> </li> <li> <p>Applies lower bounds on the improvements of the policy to address the issue on how big of a step to take.</p> </li> <li> <p>Uses a hard constraint rather than a penalty because it is hard to choose a single value of à that performs well across di erent problems.</p> </li> <li> <p>A major drawback is that such methods are not able to exploit off-policy data and thus require a large amount of on-policy interaction with the environment, making them impractical for solving challenging real-world problems.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1602.01783" rel="external nofollow noopener" target="_blank">Asynchronous Methods for Deep Reinforcement Learning</a>, algorithm: A3C</p> <ul> <li> <p>Uses asynchronous gradient descent for optimization of deep neural network controllers: run many agents on many instances of the environment.</p> </li> <li> <p>Experience replay drawbacks: uses more memory and computation per real interaction, and requires off-policy learning algorithms.</p> </li> <li> <p>Parallelism decorrelates agents data.</p> </li> <li> <p>A3C works well on 2D and 3D games, discrete and continuous action spaces, and can train feedforward or recurrent agents.</p> </li> <li> <p>Has threads over one CPU to do the learning.</p> </li> <li> <p>Multiple learners increase the exploration probability.</p> </li> <li> <p>A3C trains agents that have both a policy (actor) distribution \pi and a value (critic) estimate V \pi .</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1710.02298" rel="external nofollow noopener" target="_blank">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>, algorithm: Rainbow DQN</p> <ul> <li> <p>Combines 6 extensions to the DQN algorithm: <a href="https://arxiv.org/abs/1509.06461" rel="external nofollow noopener" target="_blank">double DQN</a>, <a href="https://arxiv.org/abs/1511.05952" rel="external nofollow noopener" target="_blank">prioritize experience replay</a>, <a href="https://arxiv.org/abs/1511.06581" rel="external nofollow noopener" target="_blank">dueling network architecture</a>, <a href="https://arxiv.org/abs/1901.07510" rel="external nofollow noopener" target="_blank">multi-step bootstrap targets</a>, <a href="https://arxiv.org/abs/1707.06887" rel="external nofollow noopener" target="_blank">distributional Q-learning</a>, and <a href="https://arxiv.org/abs/1706.10295" rel="external nofollow noopener" target="_blank">noisy DQN</a>.</p> </li> <li> <p>SOTA, both data efficiency wise and performance wise.</p> </li> <li> <p>Does an ablation study over the 6 extensions.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1511.05952" rel="external nofollow noopener" target="_blank">Prioritized Experience Replay</a>, algorithm: PER</p> <ul> <li> <p>Agents can remember and reuse experiences from the past.</p> </li> <li> <p>Increases the replay probability of experience tuples that have a high expected learning progress.</p> </li> <li> <p>Greedy TD-error prioritization: the amount the RL agent can learn from a transition in its current state.</p> </li> <li> <p>Greedy TD-error prioritization problems: only update the transitions that are replayed, sensitive to noise spikes, and focuses on a small subset of experiences.</p> </li> <li> <p>To overcome problems, use a stochastic sampling method to interpolate greedy prioritization and uniform random sampling.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1509.06461" rel="external nofollow noopener" target="_blank">Deep Reinforcement Learning with Double Q-learning</a>, algorithm: Double DQN</p> <ul> <li> <p>Reduces the observed overoptimism of the DQN: decomposing the max operation in the target into action selection and action evaluation.</p> </li> <li> <p>Good side of being optimistic: helps exploration. Bad side: leads to a poorer policy.</p> </li> <li> <p>The weights of the second network are replace with the weights of the target network for the evaluation of the current greedy policy.</p> </li> <li> <p>Evaluates the greedy policy according to the online network, but using the target network to estimate its value.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1511.06581" rel="external nofollow noopener" target="_blank">Dueling Network Architectures for Deep Reinforcement Learning</a>, algorithm: Dueling DQN</p> <ul> <li> <p>Uses two separate estimators: one for state value function(V), one for state-dependent action advantage function(A).</p> </li> <li> <p>Two streams representing V and A function, but using same convolutional neural network.</p> </li> <li> <p>Two streams are combined using an aggregation layer.</p> </li> <li> <p>Intuition: learning valuable states, w/o having to learn the effect of each action for each state.</p> </li> <li> <p>Clips gradients to have their norm less than or equal to 10.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1507.06527" rel="external nofollow noopener" target="_blank">Deep Recurrent Q-Learning for Partially Observable MDPs</a>, algorithm: Deep Recurrent Q-Learning</p> <ul> <li> <p>Adds RNNs to the DQNs.</p> </li> <li> <p>Replaces the first post-convolutional fully-connected layer with an LSTM.</p> </li> <li> <p>DQN agents are unable to perform well on games that require the agent to remember events from far past(k frame skipping).</p> </li> <li> <p>Uses only one single game state.</p> </li> <li> <p>Is able to handle partial observability.</p> </li> <li> <p>Partial observability: with probability p = 0.5, the screen is either fully obscured or fully revealed.</p> </li> <li> <p>Uses “Bootstrapped Sequential Updates” and “Bootstrapped Random Updates” to update RNN weights.</p> </li> </ul> </li> <li> <p><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="external nofollow noopener" target="_blank">Playing Atari with Deep Reinforcement Learning</a>, algorithm: DQN</p> <ul> <li> <p>Uses convolutional neural network to extract features directly from the input images, which are game states.</p> </li> <li> <p>No adjustments of the hyperparameters for each game.</p> </li> <li> <p>Model-free: no explicit construction of environment.</p> </li> <li> <p>Off-policy: learning policy and behavior policy are different, same as Q-learning.</p> </li> <li> <p>Uses experience replay: better data efficiency, decorrelate data, avoiding oscillations or divergence in parameters.</p> </li> <li> <p>Scales scores of different games to -1, 0, 1.</p> </li> <li> <p>Uses frame skipping: agent sees and selects actions of every k frame.</p> </li> <li> <p>Freezes the parameters of of the target network for a fixed time while updating the online network by gradient descent.</p> </li> </ul> </li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Mohamad H Danesh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?c76e72a42d2ffa684ad7f5449780d241"></script> <script defer src="/assets/js/common.js?df804ae9138b077a31fe21acfee94852"></script> <script defer src="/assets/js/copy_code.js?c65216eed90a75b0c06f8d6a5559bc0f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143609193-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-143609193-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>