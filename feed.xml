<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://modanesh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://modanesh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-12T13:15:34+00:00</updated><id>https://modanesh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Ablation Study of the Bayesian GAN</title><link href="https://modanesh.github.io/blog/Bayesian-GAN/" rel="alternate" type="text/html" title="Ablation Study of the Bayesian GAN"/><published>2023-02-13T00:00:00+00:00</published><updated>2023-02-13T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/Bayesian-GAN</id><content type="html" xml:base="https://modanesh.github.io/blog/Bayesian-GAN/"><![CDATA[<p>Although transformers have been dominating the generative AI domain, but I still find this ablation study of Bayesian GAN quite insightful, so I am going to publish it on my website. It is a old project done by: <b>Nicolas Aziere, Mohamad H. Danesh, and Saeed Khorram</b>.</p> <h2 id="abstract">Abstract</h2> <p>This work is about exploring the capacity and limitation of the Bayesian GAN algorithm. The existing framework of the Bayesian GAN is still an unexplored method of learning generative adversarial networks in a bayesian context where the key idea to marginalize the posterior over the weights of the generator and discriminator using a variant of the stochastic gradient descent algorithm namely Hamiltonian Monte-Carlo. It has been demonstrated that this method gives state-of-the-art results in semi-supervised and unsupervised settings and is relatively robust against mode-collapsing. Firstly, we evaluated the influence of some parameters of the algorithm that were not considered in the ablation study of the original paper, and secondly, we trained a Bayesian GAN on a more challenging dataset containing a much larger number of modes that the ones previously studied. Finally, we provide an analysis of the results that will highlight the limitations of the Bayesian GAN and give directions for future work.</p> <h2 id="introduction">Introduction</h2> <p>Generative Adversarial Network (Goodfellow et al., 2014) is one of the most powerful methods to learn the distribution of complex data such as natural images (Brock et al., 2018), audio (Pascual et al., 2017), and text (Press et al., 2017). GAN comprises two main complex mapping functions (usually neural networks): the Generator \(G\) and the Discriminator \(D\). The generator task is to create a candidate sample given a noise vector sampled from a uniform or normal distribution. On the other hand, the discriminator learns to distinguish between the samples candidates generated by the generator and the real data samples in supervised training settings. This adversarial relationship will guide the generator network to create realistic enough candidates to fool the discriminator network and to ideally learn the true data distribution. However, the training procedure of these networks is notoriously difficult, and it is prone to the typical “mode collapse” problem. As most real-world data distributions are highly complex and multi-modal, the challenge for the generator is to output samples that are highly similar to the real data (high precision) yet diverse (high recall) so that the samples can be generalized over the whole data distribution. Nevertheless, in practice, the generator network learns to output a limited number of good quality candidates (high precision) from only one mode of the data. This unique mode will be exploited by the generator to fool the discriminator constantly. This results in candidates that are virtually the same (low recall). To address these issues, various attempts have been made. One is to directly encourage diversity by improving the objective and replacing the Jensen-Shannon divergence of the original GAN paper with enhanced pseudo-metrics such as f-divergence (Nowozin et al., 2016), \(X^2\)-divergence (Mao et al., 2017), and Wasserstein distance (Arjovsky et al., 2017). Further, another thread of research is to train multiple generators (Tolstikhin et al., 2017; Hoang et al., 2018) instead of only one. This hypothesizes that multiple generators can introduce more learning capacity and model the multi-modal data distribution better as in a na¨ıve case, each generator needs to learn a subset of the data. To further extend these ensemble-of-generators models, benefits from Bayesian learning can be exploited to learn the distribution over the GAN parameters. (Saatci &amp; Wilson, 2017), the paper we are analyzing in our work proposed a probabilistic framework for training GAN under Bayesian inference. Bayesian modelling coupled with GAN will introduce new advantages and properties over the classical GAN methods: by learning the posterior over the GAN parameters rather than estimating it as a point mass centered on a single mode (MAP), different modes of the parameters can be sampled which result in totally different generators and discriminators. Moreover, learning the posterior over generator weights can advance more interpretability as each mode will generate different styles of candidates. To be more concise, by investigating the conditional distribution over the parameters of GAN, the true underlying distribution of the data can be modeled more precisely. By applying different priors to represent the distribution of variables, we investigate the effects of those priors over the Bayesian GAN performances. Also, we investigate the weaknesses and limitations represented in (Saatci &amp; Wilson, 2017). In this work, we first go through the idea of the Bayesian GAN, its contributions, and it weaknesses. Next, in <a href="#effect-of-the-prior">section Effect of the prior</a>, we discuss different aspects of out contribution and experiments. Then, our main contribution which is comparing the effects of different priors will be presented. In <a href="#experiments">the experiments section</a>, the experiments we have done will be introduced along with the results we have gotten. Also we will further investigate the validity of the theory and results presented in the (Saatci &amp; Wilson, 2017) through extensive experiments and analysis. The <a href="#conclusion">last section</a> is the conclusion, in which we basically summarize our work and highlight its key points.</p> <h2 id="bayesian-gan">Bayesian GAN</h2> <p>As the insights introduced in (Saatci &amp; Wilson, 2017) are integral to our work, in this section, we will delve into more details of this paper. The Bayesian GAN framework was proposed to learn the posterior distribution of the generator weights \(\theta_g\) and discriminator weights \(\theta_d\) instead of optimizing for point estimate (analogous to a single maximum likelihood solution). Further, an approximate inference algorithm called Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) which is a gradient-based MCMC method was used to approximate the marginal distributions of \(\theta_g\) and \(\theta_d\) corresponding to generator and discriminator, respectively. To learn a generative model over the data (estimation \(p_{data}\)) in an adversarial framework (Goodfellow et al., 2014), first, a noise vector \(z\) (dimension-d) is sampled from a uniform or normal distribution (dimension-d). Next, the noise vector is fed into the generator parameterized by \(\theta_g\) to create a candidate sample similar to the actual data. In theory, if the generator has sufficient capacity, for a set of \(\theta_g\) it can estimate the CDF inverse-CDF composition required to map a uniform noise to actual data distribution. In the proposed Bayesian GAN by (Saatci &amp; Wilson, 2017), the distribution \(p(\theta_g)\) is placed over the generator parameters, i.e. a distribution over the target distribution of the data. Then, a batch of samples from the true data distribution and the candidate samples from the generators are presented to the discriminator parameterized by \(\theta_d\) intending to distinguish between samples from the real and generated data. The distribution \(p(\theta_d)\) is also introduced over the parameters of the discriminator which entails infinite various settings. Further, we point to a few prominent properties of a Bayesian GAN paper (Saatci &amp; Wilson, 2017) in the following. The author claimed that for a Bayesian GAN, the model is robust to mode collapse, and minimal intervention is required during training for stability where conventionally, a large amount of effort is required to stabilize the GAN training: feature matching, label smoothing, and mini-batch discrimination (Radford et al., 2015). Moreover, the probabilistic ensemble of models for both generator and discriminators allow having complementary models for data, i.e., various styles corresponding to different modes in the data distribution can be generated. Moreover, (Saatci &amp; Wilson, 2017) pointed out the probabilistic formulation of the inference in Bayesian GAN with respect to the adversarial feedback as well as state-of-the-art predictive accuracy using a dramatically small amount of labeled data (e.g., 1%) in a semi-supervised scheme as more useful properties of the Bayesian approach in training GANs. To update the posterior beliefs in response to the adversarial feedback, the authors directly defined two conditional distributions for generator and discriminator parameters to iteratively update the posterior:</p> \[p(\theta_g| \textbf{z}, \theta_d) \propto \Big( \prod_{i=1}^{n_g} D(G( \textbf{z}^{(i)} ; \theta_g); \theta_d) \Big) \times p(\theta_g| \alpha_g) \label{eq:con_g}\] \[p(\theta_d| \textbf{z}, \textbf{X}, \theta_g) \propto \prod_{i=1}^{n_d} D(\textbf {x}^{(i)} ; \theta_d) \prod_{i=1}^{n_g}(1-D(G(z^{(i)};\theta_g);\theta_d)) \nonumber \times p(\theta_d|\alpha_d) \label{eq:con_d}\] <p>where the \(p(\theta_g|\alpha_g)\) and \(p(\theta_d|\alpha_d)\) are the priors over the parameters of the generator and discriminator, and \(n_g\) and \(n_d\) represent the number of mini-batch samples for generator and discriminator, respectively. Equation 1 basically suggests that to increase the posterior over the generator parameters \(p(\theta_g|z, \theta_d)\) in the neighborhood where the discriminator outputs high probability for the given generated sample as if is from the true distribution. In equation 2, the posterior \(p(\theta_d|z, X, \theta_g)\) will update according to the classification likelihood for data samples from \(X\) and generated samples \(G(z^i ; \theta_g)\) i.e, increase in the neighborhood where the output probability for real data and generated data mismatches the most. It is worth mentioning that if a vague uniform prior over \(\theta_g\) and \(\theta_d\) is assigned, an iterative estimate of Maximum A Posteriori (MAP) optimization would result in the local point mass optima achieved by classical GAN (Goodfellow et al., 2014). The Bayesian formulation of the generator and discriminator parameters will also provide the option to marginalize the noise samples \(z\) from equations 1 and 2 by simply running a few steps of Monte Carlo and obtaining accurate estimates of \(p(\theta_g|\theta_d)\) and \(p(\theta_d|\theta_g)\) which brings useful practical advantages to enhance both generator and discriminator models. To extend the unsupervised equations stated earlier to the semi-supervised scope, given \(n\) unlabeled observations \({x^i}\), and \(n_s\) labeled data \({(x^{i}_{s} , y^{i}_{s} )}\) with class labels \(y^{i}_{s} \in {1, ..., K}\) equations 1 and 2 should be slightly modified. Instead of outputting a probability form 0 to 1, the discriminator here will output a vector of \(K + 1\) different probabilities representing the probability its input is from any of the \(K\) classes in the actual data as well as one class for identifying whether the sample is from the generator or not.</p> \[p(\theta_g| \textbf{z}, \theta_d) \propto \Big( \prod_{i=1}^{n_g} \sum_{y=1}^K D(G( \textbf{z}^{(i)} ; \theta_g)=y; \theta_d) \Big) p(\theta_g| \alpha_g) \label{eq:semi_g}\] \[p(\theta_d| \textbf{z}, \textbf{x}, \textbf{y}_s, \theta_g) \propto \prod_{i=1}^{n_d} \sum_{y=1}^K D(\textbf {x}^{(i)} = \textbf{y} ; \theta_d) \nonumber \times \prod_{i=1}^{n_g}(1-D(G(z^{(i)};\theta_g);\theta_d)) \nonumber \times \prod_{i=1}^{n_s} D(\textbf{x}_s^{(i)} = \textbf{y}_s^{(i)}; \theta_d) \times p(\theta_d|\alpha_d) \label{eq:semi_d}\] <p>Here, from the perspective of the generator wants to fool the discriminator so that its samples are from the real \(K\) classes in equation 3. On the other hand, the discriminator wants to correctly label the samples that are from the \(K\) classes as well as differentiating from the real and generator samples in equation 4. Finally, in order to effectively sample from the conditional posteriors of the generator and discriminator stated earlier (for both unsupervised and semi-supervised), the Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) is proposed use by the authors. SGHMC is reminiscent of momentum-based SGD with noise where it enables sampling with no more computational complexity that SGD and is known to work well for GANs. As claimed by the authors, SGHMC is a key component of the Bayesian deep learning using noisy estimates of gradients which are guaranteed to mix in with a large number of mini-batches.</p> <h2 id="effect-of-the-prior">Effect of the prior</h2> <p>The Bayesian GAN requires different priors for representing the distribution of different variables. When one is dealing with a generator, the noisy input is generally sampled from a prior represented as a normal distribution \(\mathcal{N}(0,I)\). The goal of a Bayesian GAN is to estimate the posterior distribution of weights of both the generator and the discriminator. Using the Bayesian rule, the posterior distribution can be represented as a product of the likelihood and prior over the weights. As stated earlier, in the semi-supervised setting, the posterior distribution has the following form:</p> \[p(\theta_g| \textbf{z}, \theta_d) \propto \Big( \prod_{i=1}^{n_g} \sum_{y=1}^K D(G( \textbf{z}^{(i)} ; \theta_g)=y; \theta_d) \Big) p(\theta_g| \alpha_g)\] <p>The effect of the prior \(p(\theta_g|\alpha_g)\) becomes less dominant when more data is added to the likelihood. However, we claim that it is advantageous to use a prior more representative of the ideal distribution in order to approximate the posterior distribution more efficiently. Such as proposed in the work of (Kilcher et al., 2017), using a flexible prior distribution can increase the modeling power of a generative model. In their work, they estimate a prior distribution on the latent variable \(z\) using the data by introducing a generator reversal algorithm. They show that constructing a new prior \(p(z)\) from the data instead of using a standard normal distribution is a better-suited choice for various reasons such as: having a better generative model and better modeling performances of the latent structure, more semantically appropriate output. We use the insight of their technique to approximate the best latent variable prior \(p(z)\), however, finding a better prior on the weights of the networks is a more difficult task. Since we do not have any intuition on the type of distribution the weights should model, we designed a simple test case where we evaluate the effect of the prior on a Bayesian GAN.</p> <p><b>Synthetic dataset</b>: We do not have the possibility to use the data since they do not give any information about the prior distribution of the weights or of the latent variable \(\textbf(z)\). To alleviate this problem, we defined an arbitrary weights assignment on a generator that will represent the target model taken from a ground truth distribution \(p^*(\theta_g | \alpha_g)\) , and we do the same with \(p^*\textbf(z))\) . Once a generator is sampled, we draw \(N\) samples from \(p^*\textbf(z)\) and pass them through the target generator. The resulting output for each noisy sample will provide a ground truth sample that will form the new training set. When a new dataset is generated, we can train a Bayesian GAN to model the target distribution. We provide an evaluation of training after different variations of the prior \(p(\textbf(z)\) and \(p(\theta_g| \alpha_g)\).</p> <p><b>Results</b>: We generated a dataset where \(p^*(\theta_g| \alpha_g)\) and \(p^*\textbf(z))\) follow both a uniform distribution \(Uniform(-1,1)\) . We trained a Bayesian GAN where we varied the prior on \(p^*\textbf(z))\) , the results are shown in Figure 1. We can see that the prior has an influence on the performances, particularly on the convergence speed. However, the stability of the convergence is more stable after a few iterations when a \(Normal(0,1)\) distribution is used as prior. When not much data are seen, the effect of the prior is more dominant, which explains why the $Uniform$ distribution is faster to converge. Oppositely, after a few training iterations, the effect of the prior diminished and the generator can overcome the effect of a bad prior. Figure 2 shows how to change the performance according to the prior distribution set on the generator. The target distribution being uniform, we can see that the convergence speed evolving in the same way as in Figure 1. The effect of the prior is dominant when a few samples are seen and become less dominant over time.</p> <p style="text-align:center;"> <img src="/assets/b-gan/priors.png?raw=true" style="height: 250px;text-align:left;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Figure 1: Evolution of the Jensen-Shannon divergence measure on multiple training of a Bayesian GAN trained on 3 different distributions for sampling the noise $$\textbf{z}$$ input of the generator.</figcaption> </p> <p style="text-align:center;"> <img src="/assets/b-gan/priors_g.png?raw=true" style="height: 250px;text-align:left;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Figure 2: Evolution of the Jensen-Shannon divergence measure on multiple training of a Bayesian GAN trained on 3 different distributions for sampling the weights of the generator.</figcaption> </p> <h2 id="experiments">Experiments</h2> <p>Our experiments were to analyze the limitations (Saatci &amp; Wilson, 2017), which include the number of hidden layers in the generator and discriminator networks, the number of generator and discriminator features, different datasets with more variety in data, and a number of labeled samples. Due to computational limitations (Saatci &amp; Wilson, 2017), the results of experiments on different GAN architectures have not been examined. We have addressed this limitation. According to Figure 3, by just adding two more hidden layers, the generated data distribution is closer to the original data. Also, considering the Jensen-Shannon divergence (Fuglede &amp; Topse, 2004), the Bayesian GAN with more hidden layers showed better convergence.</p> <p style="text-align:center;"> <img src="/assets/b-gan/orig.png?raw=true" style="height: 250px;text-align:left;"/> <img src="/assets/b-gan/syn2l.png?raw=true" style="height: 250px;text-align:left;"/> <img src="/assets/b-gan/syn4l.png?raw=true" style="height: 250px;text-align:left;"/> <img src="/assets/b-gan/syn-js.png?raw=true" style="height: 250px;text-align:left;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Figure 3: Experiments done on synthetic data with different number of hidden layers on 1000 iterations. (a) the original data distribution, (b) the generated data distribution with 2 hidden layers (c) the generated data distribution with 4 hidden layers. (d) The divergence is computed using kernel density estimates. It can be seen that the Bayesian GAN is more effective with 4 hidden layers.</figcaption> </p> <p>In another experiment, we examined different numbers of features for each of the networks. For this purpose, features of sizes 64, 96, and 128 have been examined. The results of generator loss and discriminator loss are represented in Figure 4.</p> <p style="text-align:center;"> <img src="/assets/b-gan/gen_features.png?raw=true" style="height: 250px;text-align:left;"/> <img src="/assets/b-gan/disc_features.png?raw=true" style="height: 250px;text-align:left;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Figure 4: Test set loss in 50,000 iterations, trained on CIFAR-10 dataset. (a) Shows that With more features in the network, generator loss decreases faster. (b) Shows that when there are more features, discriminator performs better. By calculating the standard deviation, which is a measure useful for quantifying the amount of variation of a set of data values, it can be shown that the discriminator network with a feature vector of size 128 has the least standard deviation which is 0.39. The discriminator loss for a feature vector of size 96 is 0.52 and for a feature vector of size 64 is 0.47.</figcaption> </p> <p>The experiments which were done to investigate the performance of the Bayesian GAN on a new dataset with more variety in data. CIFAR-100 has been used for this purpose. The reason why we chose this dataset is that although it has the same amount of data as CIFAR-10, 50K images for training and 10K images for testing, the number of classes it has is 10 times bigger. This results in having a more diverse dataset with the same amount of data. The results of experimenting on the test set accuracy are drawn in Figure 5. According to Figure 5, it can be inferred that the Bayesian GAN does not perform well in datasets with diversity. This could be due to the priors it uses, which can be addressed in future works.</p> <p style="text-align:center;"> <img src="/assets/b-gan/diff_datasets.png?raw=true" style="height: 250px;text-align:left;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Figure 5: The results of the test set accuracy on semi-supervised Bayesian GAN.</figcaption> </p> <p>The last experiment we have done is targeting the number of given labeled samples. In this experiment, the CIFAR-10 dataset has been used. The results are depicted in Figure 6. As it was anticipated, the Bayesian GAN got far better accuracy when it had more samples of data.</p> <p style="text-align:center;"> <img src="/assets/b-gan/diff_n.png?raw=true" style="height: 250px;text-align:left;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Figure 6: With more input labeled samples, the Bayesian GAN performed better, almost got twice the accuracy with number of samples equals to 16K.</figcaption> </p> <h2 id="discussion">Discussion</h2> <p>The Bayesian GAN proposed in (Saatci &amp; Wilson, 2017) is valuable in the sense that bridges between Generative Adversarial Networks and Bayesian modeling exploit the best of two worlds. However, we argue there are aspects of the paper that needs further improvement and analysis. One of the most important weaknesses in the paper in our opinion is the choice of prior for both generator and discriminator posterior formulation. In the iterative process of training, the generator distribution only relies on the previous discriminator update which is not desired in cases where the discriminator is not informative in its prediction, i.e., the discriminator tends to assign equal probabilities to all data samples resulting in equal likelihood for all generators (He et al., 2018). Our attempts to alleviate this issue are presented in <a href="#effect-of-the-prior">section Effect of the prior</a>. Further, during experimenting, we faced a few deficiencies. The first one is about experimenting with the different sizes of feature vectors. Standard deviation is not a good measurement to validate the results of the discriminator network loss. Also, due to the huge amount of computations that GANs require. That is why the number of iterations is limited, and datasets with more data have not been experimented with.</p> <p>On the other hand, to point out one of the key contributions of the Bayesian GAN approach, we believe the use of Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) in inference has introduced a novel approach to estimating the posterior since the exact formulation is not practical. Algorithmically, SGHMC is very close to momentum SGD which is inspiring since SGHMC can be used in applications SGD has shown promising performance in recent deep learning breakthroughs and the technical insights of using SGD can be translated to SGHMC for Bayesian approaches. In addition, extensions including Adam-HMC (Kingma &amp; Ba, 2014) etc. can be put into practice.</p> <h2 id="conclusion">Conclusion</h2> <p>This paper proposed an ablation study of the Bayesian GAN method containing a set of additional experiments and an analysis of its advantages and limitations. Finding the right architecture and set of parameters for a model is not a trivial task, we found that the number of layers, the feature size, and the priors greatly influence the learning capabilities of the network. We varied the size of the networks and the size of the feature vector in order to understand the previous choices of the original author of the Bayesian GAN method. We proposed a method to evaluate the effect of the priors by introducing a new dataset generator generated by a target model where the target priors are known. Another significant limitation of the model is its poor ability to address a large number of classes in a dataset such as CIFAR-100. There is still a large set of unexplored areas when investigating the theoretical properties of GANs using Bayesian modeling, and we let open these studies for future work.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Exploring the capacity and limitation of the Bayesian GAN.]]></summary></entry><entry><title type="html">Generating Gaussian Samples From A Uniform Distribution</title><link href="https://modanesh.github.io/blog/Gauss-from-Uniform/" rel="alternate" type="text/html" title="Generating Gaussian Samples From A Uniform Distribution"/><published>2022-03-16T00:00:00+00:00</published><updated>2022-03-16T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/Gauss-from-Uniform</id><content type="html" xml:base="https://modanesh.github.io/blog/Gauss-from-Uniform/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rand() function generates uniformly-distributed numbers between 0~RAND_MAX, where RAND_MAX depends on the implementation and language. For example, in Matlab, RAND_MAX is 1, while in C/C++ RAND_MAX is the maximum integer number of the int representation.</p> <p>The problem is then how to generate numbers distributed with the Gaussian PDF based on rand(), and how to check that what you generate is in fact Gaussian or Pareto distributed. There are many approaches to generate normally-distributed random numbers starting from a uniform distribution. This report describes three methods: Inverse Transform Sampling, Box-Muller Algorithm, and Ziggurat Algorithm. Moreover, finally, we show how our empirically observed data can be verified.</p> <h2 id="inverse-transform-sampling">Inverse Transform Sampling</h2> <p>Inverse transform sampling applies the inverse function of the target cumulative distribution function (CDF) to transform a uniform sample into a target-distributed sample. Here, in our case, the target distribution of interest is the normal distribution. The idea behind inverse transform sampling is that for any distribution, the cumulative probability is always uniformly distributed.</p> <p>As we know, the CDF for normal distribution is defined as: \(CDF(x) = \int_{-\infty}^{x} PDF(t)dt = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}} e^{\frac{-t^2}{2}}dt\)</p> <p>However, the problem is that the above integral does not have a closed-form solution. One approach to address this problem is to measure the CDF of normal distribution using the error function. By definition, the error function is: \(erf(x) = \frac{2}{\sqrt{\pi}}\int_{0}^{x} e^{-t^2}dt\)</p> <p>By doing a small change of variable: \(t^2 = \frac{z^2}{2}\) within the integration, we will have: \(erf(x) = \frac{2}{\sqrt{2\pi}} \int_{0}^{x\sqrt{2}} e^{\frac{-z^2}{2}}dz = \\ 2( \frac{1}{\sqrt{2\pi}} \int_ {-\infty}^{x\sqrt{2}} e^{\frac{-z^2}{2}}dz - \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{0} e^{\frac{-z^2}{2}}dz )\)</p> <p>The integrals on the last line are both values of the CDF of the standard normal distribution: \(\Phi (x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{\frac{-z^2}{2}}dz\)</p> <p>Thus: \(erf(x) = 2(\Phi(x\sqrt{2}) - \Phi(0)) = 2\Phi(x\sqrt{2}) - 1\)</p> <p>This equation offers an advantage and a disadvantage. The disadvantage is that it is not possible to evaluate inverse error function directly. However, the advantage is that the inverse error function can be approximated. One of the most common methods to do so is to approximate using the Taylor series.</p> <p>The Taylor series approximation of a function around a point requires finding the derivatives of the function at that point. For the inverse erf, we set \(x = 0\) as the point to be approximated around, since the inverse erf and the erf functions are symmetric around \(x=0\): \(erf^{-1}(x) = erf^{-1}(0) +\) \(\frac{erf^{-1^{'}}(0)}{1!}x +\) \(\frac{erf^{-1^{''}}(0)}{2!}x^2 + ...\)</p> <p>In the first term, we have \(erf^{-1}(0) = 0\). So to approximate the \(erf^{-1}\), we should calculate the rest of the terms of the Taylor series. The derivatives of \(erf^{-1}\) are: \(erf^{-1^{'}}(x) = \frac{\sqrt{\pi}}{2} e^{(erf^{-1}(x))^2}\)<br/> \(erf^{-1^{''}}(x) = \frac{\sqrt{\pi}}{2} e^{(erf^{-1}(x))^2} 2 erf^{-1}(x) erf^{-1^{'}}(x) = (erf^{-1^{'}}(x))^2 2 erf^{-1}(x)\)</p> <p>When deriving higher derivatives of the \(erf^{-1}\), starting from the second derivative, each derivate of order \(n\) is a product of the first derivative to the power of \(n\), and a polynomial of \(erf^{-1}\). By investigating and simplifying further, we see that all the even-powered terms in the Taylor series have no constant term for their polynomials. Thus, the Taylor series approximation only involves odd-powered terms. That results in the following simplified Taylor series approximation of the \(erf^{-1}(x)\): \(erf^{-1}(x) = \frac{\sqrt{\pi}}{2} (x + \frac{\pi}{12}x^3 + \frac{7\pi}{480}x^5 + ...)\)</p> <p>Now, the above Taylor approximations of \(erf^{-1}\) can be used to approximate the inverse CDF of standard normal distribution. Clearly, including more terms in the Taylor series result in a better approximation of the true inverse CDF.</p> <p>Finally, we can now generate normally-distributed samples using inverse transform sampling. To do so, following steps need to be taken:</p> <ul> <li>Sample a point from a uniform distribution.</li> <li>Apply Taylor series approximation of inverse normal CDF.</li> <li>Generate normal samples.</li> </ul> <p>In conclusion, this method can generate any random variable if its CDF is easily calculated. If it is not, using this method would be challenging as discussed earlier. It requires one to use complex approximation methods. Other methods such as Box-Muller and rejection sampling could be helpful in that regard.</p> <h2 id="box-muller-algorithm">Box-Muller Algorithm</h2> <p>The Box-Muller method gets two samples from a uniform distribution and generates two independent samples from a standard normal distribution. To do so, consider two sets of random samples (IID) with equal lengths drawn from the \(U(0,1)\), \(u_0\) and \(u_1\). From these two sets, one can generate two sets of normally-distributed random variables, drawn from \(N(\mu=0, \sigma=1)\) which we call \(n_0\) and \(n_1\), where: \(n_0 = \sqrt{-2 \ln(u_0)} \cos(2\pi u_1) \\ n_1 = \sqrt{-2 \ln(u_0)} \sin(2\pi u_1)\)</p> <p>If we assume to position the variables \(u_0, u_1\) in the Cartesian plane, we can take advantage of the relationship between Cartesian coordinates and polar coordinates \((r, \theta)\), by: \(n_0 = R * \cos{\theta} \\ n_1 = R * \sin{\theta}\)</p> <p>Now, in the polar coordinates, a bivariate normal distribution \((n_0, n_1)\) has a norm $R$ corresponding to \(\sqrt{-2 \ln u_0}\), and an angle \(\theta\) corresponding to \(2 \pi u_1\). This allows us to map the variables defined in the original Cartesian system to the normally-distributed variables \(n_0, n_1\). By doing so, we will reach the variables defined in the first equation (re \(n_0\)).</p> <h3 id="implementation">Implementation</h3> <p>Implementation of the algorithm can be found here: <script src="https://gist.github.com/modanesh/a96aeb77b91e309aaa28fe96d913c171.js"></script></p> <p>Figure bellow shows the output of an example run of the provided code:</p> <p style="text-align:center;"> <img src="/assets/gauss-normal/1.png?raw=true" style="height: 250px;text-align:left;"/> <img src="/assets/gauss-normal/2.png?raw=true" style="height: 250px;text-align:right;"/> <img src="/assets/gauss-normal/3.png?raw=true" style="height: 250px;text-align:right;"/> <img src="/assets/gauss-normal/4.png?raw=true" style="height: 250px;text-align:right;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">An example run of the Box-Muller algorithm. The two top figures represent the histogram plot of 1000 samples drawn from a uniform distribution. The two histogram plots on the bottom represent their corresponding Box-Muller transformations into Gaussian distributions.</figcaption> </p> <h2 id="ziggurat-rejection-sampling-algorithm">Ziggurat (Rejection Sampling) Algorithm</h2> <p>The idea behind rejection sampling is that if one cannot sample from a distribution (target), another distribution function (proposal) could be used for sampling. However, since the target distribution and the proposal distribution are different, drawn samples (from the proposal distribution) must follow the target distribution. It means that regions with a high probability in the target distribution should be sampled more.</p> <p>Assume that we have a uniform distribution as the proposal function and a normal distribution as the target function. First, the proposal distribution should encapsulate the target distribution, so that drawn samples could be either rejected or accepted. To do so, the proposal distribution should be scaled with respect to the mean and standard deviation of the target distribution. Then, the proposal PDF is separated into a series of segments with equal areas ( bins). Next, the sampling process begins: a sample is drawn from the scaled proposal distribution. We look it up to see which segment it belongs to. If the corresponding segment density in the proposal function is lower than the corresponding segment density in the target function, the sample is accepted; otherwise, it is rejected. By repetitively doing this, more samples within the acceptable region are taken.</p> <p>There are issues with the naive rejection sampling method, which the Ziggurat algorithm addresses: first, in the naive rejection sampling method, a large number of samples will be rejected since we have only one segment. Second, by discretizing the proposal PDF using segmentation, we make it computationally tractable to evaluate whether to reject or accept a candidate.</p> <h3 id="implementation-1">Implementation</h3> <p>Ziggurat implementation is available here: <script src="https://gist.github.com/modanesh/80c28192dc56f4f32994cf9a4b8eb93b.js"></script></p> <p>Figure bellow shows an example output of running the code:</p> <p style="text-align:center;"> <img src="/assets/gauss-normal/z1.png?raw=true" style="height: 250px;text-align:left;"/> <img src="/assets/gauss-normal/z2.png?raw=true" style="height: 250px;text-align:right;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">An example run of the Ziggurat algorithm. The left plot shows the histogram plot of 1000 samples drawn from a uniform distribution. The right plot shows the corresponding accepted samples (Gaussian distribution).</figcaption> </p> <h2 id="verifying-results">Verifying Results</h2> <p>Finally, in order to verify the results, one can use the goodness-of-fit tests. They are statistical approaches aiming to determine whether a set of observed values match those expected under the normal distribution. There are several approaches for the goodness-of-fit tests, including the chi-square, the Kolmogorov-Smirnov test, and the Lilliefors test. The chi-square tests the validity of a claim made about a population of observed values based on a random sample.</p> <p>The chi-square test requires samples to be represented in a categorical format, which is limiting in our case. As a replacement, one can use the Kolmogorov-Smirnov test for normality, but the mean and the standard deviation must be known beforehand; nevertheless, the Kolmogorov-Smirnov test yields conservative results. The Lilliefors test tackles this problem by giving more accurate results. The only difference between the Lilliefors test and the Kolmogorov-Smirnov test is that the former uses the Lilliefors Test Table while the latter uses the Kolmogorov-Smirnov Table. Otherwise, both have the same calculations.</p> <p>For the Lilliefors Test, we need to define our null hypothesis and the alternate hypothesis. The null hypothesis ( \(H_0\)) for the test is that the data comes from a normal distribution, and the alternate hypothesis (\(H_1\)) is that the data does not come from a normal distribution.</p> <p>The calculation steps of Lilliefors test are:</p> <ul> <li> <p>calculate z-score \(Z_i\) using the following equation:</p> \[Z_i = \frac{X_i - \bar{X}}{s}, i = 1, 2, ..., n\] <p>where \(Z_i\) is the individual z-scores for every member in the sample set, \(X_i\) is the individual data point, \(\bar{X}\) is the sample mean, and \(s\) is the standard deviation.</p> </li> <li> <p>calculate the test statistic, which is the empirical distribution function based on the \(Z_i\)s:</p> \[D = \sup_{X} |F_0 (X) - F_{data} (X)|\] <p>where \(F_0\) is the the standard normal distribution function (hypothesized distribution), and \(F_{data}\) is the empirical distribution function of the observed values. - find the critical value for the test from the Lilliefors Test table and reject the null hypothesis if the test statistic \(D\) is greater than the critical value.</p> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Generating numbers that are distributed with the Gaussian distribution (with any mean and standard deviation as parameters), starting from the random number generator of a computer, i.e. the rand() function.]]></summary></entry><entry><title type="html">Distributional Reinforcement Learning</title><link href="https://modanesh.github.io/blog/C51-QRDQN-IQN/" rel="alternate" type="text/html" title="Distributional Reinforcement Learning"/><published>2021-03-03T00:00:00+00:00</published><updated>2021-03-03T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/C51-QRDQN-IQN</id><content type="html" xml:base="https://modanesh.github.io/blog/C51-QRDQN-IQN/"><![CDATA[<h2 id="distributional-rl">Distributional RL</h2> <p>In common RL approaches, we have a value function which returns a single value for each action. This single value is the expectation of a true distribution which in the distributional RL, we seek to return that for each action. In common RL, value function is defined using the Bellman’s equation:</p> \[Q(x,a) = \mathbb{E} R(x,a) + \gamma \mathbb{E} Q(X', A')\] <p>In distributional RL, we need to drop those expectations and so the distributional Bellman’s equation would look like (onte that this equality sign means that the sides are random variables drawn from the same probability distribution law):</p> \[Z(x,a) = R(x,a) + \gamma Z(X', A')\] <p>According to this equation, there are three random variables that affect the value distribution: reward function, transition function, and next state-action value distribution. Distributional RL has studied before, but it was limited to specific cases like risk-sensitive policies or theoretical analysis.</p> <h3 id="prerequisites">Prerequisites</h3> <p>KL divergence: sum of log probability ratios of two probability distributions. For two distributions with non-overlapping supports, it’ll be infinite.</p> <p>Total variation: how much probability mass two distributions disagree. It’s done by subtracting all the mass where they predict the same outcome. Shifting two distributions (e.g. multiplying by discount factor), total variation won’t change so it’s not a good distance.</p> <p>Wasserstein: The area (integral) between the CDFs of two random variables. The three properties of the Wasserstein metric is very useful for the RL case.</p> <h3 id="c51-1">C51 [1]</h3> <p>In common RL, the Bellman operator has the alpha-contraction property which basically means that by applying the Bellman operator, our estimation of the value function gets closer to the optimal one. But in dist RL, that’s not as straight forward as common RL. In policy evaluation, the distributional Bellman operator is an alpha-contraction only in a maximal form of the Wasserstein metric. In policy improvement, things are messier and it requires more investigation. An example is provided further in the paper that discusses this issue. Using value distributions over value functions offers a few benefits like preserving the multimodality of returns, and decreasing the bad effects of learning from a non stationary policy, which all leads to more stability in learning.</p> <p>MDP setting definition is:</p> \[(X, A, R, P, \gamma)\] <p>The common definition for value function which estimates a value for state action pair based on policy \(\pi\) is:</p> \[Q^\pi(x,a) = \mathbb{E} R(x,a) + \gamma \mathbb{E}_{P, \pi} Q^{\pi}(x', a')\] <p>We have the definition for optimal value function which has this max operator for taking the next action:</p> \[Q^{*}(x,a) = \mathbb{E} R(x,a) + \gamma \mathbb{E}_{P} \max_{a' \in A} Q^{*}(x', a')\] <p>This equation has a unique fixed point which is very important. It means that starting from any arbitrary value function, by iteratively updating the value function this way, we would end up having the optimal unique value function.</p> <p>And the definition for Bellman operator and optimality Bellman operator. They basically take a value function as input and return an updated value function which can describe the expected behavior of the learning algorithm:</p> \[\mathrm{T}^{\pi} Q(x,a) := \mathbb{E} R(x,a) + \gamma \mathbb{E}_{P, \pi} Q(x',a')\] \[\mathrm{T} Q(x,a) := \mathbb{E} R(x,a) + \gamma \mathbb{E}_{P} \max_{a'\in A} Q(x',a')\] <p>The Wasserstein metric which for two CDFs (probability that \(U\) will have a value less than or equal to \(y\)) is defined as:</p> \[d_p (F,G) := \inf_{U,V} ||U-V||_p\] <p>Infimum is actually the greatest lower bound. Next we can remove the inf and put inverse CDFs because inf is implicit in its definition and we get a LP norm of a vector. And having \(p &lt; \infty\) we arrive to this equation:</p> \[d_p (F,G) = \left( \int_{0}^{1} |F^{-1}(u) - G^{-1}(u)|^p du \right)^{1/p}\] <p>And eventually we get to this equation which basically means that Wasserstein is a metric wrt to these random variables:</p> \[d_p (U,V) := \inf_{U,V} ||U-V||_p\] <p>This Wasserstein metric has three important properties that are useful and discussed in the paper.</p> <p>The maximal form of the Wasserstein metric for value distributions are (supremum is least upper bound):</p> \[\overline{d}_p (Z_1, Z_2) := \sup_{x,a} d_p(Z_1(x,a),Z_2(x,a))\] <p>Policy evaluation: distributional Bellman operator is a gamma contraction in \(d_p\). It can be concluded that the sequence of value functions under policy \(\pi\) converges to \(Z_\pi\) in \(d_p\) (for \(1 \leq p \leq \infty\)). It’s important that it only works for Wasserstein metric, not any other metrics. But further in the paper for practical usage they switch to kl divergence.</p> <p>Policy improvement: in common RL where we have value functions, all optimal policies get to the same \(Q^*\) which is the unique fixed point, but in distributional RL, there may be many optimal value distributions for different optimal policies. And that’s what makes it a bit messy, meaning that this operator is not a contraction in any metric. In the best case, it converges to the non stationary optimal value distribution.</p> <p>Now to approximate the value distribution, they propose a method to have a discrete distribution with \(N\) atoms at fixed locations in a range of \(v_min\) to \(v_max\). Having a discrete distribution causes the Bellman update and our estimation to have disjoint supports meaning that they mostly don’t overlap. In this case, KL-JS-total variation metrics are problematic but Wasserstein metric could be very useful as we also saw it’s theoretically strong. But, since transitions are sampled, the Wasserstein loss and the sample loss are not exactly the same (proposition 5). Why Wasserstein metric hasn’t been used in the paper: biased Wasserstein gradients. There’s an approximation which make things difficult.</p> <p>To solve this issue, the Bellman update is projected into the sampled discrete distribution (fig 1 \(\phi\) operator). This projection step is simply a linear interpolation to the closest neighbor. And now since we have overlapping distributions, we can use KL divergence as the metric (and loss):</p> \[D_{KL} (\Phi \hat{\mathrm{T}} Z_{\tilde{theta}} (x,a) || Z_\theta (x,a))\] <p>Which is in contrast to what they’ve talked about in most of the paper.</p> <p>In experiments: there are sources of stochasticity even in deterministic Atari envs: partial observability, non stationary policies, and approximation errors. Authors say that having 51 atoms results in better performance compared to other numbers of atoms and they don’t know why is that. And that’s where their algorithm got it’s name from. Nothing surprising in their experiments, they have better sample efficiency and overall performance compared to DQN. The only exciting thing is that it performs very good in games with sparse rewards (VENTURE and PRIVATE EYE). This suggests that value distributions are better able to propagate rarely occurring events.</p> <p>The distinction we wish to make is that learning distributions matters in the presence of approximation.</p> <h3 id="qr-dqn-2">QR-DQN [2]</h3> <p>Using kl divergence in C51 left a theory-practice gap open which in this paper they talk about and try to address. One interesting point they raise is that since they didn’t follow the theory they provided, it’s hard to justify the improvements in performance they reported which actually is a fair point if you think about it.</p> <p>The contributions of this paper are: first, instead of having a set of fixed location atoms, they parameterize the value distribution using a set of fixed probabilities with adjustable locations. Doing this they will be able to use Wasserstein metric and so close the gap. Second, in order to have Wasserstein metric, they use quantile regression. Third, they provide the contraction property for their overall algorithm.</p> <p>They raise the concern about C51’s performance saying that although it still were acting by maximizing expectations, it’s SOTA performance was the main interesting thing in the paper. And I also think it is since in C51, in policy improvement, actions were taken based on the max operator and so it basically is DQN. Anyone has an idea to explain this concern?</p> <p>Up to here they talk about the Wasserstein metric and bellman operator’s contraction property under this metric. But Wasserstein metric cannot be used according to theorem 1: let \(\hat{y}\) be the empirical distribution draw from \(B\). And let \(b_\mu\) be a distribution parameterized by \(\mu\). Then, the minimum of the expected sample loss is in general different from the minimum of the true Wasserstein loss.</p> <p>Figure 2: it shows the Wasserstein metric between two random variables. There are 4 quantiles q1, q2, q3, q4. And their median is named as \(\hat{tau}_1, \hat{tau}_2, \hat{tau}_3, \hat{tau}_4\). The sum of the red shaded regions is the wasserstein metric. If \(n\) were bigger, the wasserstein metric would decrease because our estimation would be closer to the real distribution.</p> <p>Further, they introduce their method. Having \(N\) quantiles, each with a weight of \(1/N\). So If the atoms are \(N\) then the \(i\)-th atom corresponds to a quantile of \(\tau_i\).</p> <p>Benefits of quantile distribution: no need to have pre-specified bounds for discrete distribution’s support. No need to have a projection step since we’ll have the wasserstein metric here so the non-overlapping supports won’t make any problems. Take advantage of wasserstein metric, using quantile regression.</p> <p>(quantile regression) when we want to understand the relations between variable outside of the mean, or when we have non-normally distribution data, quantile regression can be very useful. Because it penalizes estimates differently depending on which quantile they belong to and whether they are too high or too low. For low quantiles, it penalizes overestimates more heavily, while for high quantiles it penalizes underestimates more heavily.</p> <p>Quantile huber loss: the idea of huber loss is to make L1 norm’s gradient better behaved around 0, by turning it into an L2 norm. So that’s exactly the motivation behind combining quantile regression and huber loss, to make quantile regression well-behaved around 0.</p> <p>Proposition 2 proves that the QR-DQN algorithm has a contraction property.</p> <p>Comparing QR-DQN with plain DQN: everything is the same except two things: output size is changed so as to return values according to the number of quantiles and actions. Next, the loss function is changed. Also, the rmsprop is replaced by Adam for optimization.</p> <p>Their experimental results are good, nothing surprising reported.</p> <h3 id="iqn-3">IQN [3]</h3> <p>Compared to QR-DQN, there are two major differences: instead of using the quantile divided by the same probability, the probabilities are randomly sampled implicitly, and also the network architecture is different. IQN also uses the quantile regression technique as QR-DQN. As an example: let’s say the number of quantiles is 4. In this case, the quantile value of QR-DQN is [0.25, 0.5, 0.75, 1], and QR-DQN derives a support corresponding to the median of the quantile to minimize the Wasserstein distance. That is, we estimate the supports corresponding to [0.125, 0.375, 0.625, 0.875]. However, in IQN, the quantile value \(\tau\) is randomly sampled between 0 and 1. If there are 4 quantiles, let’s say that 4 randomly extracted values between 0 and 1 are [0.12, 0.32, 0.78, 0.92]. Sampling like this from the value distribution gives the opportunity to measure the level of risk the policy takes.</p> <p>Advantages of IQN over QR-DQN are that the approximation error won’t be controlled by the number of quantiles. It has better sample efficiency. It is also capable of learning a wider range of policies.</p> <p>Risk in RL: let’s consider two actions each with a normal value distribution, but different means and standard deviations. Let’s say action 1 has smaller mean and std than action 2. Now, since action 1 that has a smaller variance, the probability of receiving a return close to the average is high. But the expected value of the value distribution is less than action 2. In Distributional RL we compare the expected value of the distribution and select an action, so in this case action 2 will be selected. For a2, it is a distribution with a very large variance. In this distribution, very small returns may be derived or very high returns may be derived in some cases. In Distributional RL, such a large variance is said to be “high risk” when confidence in the result is low. Conversely, in the case of a1, which has a relatively high confidence in the result, it can be said that the “risk is small” compared to a2. When learning is performed through sampling and selecting an action, it is possible to select an action according to this risk. There are two types of risk sensitive policies: risk-averse policy and risk-seeking policy.</p> <p>Implicit Quantile Networks (implementation): Looking at the figure 1, the result obtained through the convolution function (\(\psi\)) and the result obtained through the embedding function (\(\phi\)) for \(\tau\) are element-wise multiplied. And by performing a fully-connected layer operation on the result, we finally get the value distribution for action \(a\).</p> <p>Embedding function (\(\phi\)) (equation 4): The role of this function is to return an embedding sampled \(\tau\) as a vector. The function for embedding tau is an \(n\) cosine basis function .</p> <p>Quantile huber loss (equation 3): Quantile Huber Loss used in QR-DQN is used as it is. But, in this paper, the equation is expressed as eq 3. The only point about this equation is that \(N\) is the number of \(\tau\) samples sampled for the online network, and \(N'\) is the number of \(\tau\) samples sampled for the target network. The rest is the same as QR-DQN.</p> <p>(Under eq 4): they hypothesized that \(N\) would affect the sample complexity, and \(N'\) would affect the variance of gradients. But results show that initially \(N'\) is effective but as the training goes on, it loses its impact.</p> <p>Types of risks (fig 3): Risk-averse &gt; Neutral &gt; Risk-seeking. About this, the longer the game lasts, the higher the rewards would be, so it can be considered that the risk-averse policy of choosing a stable reward shows good performance even though it is less than the risk-seeking policy of choosing a high reward while being uncertain.</p> <p>One interesting point is that in table 1, the mean and median of scores to compare different algorithms are based on different experiment setups. I mean for DQN, PER, and c51 scores are only for 1 seed but for IQN report are for 5 seeds. Which is not fair (?).</p> <h3 id="references">References</h3> <p>[1] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 449–458. JMLR. org, 2017.</p> <p>[2] Will Dabney, Mark Rowland, Marc G Bellemare, and Rémi Munos. Distributional reinforcement learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p> <p>[3] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In International Conference on Machine Learning, pages 1104–1113, 2018.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Presenting some of the most fundamental works on distributional RL.]]></summary></entry><entry><title type="html">Actor-Critic with Experience Replay</title><link href="https://modanesh.github.io/blog/Actor-Critic-with-Experience-Replay/" rel="alternate" type="text/html" title="Actor-Critic with Experience Replay"/><published>2021-01-29T00:00:00+00:00</published><updated>2021-01-29T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/Actor-Critic-with-Experience-Replay</id><content type="html" xml:base="https://modanesh.github.io/blog/Actor-Critic-with-Experience-Replay/"><![CDATA[<h2 id="acer">ACER</h2> <p>It is an off-policy actor-critic model with experience replay, greatly increasing the sample efficiency and decreasing the data correlation. The reason for doing that is because ACER is off-policy and to control the stability of the off-policy estimator:</p> <ul> <li>It has multiple workers (as A2C);</li> <li>It uses replay buffer (as in DQN);</li> <li>It uses Retrace Q-value estimation;</li> <li>It truncates the importance weights with bias correction;</li> <li>It applies TRPO.</li> </ul> <p>Deep Q-learning methods are most sample efficient techniques. However, they have two important limitations. <strong>First</strong>, the deterministic nature of the optimal policy limits its use in adversarial domains. <strong>Second</strong>, finding the greedy action with respect to the Q function is costly for large action spaces. ACER matches SOTA of DQN with PER on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.</p> <p>In the calculation of the gradient of the policy wrt to theta (following equation), we can replace \(A\) with state-action value \(Q\), the discounted return \(R\), or the temporal difference residual, without introducing bias:</p> \[g = \mathbb{E}_{x_{0:\infty}, a_{0:\infty}} [\sum_{t \geq 0} A^{\pi} (x_t, a_t) \bigtriangledown_{\theta} log \pi_\theta (a_t, x_t)]\] <p>But since we use deep networks thus introducing additional approximation errors and biases. Comparing the expressions, we can have instead of \(A\), the discounted return will have higher variance and lower bias but the estimators using function approximation will have higher bias and lower variance. In ACER, they combine \(R\) with the current value function approximation to minimize bias while maintaining bounded variance.</p> <p>ACER is A3C’s off-policy counterpart. It uses parallel agents. It also uses a single neural net to estimate policy and value function. Gradient of policy with importance sampling:</p> \[\hat{g}^{imp} \left( \prod_{t=0}^{k} \rho_t \right) \sum_{t=0}^{k} \left( \sum_{i=0}^{k} \gamma^i r_{t+i} \right) \bigtriangledown_\theta log \pi_\theta (a_t, x_t)\] <p>It uses discounted rewards to reduce bias, but has a very high variance since it involves a product of many potentially unbounded importance weights. Truncating this product can prevent it from exploding. But this also adds bias. Then Degris suggested to use marginal value functions over the limiting distributions (The limiting distribution can be used on small, finite samples to approximate the true distribution of a random variable) to have this gradient expression:</p> \[g^{marg} = \mathbb{E}_{x_t \sim \beta , a_t \sim \mu} [\rho_t \bigtriangledown_{\theta} log \pi_\theta (a_t, x_t) Q^\pi (x_t, a_t)]\] <p>It depends on \(Q_\pi\) and not on \(Q_\mu\), consequently we must be able to estimate \(Q_\pi\). And instead of having a long product, it has marginal importance weight which lowers the variance. In ACER, they use Retrace to estimate \(Q_\pi\).</p> <p>To have off-policy samples (\(Q_\pi\)), there’s a need for importance sampling:</p> \[\Delta Q^{imp} (S_t, A_t) = \gamma^t \prod_{1 \leq \tau \leq t} \frac{\pi(A_tau, S_tau)}{\beta(A_tau, S_tau)}\delta_t\] <p>Importance sampling is a technique of estimating the expected value of \(f(x)\) where \(x\) has a data distribution \(p\). However, instead of sampling from \(p\), we calculate the result from sampling \(q\):</p> \[\mathbb{E}_p [f(x)] = \mathbb{E}_q (\frac{f(x)p(x)}{q(x)})\] <p>Retrace Q-value estimation method modifies \(\Delta Q\) to have importance weights truncated by no more than a constant \(c\):</p> \[\Delta Q^{ret} (S_t,A_t) = \gamma^t \prod_{1 \leq \tau \leq t} min(c, \frac{\pi(A_tau, S_tau)}{\beta(A_tau, S_tau)}\delta_t)\] <p>ACER uses \(Q^{ret}\) as the target to train the critic by minimizing the L2 error term: \((Q^{ret}(s,a)−Q(s,a))^2\).</p> <p>Finally, the multi-step estimator \(Q^{ret}\) has two benefits: to reduce bias in the policy gradient, and to enable faster learning of the critic, which further reduces bias.</p> <p>Truncate the importance weights with bias correction: To reduce the high variance of the policy gradient, ACER truncates the importance weights by a constant \(c\), plus a correction term.</p> <p>Furthermore, ACER adopts the idea of TRPO but with a small adjustment to make it more computationally efficient: rather than measuring the KL divergence between policies before and after one update, ACER maintains a running average of past policies and forces the updated policy to not deviate far from this average.</p> <p>Continuous case: It is not easy to integrate over \(Q\) to derive \(V\) in continuous action spaces. To overcome this issue, they propose a network architecture similar to dueling DQN which estimates both \(V_\pi\) and \(Q_\pi\) off-policy (SDNs). In addition to SDNs, they also construct (equation 14) novel target for estimating value function derived via the truncation and bias correction trick.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A brief overview of the ACER RL algorithm is provided.]]></summary></entry><entry><title type="html">Exploration and Generalization in Reinforcement Learning</title><link href="https://modanesh.github.io/blog/Exploration-and-Generalization-in-RL/" rel="alternate" type="text/html" title="Exploration and Generalization in Reinforcement Learning"/><published>2020-09-14T00:00:00+00:00</published><updated>2020-09-14T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/Exploration-and-Generalization-in-RL</id><content type="html" xml:base="https://modanesh.github.io/blog/Exploration-and-Generalization-in-RL/"><![CDATA[<h4 id="conservative-uncertainty-estimation-by-fitting-prior-networks"><a href="https://openreview.net/forum?id=BJlahxHYDS">Conservative uncertainty estimation by fitting prior networks</a>:</h4> <p>By using random prior networks and comparing its output with the learnable network, one can easily determine the level of uncertainty regarding the given input data. The more uncertain, the more unseen that data is, thus is a good option for exploration. Once the data is given to the learnable network, it tries to match its output to the one from the random prior network.</p> <p style="text-align:center;"> <img src="/assets/expgen-rl/exp_gen_rl_1.png?raw=true" style="height: 250px;text-align:center;"/> </p> <p>It is worth noting that these two networks mostly have the same architecture:</p> <p style="text-align:center;"> <img src="/assets/expgen-rl/exp_gen_rl_2.png?raw=true" style="height: 250px;text-align:center;"/> </p> <p>Theoretical results:</p> <p>Having theoretical results with function approximators and in a domain with high dimensional data are quite important and hard. First, random prior uncertainty estimates are conservative, meaning that it can be guaranteed to not over-confident which is very important in safety critical applications of ML and RL. Second, random priors are guaranteed to concentrate with infinite data, meaning that the more data that is used, eventually, these estimates will converge.</p> <h4 id="variational-deep-rl-varibad"><a href="https://arxiv.org/abs/1910.08348">Variational deep RL (VariBad)</a>:</h4> <p>Key idea: learn to learn new tasks in simulation, learn to rapidly adapt to real new tasks.</p> <p>The aim is to flexibly learn bayes-optimal behavior for rapid online adaptation with minimal assumptions.</p> <p>There is a recurrent encoder which learns variational trajectory embeddings, known as belief. The point of having recurrent network is to maintain information through multiple steps. The belief has a lower dimensions compared to states and actions:</p> <p style="text-align:center;"> <img src="/assets/expgen-rl/exp_gen_rl_3.png?raw=true" style="height: 150px;text-align:center;"/> </p> <p>With the belief that the encoder embeds, there are two things to be done.</p> <p>Decoder: Learns to predict future states and rewards. The encoder-decoder pipeline forces the agent to maintain a belief that is relevant to predicting the future.</p> <p style="text-align:center;"> <img src="/assets/expgen-rl/exp_gen_rl_4.png?raw=true" style="height: 130px;text-align:center;"/> </p> <p>And finally the agent conditions policy based on its belief. The policy learns approximately bayes-optimal behavior given current belief.</p> <p style="text-align:center;"> <img src="/assets/expgen-rl/exp_gen_rl_5.png?raw=true" style="height: 100px;text-align:center;"/> </p> <h4 id="generalization-in-rl-with-selective-noise-injection-and-information-bottleneck"><a href="https://arxiv.org/abs/1910.12911">Generalization in RL with selective noise injection and information bottleneck</a>:</h4> <p>First insight: Selective noise injection is valuable, meaning that instead of applying regularization across to all the components, it applies the noise to the components that are most helpful. For example, it does not apply noise to the components regarding the exploration, because originally it has noise embedded in it (e.g. the behavior rollout policy). Doing this speeds up the learning process.</p> <p style="text-align:center;"> <img src="/assets/expgen-rl/exp_gen_rl_6.png?raw=true" style="height: 100px;text-align:center;"/> </p> <p>Second insight: Regularization with information bottleneck is particularly effective. Because in the low data regime, e.g. in RL, it is valuable to compress observations that are available to the agent to only learn those features that provide additional features, for a particular task.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A brief description on a few methods to make RL agents explore and generalize faster/better.]]></summary></entry><entry><title type="html">Reinforcement Learning Key Papers Keynotes</title><link href="https://modanesh.github.io/blog/RL-key-papers-key-notes/" rel="alternate" type="text/html" title="Reinforcement Learning Key Papers Keynotes"/><published>2019-12-01T00:00:00+00:00</published><updated>2019-12-01T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/RL-key-papers-key-notes</id><content type="html" xml:base="https://modanesh.github.io/blog/RL-key-papers-key-notes/"><![CDATA[<ul> <li> <p><a href="https://openreview.net/forum?id=B1DmUzWAW&amp;noteId=B1DmUzWAW">A Simple Neural Attentive Meta-Learner</a>, algorithm: SNAIL</p> <ul> <li> <p>Uses a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.</p> </li> <li> <p>Rather than training the learner on a single task (with the goal of generalizing to unseen samples from a similar data distribution) a meta-learner is trained on a distribution of similar tasks, with the goal of learning a strategy that generalizes to related but unseen tasks from a similar task distribution.</p> </li> <li> <p>Combines temporal convolutions, which enable the meta-learner to aggregate contextual information from past experience, with causal attention, which allow it to pinpoint specific pieces of information within that context.</p> </li> <li> <p>Soft attention treats the context as an unordered key-value store which it can query based on the content of each element. However, the lack of positional dependence can also be undesirable, especially in reinforcement learning, where the observations, actions, and rewards are intrinsically sequential.</p> </li> <li> <p>Despite their individual shortcomings, temporal convolutions and attention complement each other: while the former provide high-bandwidth access at the expense of finite context size, the latter provide pinpoint access over an infinitely large context.</p> </li> <li> <p>By interleaving TC layers with causal attention layers, SNAIL can have high-bandwidth access over its past experience without constraints on the amount of experience it can effectively use.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>, algorithm: MAML</p> <ul> <li> <p>Proposes an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning.</p> </li> <li> <p>Aims to train models that can achieve rapid adaptation, a problem setting that is often formalized as few-shot learning.</p> </li> <li> <p>Makes no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \(\theta\), and that the loss function is smooth enough in \(\theta\) that we can use gradient-based learning techniques.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.05763">Learning to Reinforcement Learn</a></p> <ul> <li> <p>Emerges a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure.</p> </li> <li> <p>Here, the tasks that make up the training series are interrelated RL problems, for example, a series of bandit problems varying only in their parameterization. Rather than presenting target outputs as auxiliary inputs, the agent receives inputs indicating the action output on the previous step and, critically, the quantity of reward resulting from that action.</p> </li> <li> <p>At the start of a new episode, a new MDP task \(m \approx D\) and an initial state for this task are sampled, and the internal state of the agent (i.e., the pattern of activation over its recurrent units) is reset. The agent then executes its action-selection strategy in this environment for a certain number of discrete time-steps. At each step \(t\) an action \(a_t \in A\) is executed as a function of the whole history \(H_t = {x_0, a_0, r_0, . . . , x_{t-1}, a_{t-1}, r_{t-1}, x_t}\) of the agent interacting in the MDP \(m\) during the current episode. The network weights are trained to maximize the sum of observed rewards over all steps and episodes.</p> </li> <li> <p>After training, the agent’s policy is fixed (i.e. the weights are frozen, but the activations are changing due to input from the environment and the hidden state of the recurrent layer), and it is evaluated on a set of MDPs that are drawn either from the same distribution \(D\) or slight modifications of that distribution (to test the generalization capacity of the agent). The internal state is reset at the beginning of the evaluation of any new episode.</p> </li> <li> <p>Since the policy learned by the agent is history-dependent (as it makes uses of a recurrent network), when exposed to any new MDP environment, it is able to adapt and deploy a strategy that optimizes rewards for that task.</p> </li> <li> <p>All reinforcement learning was conducted using the Advantage Actor-Critic algorithm.</p> </li> <li> <p>reward and last action are additional inputs to the LSTM. For non-bandit environments, observation is also fed into the LSTM either as a one-hot or passed through an encoder model [3-layer encoder: two convolutional layers (first layer: 16 8x8 filters applied with stride 4, second layer: 32 4x4 filters with stride 2) followed by a fully connected layer with 256 units and then a ReLU non-linearity]. For bandit experiments, current time step is also fed in as input.</p> </li> <li> <p>Deep meta-RL involves a combination of three ingredients: (1) Use of a deep RL algorithm to train a recurrent neural network, (2) a training set that includes a series of interrelated tasks, (3) network input that includes the action selected and reward received in the previous time interval.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.02779">RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning</a>, algorithm: RL^2</p> <ul> <li> <p>Benefits from their prior knowledge about the world.</p> </li> <li> <p>The algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (“slow”) RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the “fast” RL algorithm on the current (previously unseen) MDP.</p> </li> <li> <p>Bayesian reinforcement learning provides a solid framework for incorporating prior knowledge into the learning process.</p> </li> <li> <p>Views the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms.</p> </li> <li> <p>Since the underlying MDP changes across trials, as long as different strategies are required for different MDPs, the agent must act differently according to its belief over which MDP it is currently in. Hence, the agent is forced to integrate all the information it has received, including past actions, rewards, and termination flags, and adapt its strategy continually. Hence, we have set up an end-to-end optimization process, where the agent is encouraged to learn a “fast” reinforcement learning algorithm.</p> </li> <li> <p>it receives the tuple \((s, a, r, d)\) as input, which is embedded using a function \(\phi(s, a, r, d)\) and provided as input to an RNN. To alleviate the difficulty of training RNNs due to vanishing and exploding gradients, they use Gated Recurrent Units (GRUs) which have been demonstrated to have good empirical performance. The output of the GRU is fed to a fully connected layer followed by a softmax function, which forms the distribution over actions.</p> </li> <li> <p>Supervised learning vs reinforcement learning: agent must not only learn to exploit existing information, but also learn to explore, a problem that is usually not a factor in supervised learning.</p> </li> <li> <p>The “fast” RL algorithm is a computation whose state is stored in the RNN activations, and the RNN’s weights are learned by a general-purpose “slow” reinforcement learning algorithm.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1705.08439">Thinking Fast and Slow with Deep Learning and Tree Search</a>, algorithm: ExIt</p> <ul> <li> <p>Decomposes the problem into separate planning and generalization tasks.</p> </li> <li> <p>Planning new policies is performed by tree search, while a deep neural network generalizes those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans.</p> </li> <li> <p>According to dual-process theory, human reasoning consists of two different kinds of thinking. System 1 is a fast, unconscious and automatic mode of thought, also known as intuition or heuristic process. System 2, an evolutionarily recent process unique to humans, is a slow, conscious, explicit and rule-based mode of reasoning.</p> </li> <li> <p>In deep RL algorithms such as REINFORCE and DQN, neural networks make action selections with no lookahead; this is analogous to System 1. Unlike human intuition, their training does not benefit from a ‘System 2’ to suggest strong policies. In this paper, they present Expert Iteration (EXIT), which uses a Tree Search as an analogue of System 2; this assists the training of the neural network.</p> </li> <li> <p>In Imitation Learning (IL), we attempt to solve the MDP by mimicking an expert policy \(\pi^*\) that has been provided. Experts can arise from observing humans completing a task, or, in the context of structured prediction, calculated from labelled training data. The policy we learn through this mimicry is referred to as the apprentice policy.</p> </li> <li> <p>Compared to IL techniques, Expert Iteration (EXIT) is enriched by an expert improvement step. Improving the expert player and then resolving the Imitation Learning problem allows us to exploit the fast convergence properties of Imitation Learning even in contexts where no strong player was originally known, including when learning tabula rasa.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a>, algorithm: AlphaZero</p> <ul> <li> <p>AlphaGo Zero algorithm achieved superhuman performance in the game of Go, by representing Go knowledge using deep convolutional neural networks, trained solely by reinforcement learning from games of self-play.</p> </li> <li> <p>AlphaGo Zero estimates and optimizes the probability of winning, assuming binary win/loss outcomes. AlphaZero instead estimates and optimizes the expected outcome, taking account of draws or potentially other outcomes.</p> </li> <li> <p>The rules of Go are invariant to rotation and reflection. This fact was exploited in AlphaGo and AlphaGo Zero in two ways. First, training data was augmented by generating 8 symmetries for each position. Second, during MCTS, board positions were transformed using a randomly selected rotation or reflection before being evaluated by the neural network, so that the Monte- Carlo evaluation is averaged over different biases. The rules of chess and shogi are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does not transform the board position during MCTS.</p> </li> <li> <p>In AlphaGo Zero, self-play games were generated by the best player from all previous iterations. After each iteration of training, the performance of the new player was measured against the best player; if it won by a margin of 55% then it replaced the best player and self-play games were subsequently generated by this new player. In contrast, AlphaZero simply maintains a single neural network that is updated continually, rather than waiting for an iteration to complete. Self-play games are generated by using the latest parameters for this neural network, omitting the evaluation step and the selection of best player.</p> </li> <li> <p>AlphaGo Zero tuned the hyper-parameter of its search by Bayesian optimization. In Alp- haZero we reuse the same hyper-parameters for all games without game-specific tuning.</p> </li> <li> <p>Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialized parameters, using 5,000 first-generation TPUs to generate self-play games and 64 second-generation TPUs to train the neural networks.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1809.01999">Recurrent World Models Facilitate Policy Evolution</a>, algorithm: World Models</p> <ul> <li> <p>A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model’s extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments.</p> </li> <li> <p>Trains the agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment.</p> </li> <li> <p>Model M will be a large RNN that learns to predict the future given the past in an unsupervised manner. M’s internal representations of memories of past observations and actions are perceived and exploited by another NN called the controller (C) which learns through RL to perform some task without a teacher. A small and simple C limits C’s credit assignment problem to a comparatively small search space, without sacrificing the capacity and expressiveness of the large and complex M.</p> </li> <li> <p>To overcome the problem of an agent exploiting imperfections of the generated environments, they adjust a temperature parameter of M to control the amount of uncertainty of the generated environments. They train C inside of a noisier and more uncertain version of its generated environment, and demonstrate that this approach helps prevent C from taking advantage of the imperfections of M.</p> </li> <li> <p>M is only an approximate probabilistic model of the environment, it will occasionally generate trajectories that do not follow the laws governing the actual environment.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1809.05214">Model-Based Reinforcement Learning via Meta-Policy Optimization</a>, algorithm: MB-MPO</p> <ul> <li> <p>Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step.</p> </li> <li> <p>Learning dynamics models can be done in a sample efficient way since they are trained with standard supervised learning techniques, allowing the use of off-policy data.</p> </li> <li> <p>Accurate dynamics models can often be far more complex than good policies.</p> </li> <li> <p>Model-bias: Model-based approaches tend to rely on accurate (learned) dynamics models to solve a task. If the dynamics model is not sufficiently precise, the policy optimization is prone to overfit on the deficiencies of the model, leading to suboptimal behavior or even to catastrophic failures.</p> </li> <li> <p>Learning an ensemble of dynamics models and framing the policy optimization step as a meta-learning problem. Meta-learning, in the context of RL, aims to learn a policy that adapts fast to new tasks or environments.</p> </li> <li> <p>Using the models as learned simulators, MB-MPO learns a policy that can be quickly adapted to any of the fitted dynamics models with one gradient step.</p> </li> <li> <p>MB-MPO learns a robust policy in the regions where the models agree, and an adaptive one where the models yield substantially different predictions.</p> </li> <li> <p>Current meta-learning algorithms can be classified in three categories. One approach involves training a recurrent or memory-augmented network that ingests a training dataset and outputs the parameters of a learner model. Another set of methods feeds the dataset followed by the test data into a recurrent model that outputs the predictions for the test inputs. The last category embeds the structure of optimization problems into the meta-learning algorithm.</p> </li> <li> <p>While model-free RL does not explicitly model state transitions, model-based RL methods learn the transition distribution, also known as dynamics model, from the observed transitions.</p> </li> <li> <p>MB-MPO frames model-based RL as meta-learning a policy on a distribution of dynamic models, advocates to maximize the policy adaptation, instead of robustness, when models disagree.</p> </li> <li> <p>First, they initialize the models and the policy with different random weights. Then, they proceed to the data collection step. In the first iteration, a uniform random controller is used to collect data from the real-world, which is stored in a buffer D. At subsequent iterations, trajectories from the real-world are collected with the adapted policies \(\{\pi_{\theta_1'} , ..., \pi_{\theta_K '} \}\), and then aggregated with the trajectories from previous iterations. The models are trained with the aggregated real-environment samples.</p> </li> <li> <p>The algorithm proceeds by imagining trajectories from each the ensemble of models \(\{f_{\phi_1} , ..., f_{\phi_K} \}\) using the policy \(\pi_\theta\) . These trajectories are are used to perform the inner adaptation policy gradient step, yielding the adapted policies \(\{\pi_{\theta_1'} , ..., \pi_{\theta_K'} \}\). Finally, they generate imaginary trajectories using the adapted policies \(\pi_{\theta_k'}\) and models \(f_{\phi_k}\) , and optimize the policy towards the meta-objective.</p> </li> </ul> </li> <li> <p><a href="https://openreview.net/forum?id=SJJinbWRZ&amp;noteId=SJJinbWRZ">Model-Ensemble Trust-Region Policy Optimization</a>, algorithm: ME-TRPO</p> <ul> <li> <p>Uses an ensemble of models to maintain the model uncertainty and regularize the learning process to overcome instability in training which is caused by the learned policy that tends to exploit regions where insufficient data is available for the model to be learned.</p> </li> <li> <p>Shows that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time.</p> </li> <li> <p>The standard approach for model-based reinforcement learning alternates between model learning and policy optimization. In the model learning stage, samples are collected from interaction with the environment, and supervised learning is used to fit a dynamics model to the observations. In the policy optimization stage, the learned model is used to search for an improved policy.</p> </li> <li> <p>During model learning, they differentiate the neural networks by varying their weight initialization and training input sequences. Then, during policy learning, they regularize the policy updates by combining the gradients from the imagined stochastic roll-outs.</p> </li> <li> <p>Standard model-based techniques require differentiating through the model over many time steps, a procedure known as backpropagation through time (BPTT). It is well-known in the literature that BPTT can lead to exploding and vanishing gradients.</p> </li> <li> <p>Proposes to use likelihood ratio methods instead of BPTT to estimate the gradient, which only make use of the model as a simulator rather than for direct gradient computation. In particular, they use Trust Region Policy Optimization (TRPO), which imposes a trust region constraint on the policy to further stabilize learning.</p> </li> <li> <p>The reward function is known but the transition function is unknown.</p> </li> <li> <p>ME-TRPO combines three modifications to the vanilla approach. First, they fit a set of dynamics models \(\{f_{\phi_1} , . . . , f_{\phi_K}\}\) (termed a model ensemble) using the same real world data. These models are trained via standard supervised learning, and they only differ by the initial weights and the order in which mini-batches are sampled. Second, they use Trust Region Policy Optimization (TRPO) to optimize the policy over the model ensemble. Third, they use the model ensemble to monitor the policy’s performance on validation data, and stops the current iteration when the policy stops improving.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1807.01675">Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</a>, algorithm: STEVE</p> <ul> <li> <p>By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors.</p> </li> <li> <p>Interpolates between many different horizon lengths, favoring those whose estimates have lower uncertainty, and thus lower error. To compute the interpolated target, they replace both the model and Q-function with ensembles, approximating the uncertainty of an estimate by computing its variance under samples from the ensemble.</p> </li> <li> <p>Uses DDPG as the base learning algorithm, but their technique is generally applicable to other methods that use TD objectives.</p> </li> <li> <p>Complex environments require much smaller rollout horizon H, which limits the effectiveness of the approach.</p> </li> <li> <p>From a single rollout of \(H\) timesteps, they can compute \(H+1\) distinct candidate targets by considering rollouts of various horizon lengths: \(T^{MVE_0},T^{MVE_1},T^{MVE_2},...,T^{MVE_H}\). Standard TD learning uses \(T^{MVE_0}\) as the target, while MVE uses \(T^{MVE_H}\) as the target. They propose interpolating all of the candidate targets to produce a target which is better than any individual.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1803.00101">Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning</a>, algorithm: MVE</p> <ul> <li> <p>‌Model-based value expansion controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, they improve value estimation, which, in turn, reduces the sample complexity of learning.</p> </li> <li> <p>Model-based (MB) methods can quickly arrive at near-optimal control with learned models under fairly restricted dynamics classes. In settings with nonlinear dynamics, fundamental issues arise with the MB approach: complex dynamics demand high-capacity models, which in turn are prone to overfitting in precisely those low-data regimes where they are most needed.</p> </li> <li> <p>Reduces sample complexity while supporting complex non-linear dynamics by combining MB and MF learning techniques through disciplined model use for value estimation.</p> </li> <li> <p>Model-based value expansion (MVE): a hybrid algorithm that uses a dynamics model to simulate the short-term horizon H and Q-learning to estimate the long-term value beyond the simulation horizon. This improves Q-learning by providing higher-quality target values for training.</p> </li> <li> <p>MVE offers a single, simple, and adjustable notion of model trust (H), and fully utilizes the model to that extent.</p> </li> <li> <p>MVE also demonstrates that state dynamics prediction enables on-policy imagination via the TD-k trick starting from off-policy data.</p> </li> <li> <p>To deal with sparse reward signals, it is important to consider exploration with the model, not just refinement of value estimates.</p> </li> <li> <p>MVE forms TD targets by combining a short term value estimate formed by unrolling the model dynamics and a long term value estimate using the learned \(Q^\pi_\theta−\) function.</p> </li> <li> <p>Replaces the standard Q-learning target with an improved target, computed by rolling the learned model out for \(H\) steps.</p> </li> <li> <p>Relies on task-specific tuning of the rollout horizon \(H\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1708.02596">Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</a>, algorithm: MBMF</p> <ul> <li> <p>Medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks.</p> </li> <li> <p>Uses deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods.</p> </li> <li> <p>Although such model-based methods are drastically more sample efficient and more flexible than task-specific policies learned with model-free reinforcement learning, their asymptotic performance is usually worse than model-free learners due to model bias. To address this issue, they use their model-based algorithm to initialize a model-free learner.</p> </li> <li> <p>The learned model-based controller provides good rollouts, which enable supervised initialization of a policy that can then be fine-tuned with model-free algorithms, such as policy gradients.</p> </li> <li> <p>Section IV - A: how to learn the dynamics function which is \(f(s_t, a_t)\) and outputs the next state \(s_{t+1}\). This function can be difficult to learn when the states \(s_t\) and \(s_{t+1}\) are too similar and the action has seemingly little effect on the output; this difficulty becomes more pronounced as the time between states \(\Delta t\) becomes smaller and the state differences do not indicate the underlying dynamics well. They overcome this issue by instead learning a dynamics function that predicts the change in state st over the time step duration of \(\Delta t\). Thus, the predicted next state is as follows: \(s_{t+1} = s_t + f(s_t , a_t)\). Note that increasing this \(\Delta t\) increases the information available from each data point, and can help with not only dynamics learning but also with planning using the learned dynamics model.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.06203">Imagination-Augmented Agents for Deep Reinforcement Learning</a>, algorithm: I2A</p> <ul> <li> <p>Combines model-free and model-based aspects.</p> </li> <li> <p>I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks.</p> </li> <li> <p>Model-free approaches usually require large amounts of training data and the resulting policies do not readily generalize to novel tasks in the same environment, as they lack the behavioral flexibility constitutive of general intelligence.</p> </li> <li> <p>Uses approximate environment models by “learning to interpret” their imperfect predictions.</p> </li> <li> <p>Allows the agent to benefit from model-based imagination without the pitfalls of conventional model-based planning.</p> </li> <li> <p>A key issue addressed by I2As is that a learned model cannot be assumed to be perfect; it might sometimes make erroneous or nonsensical predictions. They therefore do not want to rely solely on predicted rewards (or values predicted from predicted states), as is often done in classical planning.</p> </li> <li> <p>It takes information about present and imagines possible futures, and chooses the one with the highest reward.</p> </li> <li> <p>Offloads all uncertainty estimation and model use into an implicit neural network training process, inheriting the inefficiency of model-free methods.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1806.01822">Relational Recurrent Neural Networks</a>, algorithm: RMC</p> <ul> <li> <p>Proposes that it is fruitful to consider memory interactions along with storage and retrieval.</p> </li> <li> <p>Hypothesizes that such a bias may allow a model to better understand how memories are related, and hence may give it a better capacity for relational reasoning over time.</p> </li> <li> <p>RMC uses multi-head dot product attention to allow memories to interact with each other.</p> </li> <li> <p>Relational reasoning is the process of understanding the ways in which entities are connected and using this understanding to accomplish some higher order goal.</p> </li> <li> <p>Applies attention between memories at a single time step, and not across all previous representations computed from all previous observations.</p> </li> <li> <p>Employs multi-head dot product attention (MHDPA), where each memory will attend over all of the other memories, and will update its content based on the attended information.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1702.08360">Neural Map: Structured Memory for Deep Reinforcement Learning</a>, algorithm: Neural Map</p> <ul> <li> <p>Neural map is a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with.</p> </li> <li> <p>Neural networks that utilized external memories can be distinguished along two main axis: memories with write operators and those without. Writeless external memory systems, often referred to as “Memory Networks” typically fix which memories are stored. For example, at each time step, the memory network would store the past M states seen in an environment. What is learnt by the network is therefore how to access or read from this fixed memory pool, rather than what contents to store within it.</p> </li> <li> <p>The memory network introduces two main disadvantages. The first disadvantage is that a potentially significant amount of redundant information could be stored. The second disadvantage is that a domain expert must choose what to store in the memory, e.g. for the DRL agent, the expert must set M to a value that is larger than the time horizon of the currently considered task.</p> </li> <li> <p>On the other hand, external neural memories having write operations are potentially far more efficient, since they can learn to store salient information for unbounded time steps and ignore any other useless information, without explicitly needing any a priori knowledge on what to store.</p> </li> <li> <p>Neural Map uses an adaptable write operation and so its size and computational cost does not grow with the time horizon of the environment as it does with memory networks. Also, it imposes a particular inductive bias on the write operation so that it is 1) well suited to 3D environments where navigation is a core component of successful behaviours, and 2) uses a sparse write operation that prevents frequent overwriting of memory locations that can occur with NTMs and DNCs.</p> </li> <li> <p>Combination of REINFORCE with value function baseline is commonly termed the “Actor-Critic” algorithm.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01988">Neural Episodic Control</a>, algorithm: NEC</p> <ul> <li> <p>Agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function.</p> </li> <li> <p>Why learning is slow:</p> <ul> <li> <p>Stochastic gradient descent optimisation requires the use of small learning rates.</p> </li> <li> <p>Environments with a sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero. This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by an unknown number.</p> </li> <li> <p>Reward signal propagation by value-bootstrapping techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment.</p> </li> </ul> </li> <li> <p>NEC is able to rapidly latch onto highly successful strategies as soon as they are experienced, instead of waiting for many steps of optimisation (e.g., stochastic gradient descent) as is the case with DQN and A3C.</p> </li> <li> <p>The semi-tabular representation is an append-only memory that binds slow-changing keys to fast updating values and uses a context-based lookup on the keys to retrieve useful values during action selection by the agent.</p> </li> <li> <p>Values retrieved from the memory can be updated much faster than the rest of the deep neural network.</p> </li> <li> <p>The architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, they elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to where the memory is wiped at the end of each episode).</p> </li> <li> <p>Differentiable Neural Dictionary(DND): For each action \(a \in A\), NEC has a simple memory module \(Ma = (Ka, Va)\), where \(Ka\) and \(Va\) are dynamically sized arrays of vectors, each containing the same number of vectors. The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs.</p> </li> <li> <p>The pixel state s is processed by a convolutional neural network to produce a key h. The key h is then used to lookup a value from the DND, yielding weights wi in the process for each element of the memory arrays. Finally, the output is a weighted sum of the values in the DND. The values in the DND, in the case of an NEC agent, are the Q values corresponding to the state that originally resulted in the corresponding key-value pair to be written to the memory. Thus this architecture produces an estimate of \(Q(s, a)\) for a single given action a.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.04460">Model-Free Episodic Control</a>, algorithm: MFEC</p> <ul> <li> <p>Addresses the question of how to emulate such fast learning abilities in a machine—without any domain-specific prior knowledge.</p> </li> <li> <p>QEC (s, a): Each entry contains the highest return ever obtained by taking action a from state s. It estimates the highest potential return for a given state and action, based upon the states, rewards and actions seen.</p> </li> <li> <p>Tabular RL methods suffer from two key deficiencies: firstly, for large problems they consume a large amount of memory, and secondly, they lack a way to generalise across similar states. To address the first problem, they limit the size of the table by removing the least recently updated entry once a maximum size has been reached. Such forgetting of older, less frequently accessed memories also occurs in the brain. In large scale RL problems (such as real life) novel states are common; the real world, in general, also has this property. They address the problem of what to do in novel states and how to generalise values across common experiences by taking QEC to be a non-parametric nearest-neighbours model. For states that have never been visited, QEC is approximated by averaging the value of the k nearest states.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1805.08296">Data-Efficient Hierarchical Reinforcement Learning</a>, algorithm: HIRO</p> <ul> <li> <p>To address efficiency, proposes to use off-policy experience for both higher- and lower-level training. This allows HIRO to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms.</p> </li> <li> <p>HIRO: a hierarchical two-layer structure, with a lower-level policy \(\mu_{lo}\) and a higher-level policy \(\mu_{hi}\).</p> </li> <li> <p>The higher-level policy operates at a coarser layer of abstraction and sets goals to the lower-level policy, which correspond directly to states that the lower-level policy attempts to reach.</p> </li> <li> <p>At step t, the higher-level policy produces a goal gt, indicating its desire for the lower-level agent to take actions that yield it an observation \(st+c\) that is close to \(st + gt\) .</p> </li> <li> <p>HRL methods to be applicable to real-world settings, they must be sample-efficient, and off-policy algorithms (often based on some variant of Q-function learning) generally exhibit substantially better sample efficiency than on-policy actor-critic or policy gradient variants.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01161">FeUdal Networks for Hierarchical Reinforcement Learning</a>, algorithm: Feudal Networks</p> <ul> <li> <p>Employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment.</p> </li> <li> <p>key contributions:</p> <ul> <li> <p>A consistent, end-to-end differentiable model that embodies and generalizes the principles of feudal reinforcement learning(FRL).</p> </li> <li> <p>A novel, approximate transition policy gradient update for training the Manager, which exploits the semantic meaning of the goals it produces.</p> </li> <li> <p>The use of goals that are directional rather than absolute in nature.</p> </li> <li> <p>A novel RNN design for the Manager – a dilated LSTM – which extends the longevity of the recurrent state memories and allows gradients to flow through large hops in time, enabling effective back-propagation through hundreds of steps.</p> </li> </ul> </li> <li> <p>The top level produces a meaningful and explicit goal for the bottom level to achieve. Sub-goals emerge as directions in the latent state- space and are naturally diverse.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.04695">Strategic Attentive Writer for Learning Macro-Actions</a>, algorithm: STRAW</p> <ul> <li> <p>Proposes a new deep recurrent neural network architecture, dubbed STRategic Attentive Writer (STRAW), that is capable of learning macro-actions in a reinforcement learning setting.</p> </li> <li> <p>Macro-actions enable both structured exploration and economic computation.</p> </li> <li> <p>STRAW maintains a multi-step action plan. STRAW periodically updates the plan based on observations and commits to the plan between the replanning decision points.</p> </li> <li> <p>One observation can generate a whole sequence of outputs if it is informative enough.</p> </li> <li> <p>Facilitates structured exploration in reinforcement learning – as the network learns meaningful action patterns it can use them to make longer exploratory steps in the state space.</p> </li> <li> <p>Since the model does not need to process observations while it is committed its action plan, it learns to allocate computation to key moments thereby freeing up resources when the plan is being followed.</p> </li> <li> <p>Macro-action is a particular, simpler instance of options, where the action sequence (or a distribution over them) is decided at the time the macro-action is initiated.</p> </li> <li> <p>STRAW learns macro-actions and a policy over them in an end-to-end fashion from only the environment’s reward signal and without resorting to explicit pseudo-rewards or hand-crafted subgoals.</p> </li> <li> <p>STRAW is a deep recurrent neural network with two modules. The first module translates environment observations into an action-plan – a state variable which represents an explicit stochastic plan of future actions. STRAW generates macro-actions by committing to the action-plan and following it without updating for a number of steps. The second module maintains commitment-plan – a state variable that determines at which step the network terminates a macro-action and updates the action-plan.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.01495">Hindsight Experience Replay</a>, algorithm: Hindsight Experience Replay (HER)</p> <ul> <li> <p>The necessity of cost engineering limits the applicability of RL in the real world because it requires both RL expertise and domain-specific knowledge.</p> </li> <li> <p>The approach is based on training universal policies which takes as input not only the current state, but also a goal state.</p> </li> <li> <p>The pivotal idea behind HER is to replay each episode with a different goal than the one the agent was trying to achieve.</p> </li> <li> <p>Shows that training an agent to perform multiple tasks can be easier than training it to perform only one task.</p> </li> <li> <p>HER: after experiencing some episode \(s_0, s_1, ..., s_T\) they store in the replay buffer every transition \(s_t \rightarrow s_{t + 1}\) not only with the original goal used for this episode but also with a subset of other goals.</p> </li> <li> <p>Not only does HER learn with extremely sparse rewards, in their experiments it also performs better with sparse rewards than with shaped ones. These results are indicative of the practical challenges with reward shaping, and that shaped rewards would often constitute a compromise on the metric they truly care about (such as binary success/failure).</p> </li> </ul> </li> <li> <p><a href="https://openreview.net/forum?id=rk07ZXZRb&amp;noteId=rk07ZXZRb">Learning an Embedding Space for Transferable Robot Skills</a></p> <ul> <li> <p>Allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks.</p> </li> <li> <p>Recent RL advances learn solutions from scratch for every task. Not only this is inefficient and constrains the difficulty of the tasks that can be solved, but also it limits the versatility and adaptivity of the systems that can be built.</p> </li> <li> <p>The main contribution of is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients.</p> </li> <li> <p>Desires an embedding space, in which solutions to different, potentially orthogonal, tasks can be represented.</p> </li> <li> <p>Aims to learn a skill embedding space, in which different embedding vectors that are “close” to each other in the embedding space correspond to distinct solutions to the same task.</p> </li> <li> <p>Given the state and action trace of an executed skill, it should be possible to identify the embedding vector that gave rise to the solution: i.e. derive a new skill by re-combining a diversified library of existing skills.</p> </li> <li> <p>For the policy to learn a diverse set of skills instead of just T separate solutions (one per task), they endow it with a task-conditional latent variable z. With this latent variable, which they also refer to as “skill embedding”, the policy is able to represent a distribution over skills for each task and to share these across tasks.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.07907">Mutual Alignment Transfer Learning</a>, algorithm: MATL</p> <ul> <li> <p>Harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa.</p> </li> <li> <p>Real world applications of RL present a significant challenge to the reinforcement learning paradigm as it is constrained to learn from comparatively expensive and slow task executions.</p> </li> <li> <p>As policies trained via reinforcement learning will learn to exploit the specific characteristics of a system – optimizing for mastery instead of generality – a policy can overfit to the simulation.</p> </li> <li> <p>Guides the exploration for both systems towards mutually aligned state distributions via auxiliary rewards.</p> </li> <li> <p>Employs an adversarial approach to train policies with additional rewards based on confusing a discriminator with respect to the originating system for state sequences visited by the agents. By guiding the target agent on the robot towards states that the potentially more proficient source agent visits in simulation, they can accelerate training.</p> </li> <li> <p>Also, the agent in simulation will be driven to explore better trajectories from states visited by the real-world policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1701.08734">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</a>, algorithm: PathNet</p> <ul> <li> <p>It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.</p> </li> <li> <p>Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropagation algorithm.</p> </li> <li> <p>Fixes the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning.</p> </li> <li> <p>Uses genetic algorithms to select a population of pathways through the neural network for replication and mutation.</p> </li> <li> <p>PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.03300">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a>, algorithm: IU Agent</p> <ul> <li> <p>Learns to solve many tasks simultaneously and faster than agents that target a single task at-a-time comparing with DDPG.</p> </li> <li> <p>The architecture enables the agent to attend to one task on-policy, while unintentionally learning to solve many other tasks off-policy. Due to the fact that multiple policies are being learned at once they must necessarily be learning off-policy.</p> </li> <li> <p>Consists of two neural networks. The actor neural network has multiple-heads representing different policies with shared lower-level representations. The critic network represents several state-action value functions, sharing a common representation for the observations.</p> </li> <li> <p>Refers to the task whose behavior the agent follows during training as the intentional task, and to the remaining tasks as unintentional.</p> </li> <li> <p>The experiments demonstrate that when acting according to the policy associated with one of the hardest tasks, they are able to learn all other tasks off-policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.05397">Reinforcement Learning with Unsupervised Auxiliary Tasks</a>, algorithm: UNREAL</p> <ul> <li> <p>Hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented.</p> </li> <li> <p>Predicts and controls features of the sensorimotor stream, by treating them as pseudo- rewards for reinforcement learning.</p> </li> <li> <p>Uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards.</p> </li> <li> <p>Both the auxiliary control and auxiliary prediction tasks share the convolutional neural network and LSTM that the base agent uses to act.</p> </li> <li> <p>The auxiliary control tasks are defined as additional pseudo-reward functions in the environment the agent is interacting with.</p> </li> <li> <p>Changes in the perceptual stream often correspond to important events in an environment. They train agents that learn a separate policy for maximally changing the pixels in each cell of an n × n non-overlapping grid placed over the input image. They refer to these auxiliary tasks as pixel control.</p> </li> <li> <p>The policy or value networks of an agent learn to extract task- relevant high-level features of the environment, they can be useful quantities for the agent to learn to control. Hence, the activation of any hidden unit of the agent’s neural network can itself be an auxiliary reward. They train agents that learn a separate policy for maximally activating each of the units in a specific hidden layer. They refer to these tasks as feature control.</p> </li> <li> <p>Reward prediction auxiliary task: predicting the onset of immediate reward given some historical context.</p> </li> <li> <p>The auxiliary tasks are trained on very recent sequences of experience that are stored and randomly sampled; these sequences may be prioritised(in this case according to immediate rewards).</p> </li> <li> <p>The auxiliary control tasks (pixel changes and simple network features) are shown to enable the A3C agent to learn to achieve the scalar reward faster in domains where the action-space is discrete.</p> </li> </ul> </li> <li> <p><a href="http://proceedings.mlr.press/v37/schaul15.pdf">Universal Value Function Approximators</a>, algorithm: UVFA</p> <ul> <li> <p>Introduces universal value function approximators (UVFAs) \(V (s, g; \theta)\) that generalise not just over states s but also over goals g.</p> </li> <li> <p>The goal space often contains just as much structure as the state space.</p> </li> <li> <p>By universal, they mean that the value function can generalise to any goal g in a set G of possible goals.</p> </li> <li> <p>UVFAs can exploit two kinds of structure between goals: similarity encoded a priori in the goal representations g, and the structure in the induced value functions discovered bottom-up.</p> </li> <li> <p>The complexity of UVFA learning does not depend on the number of demons but on the inherent domain complexity.</p> </li> <li> <p>Introduces a novel factorization approach that decomposes the regression into two stages. They view the data as a sparse table of values that contains one row for each observed state \(s\) and one column for each observed goal \(g\), and find a low-rank factorization of the table into state embeddings \(\phi(s)\) and goal embeddings \(\psi(g)\).</p> </li> <li> <p>Provides two algorithms for learning UVFAs directly from rewards. The first algorithm maintains a finite Horde of general value functions \(V_g(s)\), and uses these values to seed the table and hence learn a UVFA \(V (s, g; \theta)\) that generalizes to previously unseen goals. The second algorithm bootstraps directly from the value of the UVFA at successor states.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.04671">Progressive Neural Networks</a>, algorithm: Progressive Networks</p> <ul> <li> <p>Progressive networks: immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features.</p> </li> <li> <p>Progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task.</p> </li> <li> <p>Solves K independent tasks at the end of training.</p> </li> <li> <p>Accelerates learning via transfer when possible.</p> </li> <li> <p>Avoids catastrophic forgetting.</p> </li> <li> <p>Makes no assumptions about the relationship between tasks, which may in practice be orthogonal or even adversarial.</p> </li> <li> <p>Each column is trained to solve a particular Markov Decision Process (MDP), the k-th column thus defines a policy \(\pi(k)(a | s)\) taking as input a state s given by the environment, and generating probabilities over actions.</p> </li> <li> <p>A downside of the approach is the growth in number of parameters with the number of tasks.</p> </li> <li> <p>Studies in detail which features and at which depth transfer actually occurs. They explored two related methods: an intuitive, but slow method based on a perturbation analysis (APS), and a faster analytical method derived from the Fisher Information (AFS).</p> </li> <li> <p>APS: To evaluate the degree to which source columns contribute to the target task, they inject Gaussian noise at isolated points in the architecture (e.g. a given layer of a single column) and measure the impact of this perturbation on performance.</p> </li> <li> <p>AFS: By using the Fisher Information matrix, they get a local approximation to the perturbation sensitivity.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1807.10299">Variational Option Discovery Algorithms</a>, algorithm: VALOR</p> <ul> <li> <p>Highlights a tight connection between variational option discovery methods and variational autoencoders, and introduces Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection.</p> </li> <li> <p>In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories.</p> </li> <li> <p>Proposes a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent’s performance is strong enough (as measured by the decoder) on the current set of contexts.</p> </li> <li> <p>Shows that Variational Intrinsic Control (VIC) and the Diversity is All You Need (DIAYN) are specific instances of this template which decode from states instead of complete trajectories.</p> </li> <li> <p>VALOR can attain qualitatively different behavior of VIC and DIAYN because of its trajectory-centric approach, and DIAYN learns more quickly because of its denser reward signal.</p> </li> <li> <p>Learns a policy \pi where action distributions are conditioned on both the current state \(s_t\) and a context \(c\) which is sampled at the start of an episode and kept fixed throughout. The context should uniquely specify a particular mode of behavior (also called a skill). But instead of using reward functions to ground contexts to trajectories, they want the meaning of a context to be arbitrarily assigned (‘discovered’) during training.</p> </li> <li> <p>VALOR is a variational option discovery method with two key decisions about the decoder:</p> <ul> <li> <p>The decoder never sees actions. Their conception of “interesting” behaviors requires that the agent attempt to interact with the environment to achieve some change in state. If the decoder was permitted to see raw actions, the agent could signal the context directly through its actions and ignore the environment. Limiting the decoder in this way forces the agent to manipulate the environment to communicate with the decoder.</p> </li> <li> <p>Unlike in DIAYN, the decoder does not decompose as a sum of per-timestep computations.</p> </li> </ul> </li> <li> <p>VALOR has a recurrent architecture for the decoder, using a bidirectional LSTM to make sure that both the beginning and end of a trajectory are equally important.</p> </li> <li> <p>Starts training with small K (where learning is easy), and gradually increase it over time as the decoder gets stronger.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1802.06070">Diversity is All You Need: Learning Skills without a Reward Function</a>, algorithm: DIAYN</p> <ul> <li> <p>Learns useful skills without a reward function, by maximizing an information theoretic objective using a maximum entropy policy.</p> </li> <li> <p>Unsupervised discovery of skills can serve as an effective pre-training mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.</p> </li> <li> <p>A skill is a latent-conditioned policy that alters that state of the environment in a consistent way.</p> </li> <li> <p>Uses discriminability between skills as an objective.</p> </li> <li> <p>Learns skills that not only are distinguishable, but also are as diverse as possible.</p> </li> <li> <p>Proposes a simple method for using learned skills for hierarchical RL and find this methods solves challenging tasks.</p> </li> <li> <p>Because skills are learned without a priori knowledge of the task, the learned skills can be used for many different tasks.</p> </li> <li> <p>First, for skills to be useful, they want the skill to dictate the states that the agent visits. Different skills should visit different states, and hence be distinguishable. Second, they want to use states, not actions, to distinguish skills, because actions that do not affect the environment are not visible to an outside observer. For example, an outside observer cannot tell how much force a robotic arm applies when grasping a cup if the cup does not move. Finally, they encourage exploration and incentivize the skills to be as diverse as possible by learning skills that act as randomly as possible.</p> </li> <li> <p>Performs option discovery by optimizing a variational lower bound for an objective function designed to maximize mutual information between context and every state in a trajectory, while minimizing mutual information between actions and contexts conditioned on states, and maximizing entropy of the mixture policy over contexts.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.07507">Variational Intrinsic Control</a>, algorithm: VIC</p> <ul> <li> <p>Introduces two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly.</p> </li> <li> <p>Addresses the question of what intrinsic options are available to an agent in a given state?</p> </li> <li> <p>The objective of empowerment: long-term goal of the agent is to get to a state with a maximal set of available intrinsic options.</p> </li> <li> <p>The primary goal of empowerment is not to understand or predict the observations but to control the environment.</p> </li> <li> <p>Learns to represent the intrinsic control space of an agent.</p> </li> <li> <p>Data likelihood and empowerment are both information measures: likelihood measures the amount of information needed to describe data and empowerment measures the mutual information between action choices and final states.</p> </li> <li> <p>VIC is an option discovery technique based on optimizing a variational lower bound on the mutual information between the context and the final state in a trajectory, conditioned on the initial state.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1810.12894">Exploration by Random Network Distillation</a>, algorithm: RND</p> <ul> <li> <p>Introduces exploration bonus that is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network .</p> </li> <li> <p>Flexibly combines intrinsic and extrinsic rewards.</p> </li> <li> <p>Can be used with any policy optimization algorithm.</p> </li> <li> <p>Efficient to compute as it requires only a single forward pass of a neural network on a batch of experience.</p> </li> <li> <p>Predicts the output of a fixed randomly initialized neural network on the current observation.</p> </li> <li> <p>Introduces a modification of Proximal Policy Optimization (PPO) that uses two value heads for the two reward streams, to combine the exploration bonus with the extrinsic rewards. This allows the use of different discount rates for the different rewards, and combining episodic and non-episodic returns.</p> </li> <li> <p>In a tabular setting with a finite number of states one can define it to be a decreasing function of the visitation count \(nt(s)\) of the state s. In non-tabular cases it is not straightforward to produce counts, as most states will be visited at most once. One possible generalization of counts to non-tabular settings is pseudo-counts (Bellemare et al., 2016) which uses changes in state density estimates as an exploration bonus.</p> </li> <li> <p>RND involves two neural networks: a fixed and randomly initialized target network which sets the prediction problem, and a predictor network trained on data collected by the agent.</p> </li> <li> <p>Prediction errors can be attributed to a number of factors:</p> <ul> <li> <p>Amount of training data.</p> </li> <li> <p>Stochasticity.</p> </li> <li> <p>Model misspecification.</p> </li> <li> <p>Learning dynamics.</p> </li> </ul> </li> <li> <p>The distillation error could be seen as a quantification of uncertainty in predicting the constant zero function.</p> </li> <li> <p>In order to keep the rewards on a consistent scale they normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of the intrinsic returns.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1808.04355">Large-Scale Study of Curiosity-Driven Learning</a></p> <ul> <li> <p>Curiosity is a type of intrinsic reward function which uses prediction error as reward signal.</p> </li> <li> <p>Examples of intrinsic reward include “curiosity” which uses prediction error as reward signal, and “visitation counts” which discourages the agent from revisiting the same states. The idea is that these intrinsic rewards will bridge the gaps between sparse extrinsic rewards.</p> </li> <li> <p>Studies agents driven purely by intrinsic rewards.</p> </li> <li> <p>Shows that encoding observations via a random network turns out to be a simple, yet effective technique for modeling curiosity across many popular RL benchmarks. This might suggest that many popular RL video game test-beds are not as visually sophisticated as commonly thought.</p> </li> <li> <p>If the agent itself is the source of stochasticity in the environment, it can reward itself without making any actual progress.</p> </li> <li> <p>One important point is that the use of an end of episode signal, sometimes called a ‘done’, can often leak information about the true reward function. If they don’t remove the ‘done’ signal, many of the Atari games become too simple. For example, a simple strategy of giving +1 artificial reward at every time-step when the agent is alive and 0 on death is sufficient to obtain a high score in some games.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1705.05363">Curiosity-driven Exploration by Self-supervised Prediction</a>, algorithm: Intrinsic Curiosity Module (ICM)</p> <ul> <li> <p>Formulates curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.</p> </li> <li> <p>Motivation/curiosity have been used to explain the need to explore the environment and discover novel states.</p> </li> <li> <p>Measuring “novelty” requires a statistical model of the distribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state \(s_{t+1}\) given the current state \(s_t\) and the action \(a_t\) executed at time t.</p> </li> <li> <p>Predicts those changes in the environment that could possibly be due to the actions of their agent or affect the agent, and ignore the rest.</p> </li> <li> <p>Curiosity helps an agent explore its environment in the quest for new knowledge. Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios.</p> </li> <li> <p>Agent is composed of two subsystems: a reward generator that outputs a curiosity-driven intrinsic reward signal and a policy that outputs a sequence of actions to maximize that reward signal.</p> </li> <li> <p>Making predictions in the raw sensory space (e.g. when st corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because it is unclear if predicting pixels is even the right objective to optimize.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01260">EX2: Exploration with Exemplar Models for Deep Reinforcement Learning</a>, algorithm: EX2</p> <ul> <li> <p>When the reward signals are rare and sparse, function approximation methods such as deep RL struggle to acquire meaningful policies.</p> </li> <li> <p>Intuitively, novel states are easier to distinguish against other states seen during training.</p> </li> <li> <p>Estimate novelty by considering how easy it is for a discriminatively trained classifier to distinguish a given state from other states seen previously.</p> </li> <li> <p>Trains exemplar models for each state that distinguish that state from all other observed states.</p> </li> <li> <p>Given a dataset \(X = \{x_1, ...x_n\}\), an exemplar model consists of a set of \(n\) classifiers or discriminators \(\{D_{x_1} , ....D_{x_n}\}\), one for each data point. Each individual discriminator \(D_{x_i}\) is trained to distinguish a single positive data point xi, the “exemplar,” from the other points in the dataset X.</p> </li> <li> <p>In GANs, the generator plays an adversarial game with the discriminator by attempting to produce indistinguishable samples in order to fool the discriminator. However, in this work, the generator is rewarded for helping the discriminator rather than fooling it, so their algorithm plays a cooperative game instead of an adversarial one.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.04717">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a>, algorithm: Hash-based Counts</p> <ul> <li> <p>It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once.</p> </li> <li> <p>States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory.</p> </li> <li> <p>Important aspects of a good hash function are, first, having appropriate granularity, and second, encoding information relevant to solving the MDP.</p> </li> <li> <p>The sample complexity can grow exponentially(with state space size) in tasks with sparse rewards.</p> </li> <li> <p>Discretizes the state space with a hash function and apply a bonus based on the state-visitation count.</p> </li> <li> <p>The agent is trained with rewards \((r + r+)\), while performance is evaluated as the sum of rewards without bonuses.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.01310">Count-Based Exploration with Neural Density Models</a>, algorithm: PixelCNN-based Pseudocounts</p> <ul> <li> <p>Considers two questions left open by CTS-based Pseudocounts work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration?</p> </li> <li> <p>Answers the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count.</p> </li> <li> <p>The mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings.</p> </li> <li> <p>Trains the density model completely online on the sequence of experienced states.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1606.01868">Unifying Count-Based Exploration and Intrinsic Motivation</a>, algorithm: CTS-based Pseudocounts</p> <ul> <li> <p>In a tabular setting, agent’s uncertainty over the environment’s reward and transition functions can be quantified using confidence intervals derived from Chernoff bounds, or inferred from a posterior over the environment parameters.</p> </li> <li> <p>Count-based exploration methods directly use visit counts to guide an agent’s behaviour towards reducing uncertainty.</p> </li> <li> <p>In spite of their pleasant theoretical guarantees, count-based methods have not played a role in the contemporary successes of reinforcement learning. The issue is that visit counts are not directly useful in large domains, where states are rarely visited more than once.</p> </li> <li> <p>Pseudo-count estimates the uncertainty of an agent’s knowledge.</p> </li> <li> <p>Derives the pseudo-count from a density model over the state space to generalize count-based exploration to non-tabular reinforcement learning.</p> </li> <li> <p>The model should be learning-positive, i.e. the probability assigned to a state x should increase with training.</p> </li> <li> <p>It should be trained on- line, using each sample exactly once.</p> </li> <li> <p>The effective model step-size should decay at a rate of \(n^{−1}\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1605.09674">VIME: Variational Information Maximizing Exploration</a>, algorithm: VIME</p> <ul> <li> <p>Maximizes information gain about the agent’s belief of environment dynamics.</p> </li> <li> <p>Modifies the MDP reward function.</p> </li> <li> <p>Agents are encouraged to take actions that result in states they deem surprising, i.e., states that cause large updates to the dynamics model distribution.</p> </li> <li> <p>Variational inference is used to approximate the posterior distribution of a Bayesian neural network that represents the environment dynamics.</p> </li> <li> <p>Using information gain in this learned dynamics model as intrinsic rewards allows the agent to optimize for both external reward and intrinsic surprise simultaneously.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1703.03864">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, algorithm: ES</p> <ul> <li> <p>Needs to communicate scalars, making it possible to scale to over a thousand parallel workers.</p> </li> <li> <p>It is invariant to action frequency and delayed rewards.</p> </li> <li> <p>Tolerant of extremely long horizons.</p> </li> <li> <p>Does not need temporal discounting or value function approximation.</p> </li> <li> <p>Uses of virtual batch normalization and other reparameterizations of the neural network policy greatly improve the reliability of evolution strategies.</p> </li> <li> <p>The data efficiency of evolution strategies was surprisingly good.</p> </li> <li> <p>Exhibits better exploration behaviour than policy gradient methods like TRPO.</p> </li> <li> <p>Black-box optimization methods have several highly attractive properties: indifference to the distribution of rewards (sparse or dense), no need for backpropagating gradients, and tolerance of potentially arbitrarily long time horizons.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1704.06440">Equivalence Between Policy Gradients and Soft Q-Learning</a></p> <ul> <li> <p>Q-learning methods are not well-understood why they work, since empirically, the Q-values they estimate are very inaccurate.</p> </li> <li> <p>“Soft” (entropy-regularized) Q-learning is exactly equivalent to a policy gradient method.</p> </li> <li> <p>In both cases, if the return following an action at is high, then that action is reinforced: in policy gradient methods, the probability \(\pi (a_t |s_t)\) is increased; whereas in Q-learning methods, the Q-value \(Q (s_t, a_t)\) is increased.</p> </li> <li> <p>Problem setting described in the paper is the bandit problem.</p> </li> </ul> </li> <li> <p><a href="http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning</a>, algorithm: IPG</p> <ul> <li> <p>Examines approaches to merging on- and off-policy updates for deep reinforcement learning.</p> </li> <li> <p>On-policy learning: One of the simplest ways to learn a neural network policy is to collect a batch of behavior wherein the policy is used to act in the world, and then compute and apply a policy gradient update from this data. This is referred to as on-policy learning because all of the updates are made using data that was collected from the trajectory distribution induced by the current policy of the agent.</p> <ul> <li>Drawbacks: Data inefficient, because they only look at each data point once.</li> </ul> </li> <li> <p>Off-policy learning: Such methods reuse samples by storing them in a memory replay buffer and train a value function or Q-function with off-policy updates.</p> <ul> <li>Drawbacks: This improves data efficiency, but often at a cost in stability and ease of use.</li> </ul> </li> <li> <p>Mixes likelihood ratio gradient with \(\hat{Q}\), which provides unbiased but high-variance gradient estimation, and deterministic gradient through an off-policy fitted critic \(Q_w\), which provides low-variance but biased gradients.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1704.04651">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a>, algorithm: Reactor</p> <ul> <li> <p>First contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting.</p> </li> <li> <p>Introduces the β-leave-one-out policy gradient algorithm, which improves the trade-off between variance and bias by using action values as a baseline.</p> </li> <li> <p>Exploits the temporal locality of neighboring observations for more efficient replay prioritization.</p> </li> <li> <p>Combines the sample-efficiency of off-policy experience replay with the time-efficiency of asynchronous algorithms.</p> </li> <li> <p>The Reactor architecture represents both a policy \(\pi (a|x)\) and action-value function \(Q(x, a)\).</p> </li> <li> <p>Temporal differences are temporally correlated, with correlation decaying on average with the time-difference between two transitions.</p> </li> <li> <p>More samples are made in areas of high estimated priorities, and in the absence of weighting this would lead to overestimation of unassigned priorities.</p> </li> <li> <p>An important aspect of the architecture: an acting thread receives observations, submits actions to the environment, and stores transitions in memory, while a learning thread re-samples sequences of experiences from memory and trains on them.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.01626">Combining Policy Gradient and Q-learning</a>, algorithm: PGQL</p> <ul> <li> <p>Combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer.</p> </li> <li> <p>Considers model-free RL, where the state-transition function is not known or learned.</p> </li> <li> <p>In policy gradient techniques the policy is represented explicitly and they improve the policy by updating the parameters in the direction of the gradient of the performance.</p> </li> <li> <p>The actor refers to the policy and the critic to the estimate of the action-value function.</p> </li> <li> <p>Combines two updates to the policy, the regularized policy gradient update, and the Q-learning update.</p> </li> <li> <p>Mix some ratio of on- and off-policy gradients or update steps in order to update a policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.01891">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a>, algorithm: Trust-PCL</p> <ul> <li> <p>Under entropy regularization, the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path.</p> </li> <li> <p>By alternatively augmenting the maximum reward objective with a relative entropy regularizer, the optimal policy and values still satisfy a certain set of pathwise consistencies along any sampled trajectory.</p> </li> <li> <p>Maximizes entropy regularized expected reward while maintaining natural proximity to the previous policy.</p> </li> <li> <p>Entropy regularization helps improve exploration, while the relative entropy improves stability and allows for a faster learning rate. This combination is a key novelty.</p> </li> <li> <p>It is beneficial to learn the parameter \(\phi\) at least as fast as \(\theta\), and accordingly, given a mini-batch of episodes they perform a single gradient update on \(\theta\) and possibly multiple gradient updates on \(\phi\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1702.08892">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a>, algorithm: PCM</p> <ul> <li> <p>Identifies a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence.</p> </li> <li> <p>Use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning.</p> </li> <li> <p>Attempts to minimize the squared soft consistency error over a set of sub-trajectories \(E\).</p> </li> <li> <p>Given a fixed rollout parameter d, at each iteration, PCL samples a batch of on-policy trajectories and computes the corresponding parameter updates for each sub-trajectory of length d. Then PCL exploits off-policy trajectories by maintaining a replay buffer and applying additional updates based on a batch of episodes sampled from the buffer at each iteration. So, PCL is applicable to both on-policy and off-policy trajectories.</p> </li> <li> <p>Unified PCL optimizes the same objective as PCL but differs by combining the policy and value function into a single model.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1802.10031">The Mirage of Action-Dependent Baselines in Reinforcement Learning</a></p> <ul> <li> <p>Demonstrates that the state-action-dependent(like Q-Prop or Stein Control Variates) don’t result in variance reduction over a state-dependent baseline in commonly tested benchmark domains.</p> </li> <li> <p>Having bias leads to instability and sensitivity to hyperparameters.</p> </li> <li> <p>The variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline.</p> </li> <li> <p>“We emphasize that without the open-source code accompanying, this work would not be possible. Releasing the code has allowed us to present a new view on their work and to identify interesting implementation decisions for further study that the original authors may not have been aware of.”</p> </li> <li> <p>Shows that prior works actually introduce bias into the policy gradient due to subtle implementation decisions:</p> <ul> <li> <p>Applies an adaptive normalization to only some of the estimator terms, which introduces a bias.</p> </li> <li> <p>Poorly fit value function.</p> </li> <li> <p>Fitting the baseline to the current batch of data and then using the updated baseline to form the estimator results in a biased gradient.</p> </li> </ul> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1710.11198">Action-depedent Control Variates for Policy Optimization via Stein’s Identity</a>, algorithm: Stein Control Variates</p> <ul> <li> <p>The idea of the control variate method is to subtract a Monte Carlo gradient estimator by a baseline function that analytically has zero expectation.</p> </li> <li> <p>Constructs a class of Stein control variate that allows to use arbitrary baseline functions that depend on both actions and states.</p> </li> <li> <p>Does not change the expectation but can decrease the variance significantly when it is chosen properly to cancel out the variance of Q.</p> </li> <li> <p>The gradient in is taken w.r.t. the action a, no w.r.t the parameter \theta.</p> </li> <li> <p>Connect \(\bigtriangledown_a log \pi(a|s)\) to \(\bigtriangledown_\theta log \pi(a|s)\) in order to apply Stein’s identity as a control variate for policy gradient. This is possible when the policy is reparameterizable in that \(a \approx \pi_\theta(a|s)\) can be viewed as generated by \(a = f_\theta(s,\xi)\) where \(\xi\) is a random noise drawn from some distribution independently of \(\theta\).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.02247">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a>, algorithm: Q-Prop</p> <ul> <li> <p>Combines the stability of policy gradients with the efficiency of off-policy RL.</p> </li> <li> <p>Uses a Taylor expansion of the off-policy critic as a control variate.</p> </li> <li> <p>Reduces the variance of gradient estimator without adding bias.</p> </li> <li> <p>It can be easily incorporated into any policy gradient algorithm.</p> </li> <li> <p>A common choice is to estimate the value function of the state \(V_\theta(s_t)\) to use as the baseline, which provides an estimate of advantage function \(A_\theta(s_t , a_t)\).</p> </li> <li> <p>Constructs a new estimator that in practice exhibits improved sample efficiency through the inclusion of off-policy samples while preserving the stability of on-policy Monte Carlo policy gradient.</p> </li> <li> <p>A more general baseline function that linearly depends on the actions.</p> </li> <li> <p>An off-policy Q critic is trained but is used as a control variate to reduce on-policy gradient variance.</p> </li> <li> <p>Uses only on-policy samples for estimating the policy gradient.</p> </li> </ul> </li> <li> <p><a href="https://openreview.net/forum?id=ByG_3s09KX">Dopamine: A Research Framework for Deep Reinforcement Learning</a></p> <ul> <li> <p>A new research framework for deep RL that aims to support some of the RL goals diversities.</p> </li> <li> <p>Open-source, TensorFlow-based, and provides compact yet reliable implementations of some state-of-the-art deep RL agents.</p> </li> <li> <p>DQN is architecture research.</p> </li> <li> <p>Double DQN, distributional methods, prioritized experience replay are algorithmic research.</p> </li> <li> <p>Rainbow is comprehensive study.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1806.06923">Implicit Quantile Networks for Distributional Reinforcement Learning</a>, algorithm: IQN</p> <ul> <li> <p>Reparameterizes a distribution over the sample space, causing to yield an implicitly defined return distribution and give rise to a large class of risk-sensitive policies.</p> </li> <li> <p>Learns the full quantile function, a continuous map from probabilities to returns.</p> </li> <li> <p>The approximation error for the distribution is controlled by the size of the network itself, and the amount of training.</p> </li> <li> <p>Provides improved data efficiency with increasing number of samples per training update.</p> </li> <li> <p>Expands the class of policies to more fully take advantage of the learned distribution.</p> </li> <li> <p>IQN is a type of <a href="http://proceedings.mlr.press/v37/schaul15.pdf">universal value function approximator (UVFA)</a>.</p> </li> <li> <p>Samples TD errors decorrelated and the estimated action-values go from being the true mean of a mixture of n Diracs to a sample mean of the implicit distribution.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1710.10044">Distributional Reinforcement Learning with Quantile Regression</a>, algorithm: QR-DQN</p> <ul> <li> <p>Sampling probabilistically, randomness in the observed long-term return will be induced.</p> </li> <li> <p>The distribution over returns is modeled explicitly instead of only estimating the mean, which means learning the value distribution instead of the value function.</p> </li> <li> <p>Assigns fixed, uniform probabilities to N adjustable locations.</p> </li> <li> <p>Uses quantile regression to stochastically adjust the distributions’ locations so as to minimize the Wasserstein distance to a target distribution(adapts return quantiles to minimize the Wasserstein distance between the Bellman updated and current return distributions).</p> </li> <li> <p>The distribution over returns plays the central role and replaces the value function.</p> </li> <li> <p>Transposes parametrization of C51 approach by considering fixed probabilities but variable locations.</p> </li> <li> <p>Distributional Reinforcement Learning with Quantile Regression</p> </li> <li> <p>Uses a similar neural network architecture as DQN, changing the output layer to be of size \(\|A\| × N\), where \(N\) is a hyper-parameter giving the number of quantile targets.</p> </li> <li> <p>Replaces the Huber loss used by DQN with a quantile Huber loss.</p> </li> <li> <p>Replaces RMSProp with ADAM.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.06887">A Distributional Perspective on Reinforcement Learning</a>, algorithm: C51</p> <ul> <li> <p>Studies the random return Z whose expectation is the value Q.</p> </li> <li> <p>The distributional Bellman equation states that the distribution of Z is characterized by the interaction of three random variables: the reward R, the next state-action (X′, A′), and its random return Z(X′,A′).</p> </li> <li> <p>Instead of trying to minimize the error between the expected values, it tries to minimize a distributional error, which is a distance between full distributions.</p> </li> <li> <p>Proves that the distributional Bellman operator is a contraction in a maximal form of the Wasserstein metric between probability distributions.</p> </li> <li> <p>The Wasserstein metric, viewed as a loss, cannot generally be minimized using stochastic gradient methods.</p> </li> <li> <p>Approximates the distribution at each state by attaching variable(parametrized) probabilities to fixed locations.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1802.09477">Addressing Function Approximation Error in Actor-Critic Methods</a>, algorithm: TD3</p> <ul> <li> <p>Tackles the problem of overestimation bias and the accumulation of error in temporal difference method, by using double Q-learning. Double DQN doesn’t work.</p> </li> <li> <p>Target networks are critical for variance reduction by reducing the accumulation of errors.</p> </li> <li> <p>Delaying policy updates until the value estimate has converged, to overcome the the coupling of value and policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1509.02971">Continuous Control With Deep Reinforcement Learning</a>, algorithm: DDPG</p> <ul> <li> <p>Off-policy actor-critic, model-free, deterministic policy gradient on continuous action space.</p> </li> <li> <p>Actor maps states to actions, instead of outputting the probability distribution across a discrete action space.</p> </li> <li> <p>Next-state Q values are calculated with the target value network and target policy network.</p> </li> <li> <p>For discrete action spaces, exploration is selecting a random action(e.g. epsilon-greedy) by a given probability. For continuous action spaces, exploration is adding some noise to the action itself(using Ornstein-Uhlenbeck process).</p> </li> <li> <p>Soft target update: solve the Q update divergence. The target values are constrained to change slowly, improving the stability of learning.</p> </li> <li> <p>Having both a target \(\mu′\) and \(Q′\) was required to have stable targets \(y_i\) in order to consistently train the critic without divergence, which slows learning. However, in practice this was greatly outweighed by the stability of learning.</p> </li> </ul> </li> <li> <p><a href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic Policy Gradient Algorithms</a>, algorithm: DPG</p> <ul> <li> <p>Deterministic policy gradient can be estimated more efficiently than the usual stochastic policy gradient.</p> </li> <li> <p>Uses an off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory behaviour policy to address the exploration concerns.</p> </li> <li> <p>Proves that deterministic policy gradient exists and has a simple model-free form.</p> </li> <li> <p>Stochastic vs Deterministic policy gradient:</p> <ul> <li> <p>stochastic case: policy gradient integrates over both state and action spaces.</p> </li> <li> <p>deterministic case: policy gradient integrates over the state space.</p> </li> </ul> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, algorithm: SAC</p> <ul> <li> <p>Entropy is a quantity which basically determines how random a random variable should be.</p> </li> <li> <p>Includes entropy regularization in the RL problem: policy function, value function, Q function.</p> </li> <li> <p>Maximum entropy RL alters the RL objective, though the original objective can be recovered using a temperature parameter.</p> </li> <li> <p>The maximum entropy formulation provides a substantial improvement in exploration and robustness.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1611.01224">Sample Efficient Actor-Critic with Experience Replay</a>, algorithm: ACER</p> <ul> <li> <p>Introduces truncated importance sampling with bias correction, stochastic dueling network architectures, and efficient trust region policy optimization.</p> </li> <li> <p>Matches the state-of-the-art performance of deep Q-networks with prioritized replay on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.</p> </li> <li> <p>ACER may be understood as the off-policy counterpart of the A3C method.</p> </li> <li> <p>Retrace and off- policy correction, SDNs, and trust region are critical: removing any one of them leads to a clear deterioration of the performance.</p> </li> <li> <p>Mix some ratio of on- and off-policy gradients or update steps in order to update a policy.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1708.05144">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a>, algorithm: ACKTR</p> <ul> <li> <p>Optimizes both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region.</p> </li> <li> <p>K-FAC: approximates the curvature by Kronecker factorization, reducing the computation complexity of the parameter updates.</p> </li> <li> <p>Learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs.</p> </li> <li> <p>ACKTR substantially improves both sample efficiency and the final performance of the agent in the Atari environments and the MuJoCo tasks compared to the state-of-the-art on-policy actor-critic method A2C and the famous trust region optimizer TRPO.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.02286">Emergence of Locomotion Behaviours in Rich Environments</a>, algorithm: PPO-Penalty</p> <ul> <li> <p>Investigates learning complex behavior in rich environments.</p> </li> <li> <p>Locomotion: behaviors that are known for their sensitivity to the choice of reward.</p> </li> <li> <p>Environments include a wide range of obstacles with varying levels of difficulty causing the implicit curriculum learning to the agent.</p> </li> <li> <p>Uses TRPO, PPO and parallelism(like A3C).</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>, algorithm: PPO-Clip, PPO-Penalty</p> <ul> <li> <p>Compares to TRPO, simpler to implement, more general, and better sample complexity.</p> </li> <li> <p>Algorithm:</p> <ul> <li> <p>Each iteration, each of N parallel actors collect T timesteps of data.</p> </li> <li> <p>Then they construct the surrogate loss on these NT timesteps of data.</p> </li> <li> <p>Optimize it with minibatch SGD for K epochs.</p> </li> </ul> </li> <li> <p>PPO is a robust learning algorithm that requires little hyper-parameter tuning.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>, algorithm: GAE</p> <ul> <li> <p>Uses value functions to estimate how the policy should be improved.</p> </li> <li> <p>Reduces variance while maintaining a tolerable level of bias, called Generalized Advantage Estimator(GAE).</p> </li> <li> <p>Trains value functions with trust region optimization.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>, algorithm: TRPO</p> <ul> <li> <p>Uses the expected advantage value of function A to improve the current policy.</p> </li> <li> <p>Guarantees to increase the policy performance, or leave it constant when the expected advantage is zero everywhere.</p> </li> <li> <p>Applies lower bounds on the improvements of the policy to address the issue on how big of a step to take.</p> </li> <li> <p>Uses a hard constraint rather than a penalty because it is hard to choose a single value of à that performs well across di erent problems.</p> </li> <li> <p>A major drawback is that such methods are not able to exploit off-policy data and thus require a large amount of on-policy interaction with the environment, making them impractical for solving challenging real-world problems.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a>, algorithm: A3C</p> <ul> <li> <p>Uses asynchronous gradient descent for optimization of deep neural network controllers: run many agents on many instances of the environment.</p> </li> <li> <p>Experience replay drawbacks: uses more memory and computation per real interaction, and requires off-policy learning algorithms.</p> </li> <li> <p>Parallelism decorrelates agents data.</p> </li> <li> <p>A3C works well on 2D and 3D games, discrete and continuous action spaces, and can train feedforward or recurrent agents.</p> </li> <li> <p>Has threads over one CPU to do the learning.</p> </li> <li> <p>Multiple learners increase the exploration probability.</p> </li> <li> <p>A3C trains agents that have both a policy (actor) distribution \pi and a value (critic) estimate V \pi .</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>, algorithm: Rainbow DQN</p> <ul> <li> <p>Combines 6 extensions to the DQN algorithm: <a href="https://arxiv.org/abs/1509.06461">double DQN</a>, <a href="https://arxiv.org/abs/1511.05952">prioritize experience replay</a>, <a href="https://arxiv.org/abs/1511.06581">dueling network architecture</a>, <a href="https://arxiv.org/abs/1901.07510">multi-step bootstrap targets</a>, <a href="https://arxiv.org/abs/1707.06887">distributional Q-learning</a>, and <a href="https://arxiv.org/abs/1706.10295">noisy DQN</a>.</p> </li> <li> <p>SOTA, both data efficiency wise and performance wise.</p> </li> <li> <p>Does an ablation study over the 6 extensions.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a>, algorithm: PER</p> <ul> <li> <p>Agents can remember and reuse experiences from the past.</p> </li> <li> <p>Increases the replay probability of experience tuples that have a high expected learning progress.</p> </li> <li> <p>Greedy TD-error prioritization: the amount the RL agent can learn from a transition in its current state.</p> </li> <li> <p>Greedy TD-error prioritization problems: only update the transitions that are replayed, sensitive to noise spikes, and focuses on a small subset of experiences.</p> </li> <li> <p>To overcome problems, use a stochastic sampling method to interpolate greedy prioritization and uniform random sampling.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>, algorithm: Double DQN</p> <ul> <li> <p>Reduces the observed overoptimism of the DQN: decomposing the max operation in the target into action selection and action evaluation.</p> </li> <li> <p>Good side of being optimistic: helps exploration. Bad side: leads to a poorer policy.</p> </li> <li> <p>The weights of the second network are replace with the weights of the target network for the evaluation of the current greedy policy.</p> </li> <li> <p>Evaluates the greedy policy according to the online network, but using the target network to estimate its value.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>, algorithm: Dueling DQN</p> <ul> <li> <p>Uses two separate estimators: one for state value function(V), one for state-dependent action advantage function(A).</p> </li> <li> <p>Two streams representing V and A function, but using same convolutional neural network.</p> </li> <li> <p>Two streams are combined using an aggregation layer.</p> </li> <li> <p>Intuition: learning valuable states, w/o having to learn the effect of each action for each state.</p> </li> <li> <p>Clips gradients to have their norm less than or equal to 10.</p> </li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/1507.06527">Deep Recurrent Q-Learning for Partially Observable MDPs</a>, algorithm: Deep Recurrent Q-Learning</p> <ul> <li> <p>Adds RNNs to the DQNs.</p> </li> <li> <p>Replaces the first post-convolutional fully-connected layer with an LSTM.</p> </li> <li> <p>DQN agents are unable to perform well on games that require the agent to remember events from far past(k frame skipping).</p> </li> <li> <p>Uses only one single game state.</p> </li> <li> <p>Is able to handle partial observability.</p> </li> <li> <p>Partial observability: with probability p = 0.5, the screen is either fully obscured or fully revealed.</p> </li> <li> <p>Uses “Bootstrapped Sequential Updates” and “Bootstrapped Random Updates” to update RNN weights.</p> </li> </ul> </li> <li> <p><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, algorithm: DQN</p> <ul> <li> <p>Uses convolutional neural network to extract features directly from the input images, which are game states.</p> </li> <li> <p>No adjustments of the hyperparameters for each game.</p> </li> <li> <p>Model-free: no explicit construction of environment.</p> </li> <li> <p>Off-policy: learning policy and behavior policy are different, same as Q-learning.</p> </li> <li> <p>Uses experience replay: better data efficiency, decorrelate data, avoiding oscillations or divergence in parameters.</p> </li> <li> <p>Scales scores of different games to -1, 0, 1.</p> </li> <li> <p>Uses frame skipping: agent sees and selects actions of every k frame.</p> </li> <li> <p>Freezes the parameters of of the target network for a fixed time while updating the online network by gradient descent.</p> </li> </ul> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Keynotes from teh RL Key Papers of Spinning Up by OpenAI.]]></summary></entry><entry><title type="html">Convolutional Neural Network Explanation Methods</title><link href="https://modanesh.github.io/blog/Conv-Neur-Net-Explanations/" rel="alternate" type="text/html" title="Convolutional Neural Network Explanation Methods"/><published>2019-11-19T00:00:00+00:00</published><updated>2019-11-19T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/Conv-Neur-Net-Explanations</id><content type="html" xml:base="https://modanesh.github.io/blog/Conv-Neur-Net-Explanations/"><![CDATA[<h3 id="methods">Methods:</h3> <ul> <li> <p>Saliency Maps:</p> <ul> <li>Intuitively, the absolute value of the gradient indicates those input features (pixels, for image classification) that can be perturbed the least in order for the target output to change the most, with no regards for the direction of this change.</li> </ul> </li> <li> <p>Gradient Input:</p> <ul> <li>The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them feature-wise with the input itself.</li> </ul> </li> <li> <p>Integrated Gradient:</p> <ul> <li>Similarly to Gradient Input, computes the partial derivatives of the output with respect to each input feature. However, instead of evaluating the partial derivative at the provided input \(x\) only, it computes its average value while the input varies along a linear path from a baseline \(x'\) to \(x\). The baseline is defined by the user and often chosen to be zero.</li> </ul> </li> <li> <p>Layer-wise Relevance Propagation (LRP):</p> <ul> <li>Considers a quantity \(r^l_i\) , called “relevance” of unit \(i\) of layer \(l\). The algorithm starts at the output layer \(L\), assigning the relevance of the target neuron \(c\) equal to the activation of the neuron itself, and the relevance of all other neurons to zero. Then it proceeds layer by layer, redistributing the prediction score \(S_i\) until the input layer is reached.</li> </ul> </li> <li> <p>DeepLIFT:</p> <ul> <li>Similar to LRP, but the difference is that there is a baseline. The relevance of unit \(i\) of layer \(l\) for input \(x\) is subtracted by the relevance of unit \(i\) of layer \(l\) for input \(x'\) (baseline). Also, for redistributing the prediction score \(S_i\), the baseline plays a role.</li> </ul> </li> </ul> <h3 id="resources">Resources:</h3> <ul> <li> <p><a href="https://psturmfels.github.io/VisualizingExpectedGradients/">Visualizing Expected Gradients</a></p> </li> <li> <p><a href="https://pdfs.semanticscholar.org/7a56/72796aeca8605b2e370d8a756a7a311fd171.pdf">A unified view of gradient-based attribution methods for Deep Neural Networks</a></p> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[A brief description on explanations methods in the computer vision literature.]]></summary></entry><entry><title type="html">Automatic Environment Generation to Generalize Agents</title><link href="https://modanesh.github.io/blog/Automatic-Environment-Generation-to-Generalize-Agents/" rel="alternate" type="text/html" title="Automatic Environment Generation to Generalize Agents"/><published>2019-05-16T00:00:00+00:00</published><updated>2019-05-16T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/Automatic-Environment-Generation-to-Generalize-Agents</id><content type="html" xml:base="https://modanesh.github.io/blog/Automatic-Environment-Generation-to-Generalize-Agents/"><![CDATA[<p>Done by: <b>Mohamad H. Danesh, Gaurav Dixit, Ali Raza</b></p> <p>In RL, an agent tries to learn the dynamics of an environment by trial and error by interacting with the environment. It is difficult for the agents to learn a general policy that applies across similar environments. Furthermore, they do not get reasonable performances on the same environments of varying difficulty level. To address it, here I try to introduce a new pipeline for generating environments with varying difficulty levels to improve the agents’ performances. Inspired by how humans learn difficult tasks, which basically starts with easy settings and slowly increases the difficulty of the settings, we designed a curriculum learning framework in which the agent first is trained on a simple environment. Then by gradually increasing the difficulty level of the environment, the agent will be fine-tuned on new environments. Finally, it will be able to have a good performance even on the most difficult environments.</p> <h3 id="related-works">Related works</h3> <p>In recent years, a lot of work have been done regarding motion transfer. In one of the proposed approaches, researchers use evolutionary algorithms to tackle this problem. In POET [1], first every environment is paired with an agent, which is controlled by a neural network, that learns how to navigate through the paired environment. Then they use evolutionary algorithms to come up with more advanced environments with the paired agent. In this way, the agent better learns how to gain more rewards, and thus performs better. One approach towards these problems is by changing the location of goal and start state in order to make the environment gradually harder to be solvable for the agent. In [2], the main approach is that by generating a set of feasible start states and then increasingly move it farther away from the goal, the agent learns better general policy eventually. Another approach is conventional coevolutionary algorithms [3]. They are usually divided between competitive coevolution, where agents are rewarded according to their performance against other individuals in a competition in the environment. This way, agents will eventually learn how to have a good performance.</p> <h3 id="datasets">Datasets</h3> <p>We used two different environments for our approach. First one is a modified version of the Bipedal Walker from the famous OpenAI Gym. The other environment, 2D-Bike, is a 2-dimensional physics based bike simulation, in which an agent must learn to drive on a rugged terrain. The terrain is procedurally generated using an input vector consisting of parameters that define its difficulty.</p> <p>In Bipedal Walker, we use the distance covered by the agent as a measurement of difficulty level of the environment. There are several features of the environment that affect the difficulty level. We use four features of the environment, which are random structures, amplitude, and pit range.</p> <p>The bipedal walker has a rather intricate walking mechanic which only allows for walking on terrains of limited difficulty. We needed an environment that was scalable linearly in terms of difficulty. We developed the 2D-Bike environment, with exactly this constraint in mind. In the base environment, an agent must learn to navigate a sufficiently smooth terrain as depicted bellow:</p> <p style="text-align:center;"> <img src="/assets/curriculum-rl/cur_learning_rl_fig_1.png?raw=true" style="height: 250px;text-align:center;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">An agent in the 2D-Bike environment traversing a noisy terrain.</figcaption> </p> <p>The reward is a weighted sum of the distance covered and the average speed. Difficultness is scaled up according to the parameters listed be- low:</p> <ol> <li> <p>Noise in terrain generation: This creates structures that are difficult to navigate.</p> </li> <li> <p>Gaps: An agent must learn to gather momentum before jumping over.</p> </li> <li> <p>Stumps: These break momentum and an agent must learn to use lean to overcome them.</p> </li> <li> <p>Reward dynamics: Generated environments are allowed to generate new reward dynamics that reward using just one wheel to complete the track, greater air time, extra rewards for tricks (flips and wheelies).</p> </li> </ol> <h3 id="framework">Framework</h3> <p>We carry out tests and compare three methods to create a learning curriculum. The first approach uses Generative Adversarial Networks (GANs) to learn to generate new environments. The second approach uses a Deep Neural Network to learn the difficulty level (between 0 and 1) given an input image of the environment. The training data is generated by running a trained agent on environments generated by random sampling from a uniform distribution over all parameters. The third approach uses a neuroevolution technique [4], NeuroEvolution of Augmenting Topologies (NEAT) [5], to evolve a generation of deep neural networks. The fitness for an environment in a generation is calculated based on average rewards obtained by a trained agent. Thus, environments where the agent did poorly have a higher fitness and will reproduce to create more such environments. Pruning and mutation allow for harder environments. The overall framework is shown bellow:</p> <p style="text-align:center;"> <img src="/assets/curriculum-rl/cur_learning_rl_fig_2.png?raw=true" style="height: 250px;text-align:center;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Our approach starts with creating random environments from the search-space, trains an agent on them to get their difficult levels, and cluster them according to their difficulty. We use GANs, DNNs, and NEATs to generate new environments of a given difficulty for curriculum learning.</figcaption> </p> <h3 id="gans">GANs</h3> <p>Features of the environment form an <b><i>n</i></b>-dimensional search space where each point corresponds to an environment with a certain difficulty level. We select 100 environments randomly from the search space, and train a vanilla agent on them to get their difficulty levels. We group these environment according to their difficulty as shown bellow:</p> <p style="text-align:center;"> <img src="/assets/curriculum-rl/cur_learning_rl_fig_3.png?raw=true" style="height: 250px;text-align:center;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Environments sorted and clustered according to their difficulty levels. When the difficulty level of an environment is less than 490, it will be categorized as an easy environment. If the difficulty level be between 490 and 630 it will be categorized as a medium environment. If it is more than 630, it will be known as a difficult environment.</figcaption> </p> <p>We use GANs to generate new environments corresponding to a certain difficulty level given the environments with the same difficulty level.</p> <h3 id="dnns">DNNs</h3> <p>The objective is to predict the difficulty of an environment based on the agent’s current policy and create a curriculum that helps the agent learn and bootstrap from easier environments. The data set is created by creating environments with parameters sampled from a uniform distribution and evaluating a trained agent on them. The deep neural network is then trained on this data to reduce the loss between the actual distance covered by the agent and the distance predicted by the network.</p> <h3 id="neuroevolution-of-augmenting-topologies-neat">Neuroevolution of Augmenting Topologies (NEAT)</h3> <p>NEAT has several key features that make it ideal for evolving a population of environments. They are:</p> <h4 id="complexification">Complexification</h4> <p>The networks in the first generation of the population are allowed to be extremely small (perceptrons are also allowed). They add layers, units and connections as they evolve with generations.</p> <h4 id="competing-conventions">Competing Conventions</h4> <p>In most evolutionary algorithms two individuals can encode similar behavior but still have very different genotype. This is competing conventions. When such networks are subject to crossover, the children network are likely to be worse than the parent network. NEAT solves this by keeping historical markings of new structural elements. When a new structural element is created (via structural mutation), it is assigned an innovation number. When two individuals are crossed over, their genotypes are aligned in such a way that the corresponding innovation numbers match and only the differing elements are exchanged.</p> <h4 id="speciation-and-fitness-sharing">Speciation and Fitness Sharing</h4> <p>Like some other newer neuroevolution techniques, NEAT also divides its population into several teams, knows as species. This subdivision is based on the dissimilarity of the individuals that is computed based on similar alignment of their genotypes as is used when doing crossover. Probability of crossing over individuals from different species is then much smaller than crossover inside species. By promoting the mating of more similar parent networks, the children are less likely to be worse than the parents because the parents were compatible.</p> <p>Within a species, the fitness is shared among the individuals. This protects and promotes individuals to mutations - when a mutation happens, the fitness would normally be low but because there is fitness sharing, the individual has time to optimize itself (the weights) to adapt to this new structural change. This indirectly will also promote diversity because the bigger the species, the more is the fitness shared and the less fit are the members of the species.</p> <p>The fitness in our experiments is a weighted sum of several factors. It is inversely proportional to the average performance (distance covered) by the agents. This implies that the better the agent gets at an environment, the lesser fit it is. To avoid generating environments that are too complex for the policy of the current agent, environments in which the agent is not able to perform above a certain threshold (20% distance covered), are also assigned a low fitness and effectively pruned from the next generation. This allows for a steady increase in difficulty such that the agent can learn harder environments and improve its performance and the environments can evolve every generation simultaneously to slightly increase the overall fitness of all the species of the population.</p> <h3 id="experimental-results">Experimental results</h3> <p>We use the 2D-Bike environment as the base environment for all three approaches. The initial terrain is an interpolated list of points (smooth). The agent is trained with PPO [6]. The learned policy can solve the environment (complete 100% distance). The agent uses 10 LIDAR sensors that scope out the terrain around the terrain (45 degrees in each direction – partial observability). The input to the agent neural network then is a vector with 10 real valued numbers (distances from LIDAR sensors), the angle of the chassis and the current velocity vector.</p> <p>The GANs use an initial set of randomly generated environment parameters (labelled with the pre-trained agent’s performance) to create new similar environments.</p> <p>The Deep neural network is a convolutional network that learns to map <b><i>64x64</i></b> frames of the environment (pre-processed to highlight the terrain driving line) to the corresponding agent performance (a value between 0 and 1 for the percent of distance that the current agent policy can complete).</p> <p>NEAT starts with a generation of 10 neural networks with no hidden layers. The topologies are allowed to evolve and create deeper networks with many hidden layers and units.</p> <p>We use the initial pre-trained agent’s policy as a baseline to compare against its performance when it trains for the entire curriculum.</p> <p>As can be observed from the figure bellow:</p> <p style="text-align:center;"> <img src="/assets/curriculum-rl/cur_learning_rl_fig_4.png?raw=true" style="height: 250px;text-align:center;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">Comparing % environment solved (distance covered) by a pre-trained agent for increasingly difficult environments for all the approaches used. The baseline is the performance of the pre-trained agent on these environments without any curriculum.</figcaption> </p> <p>a curriculum generated by our neural network and by NEAT significantly outperforms the baseline. The NEAT generated curriculum grows linearly in difficulty since every new generation of the environment is perturbed only slightly allowing for small incremental changes to the environments. The curriculum generated by the neural network does not generate a curriculum with linear difficulty since it creates the curriculum by randomly searching through the parameter space and will not explore the entire space. The GAN could not be trained to generate environments and completely fails in creating playable (and hence learnable) environments.</p> <p>We experimented with different architectures, activation functions, and optimization schemes. However, GANs could not lean the dynamics of the environments and failed to produce playable synthetic environments. After a few hundred iterations, discriminator loss converges to 0.6; whereas, generator loss keeps on increasing. We tested our implementation on the MNIST dataset, and as it is shown in Figure bellow:</p> <p style="text-align:center;"> <img src="/assets/curriculum-rl/cur_learning_rl_fig_5.png?raw=true" style="height: 250px;text-align:center;"/> <figcaption style="text-align:center;width: 70%;margin-left: auto;margin-right: auto;">A sample data distribution generated from noise after a few hundred iterations by our GAN.</figcaption> </p> <p>and we were able to produce reasonable synthetic samples which confirms the correctness of our GANs implementation. The above mentioned results are available <a href="https://drive.google.com/open?id=1j6lHJYKvICvzu_Twzh8D1skk8AwKiZVx">here</a>.</p> <h3 id="conclusion">Conclusion</h3> <p>In this work, we introduced a new method of learning a curriculum by generating environments of increasing difficulty, which allows agents to generalize learning and improves their performance even on the original base environments. Our experiments show how a curriculum can be generated effectively without having a clear understanding of how harder tasks should be broken down into easier sub tasks. Finally, we see that generating environments with evolutionary methods works much better than a deep neural network since evolution promotes small changes to the parameters of the environment ensuring a continuous and smooth curriculum. On the other hand, since the deep neural network uses random sampling to generate environment parameters, its effectiveness is effectively bound by the amount of search space it can explore.</p> <h3 id="references">References</h3> <p>[1] R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019.</p> <p>[2] S. Forestier, Y. Mollard, and P.-Y. Oudeyer. Intrinsically motivated goal exploration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190, 2017.</p> <p>[3] J. B. Pollack. Challenges in coevolutionary learning : Arms-race dynamics , open-endedness , and mediocre stable. 1998.</p> <p>[4] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning, 2019.</p> <p>[5] K. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies. Evol. omput., 10(2):99–127, June 2002.</p> <p>[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Using GANs and evolution algorithms to generate a curriculum for the RL agent.]]></summary></entry><entry><title type="html">RL Course by David Silver Notes</title><link href="https://modanesh.github.io/blog/RL-Course-by-David-Silver-Notes/" rel="alternate" type="text/html" title="RL Course by David Silver Notes"/><published>2018-12-14T00:00:00+00:00</published><updated>2018-12-14T00:00:00+00:00</updated><id>https://modanesh.github.io/blog/RL-Course-by%E2%80%93David-Silver-Notes</id><content type="html" xml:base="https://modanesh.github.io/blog/RL-Course-by-David-Silver-Notes/"><![CDATA[<h3 id="lecture-1-introduction-to-reinforcement-learning">Lecture 1: Introduction to Reinforcement Learning</h3> <ul> <li> <p>Planning: rules of the game are given, perfect model inside agent’s head, plan ahead to find optimal policy(look ahead search or tree search).</p> </li> <li> <p>In RL environment is unknown, in planning environment is known.</p> </li> <li> <p>Types of RL agents:</p> <ul> <li> <p>Policy based</p> </li> <li> <p>Value function based</p> </li> <li> <p>Actor critic(combines policy and value function)</p> </li> </ul> </li> <li> <p>Agent’s model is a representation of the environment in the agent’s head.</p> </li> <li> <p>Agent is our brain, is the algorithm we come up with.</p> </li> <li> <p>To get the maximum expected reward, risk is already included.</p> </li> <li> <p>Value function: goodness/badness of states, so can be used to select between actions.</p> </li> <li> <p>Fully observability: agent sees the environment state.</p> </li> <li> <p>Data are not iid.</p> </li> <li> <p>Reward is delayed.</p> </li> <li> <p>There is only a reward signal, no supervision.</p> </li> </ul> <p>Reinforcement learning is the science of decision making.</p> <h3 id="lecture-2-markov-decision-process">Lecture 2: Markov Decision Process</h3> <ul> <li> <p>Partial ordering over policies:</p> \[\pi' \geq \pi \Longleftarrow v_\pi' (s) \geq v_\pi (s), \forall s\] </li> <li> <p>Action-Value function Q is the same as value function, but also takes action as input:</p> \[Q_\pi (s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] = \mathbb{E}_\pi [R_{t+1} + \gamma * Q_\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a]\] </li> <li> <p>Policy is a distribution over actions given states: \(\pi (a | s) = P [ A_t = a | S_t = s]\)</p> <p>Policies are stationary: \(A_t \approx \pi (\cdot | S_t), \forall t &gt; 0\)</p> </li> <li> <p>The returns are random samples, the value function is an expectation over there random samples.</p> </li> <li> <p>Value function: \(v_\pi (s) = \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi [R_{t+1} + \gamma * v_\pi (S_{t+1}) | S_t = s]\)</p> <p>It also can be expressed using matrices: \(v_\pi = R_\pi + \gamma * \rho_\pi * v\)</p> </li> <li> <p>Total discounted reward from state \(t\): \(G_t = R_{t+1} + \gamma * R_{t+2} + \gamma^2 * R_{t+3} ... + \gamma^{T-1} * R_T\)</p> </li> <li> <p>Why having discount factor?</p> <ul> <li> <p>Uncertainty in the future.</p> </li> <li> <p>Mathematically convenient.</p> </li> <li> <p>Avoids cycles.</p> </li> <li> <p>Immediate rewards worth more that delayed rewards.</p> </li> </ul> </li> <li> <p>Reward function: \(R = \mathbb{E} [R_{t+1} | S_t = s, A_t = a]\)</p> </li> <li> <p>Sample episodes from a MDP: different iterations over different states in each episode.</p> </li> <li> <p>State transition probability: \(\rho_{ss'} = P [S_{t+1} = s' | S_t = s, A_t = a]\)</p> </li> <li> <p>Showing an ad on a website is actually a bandit problem.</p> </li> <li> <p>Almost all RL problems can be modeled using MDPs.</p> </li> </ul> <h3 id="lecture-3-planning-by-dynamic-programming">Lecture 3: Planning by Dynamic Programming</h3> <ul> <li> <p>Value Iteration:</p> <ul> <li> <p>Goal: find optimal policy \(\pi\)</p> </li> <li> <p>Start with an arbitrary value function: \(v_1\) and apply Bellman backup to get to \(v_2\) and finally \(v^*\).</p> </li> <li> <p>Unlike policy iteration, there is no explicit policy.</p> </li> <li> <p>Intermediate value functions may not correspond to any policy.</p> </li> <li> <p>Will converge.</p> </li> <li> <p>Intuition: start with final reward and work backwards.</p> </li> </ul> </li> <li> <p>Policy Iteration:</p> <ul> <li> <p>Given a policy \(\pi\) (any policy works):</p> </li> <li> <p>Evaluate the policy:</p> \[V_\pi (s) = \mathbb{E} [ R_{t+1} + \gamma * R_{t+2} + ... | S_t = s ]\] </li> <li> <p>Improve the policy: \(\pi' = greedy ( V_\pi )\)</p> </li> <li> <p>Will converge.</p> </li> </ul> </li> <li> <p>Policy Evaluation:</p> <ul> <li> <p>Start with an arbitrary value function: \(v_1\) and apply Bellman backup to get to \(v_2\) and finally \(v_\pi\).</p> </li> <li> <p>Use both synchronous and asynchronous backups.</p> </li> <li> <p>Will converge.</p> </li> <li> \[V^{k+1} = R^\pi + \gamma * P^\pi * V^k\] </li> </ul> </li> <li> <p>Input: \(MDP(S, A, P, R, \gamma)\) - Output: optimal value function \(V^*\) and optimal policy \(\pi^*\)</p> </li> <li> <p>In planning full knowledge of MDP is given. We want to solve the MPD, i.e. finding a policy.</p> </li> <li> <p>MDPs satisfy both DP properties.</p> </li> <li> <p>Dynamic programming applies to problems that have:</p> <ul> <li> <p>Optimal substructure.</p> </li> <li> <p>Overlapping subproblems.</p> </li> </ul> </li> <li> <p>Dynamic Programming: method for solving complex problems. Breaking them down into subproblems. Solve subproblems and combine solutions.</p> <ul> <li> <p>Dynamic: sequential component to the problem.</p> </li> <li> <p>Programming: a mapping, e.g. a policy.</p> </li> </ul> </li> </ul> <h3 id="lecture-4-model-free-prediction">Lecture 4: Model-Free Prediction</h3> <ul> <li> <p>Bootstrapping: update involves estimated rewards.</p> </li> <li> <p>Monte-Carlo Learning:</p> <ul> <li> <p>In a nutshell, goes all the way through the trajectory and estimates the value by looking at the sample returns.</p> </li> <li> <p>Useful only for episodic(terminating) settings, and applies once an episode is complete. No bootstrapping.</p> </li> <li> <p>High variance, zero bias.</p> </li> <li> <p>Not exploit Markov property. In partially observed environments, MC is a better choice.</p> </li> </ul> </li> <li> <p>Temporal-Difference Learning:</p> <ul> <li> <p>Looks one step ahead, and then estimates the return.</p> </li> <li> <p>Uses bootstrapping to learn, so learns from incomplete episodes. Does online learning.</p> </li> <li> <p>Low variance, some bias.</p> </li> <li> <p>Exploits Markov property. Implicitly building MDP structure and solving for that MDP structure(refer to the AB example). More efficient in Markov environments.</p> </li> <li> <p>\(TD(\lambda)\): looks into \(\lambda\) steps of the future to update. Also \(\lambda\) can be defined as averaging over all n-step returns.</p> </li> </ul> </li> </ul> <h3 id="lecture-5-model-free-control">Lecture 5: Model Free Control</h3> <ul> <li> <p>On-policy Learning:</p> <ul> <li> <p>Learn about policy \(\pi\) from experience sampled from \(\pi\).</p> </li> <li> <p>\(\epsilon\)-greedy Exploration:</p> <ul> <li> <p>With probability = \(1 - \epsilon\) choose the greedy action. With probability = \(\epsilon\) choose a random action.</p> </li> <li> <p>Guarantees to have improvements.</p> </li> </ul> </li> <li> <p>Greedy in the Limit of Infinite Exploration(GLIE):</p> <ul> <li>All state-action pairs are explored infinitely many times.</li> </ul> </li> <li> <p>Policy converges to the greedy policy.</p> </li> <li> <p>SARSA:</p> <ul> <li> \[Q(S, A) \leftarrow Q(S, A) + \alpha * (R + \gamma * Q(S', A') - Q(S, A))\] </li> <li>Converges to the optimal action-value function, under some conditions.</li> </ul> </li> </ul> </li> <li> <p>Off-policy Learning:</p> <ul> <li> <p>Learn about policy \(\pi\) from experience sampled from \(\mu\).</p> </li> <li> <p>Evaluate target policy \(\pi (a | s)\) to compute \(v_\pi (s)\) or \(q_\pi (s, a)\) while following the behavior policy \(\mu (a | s)\).</p> </li> <li> <p>Can learn from observing human behaviors or other agents.</p> </li> <li> <p>Learn optimal policy while following exploratory policy.</p> </li> <li> <p>Monte-carlo learning doesn’t work in off-policy learning, because of really high variance. Over many steps, the target policy and the behavior policy never match enough to be useful.</p> </li> <li> <p>Using TD leaning, it works best. Because policies only need to be similar over one step.</p> </li> <li> <p>Q-Learning:</p> <ul> <li> <p>\(S'\) is gathered using the behavior policy.</p> </li> <li> \[Q(S, A) \leftarrow Q(S, A) + \alpha * (R + \gamma * Q(S, A') - Q(S, A))\] </li> <li>Converges to the optimal action-value function.</li> </ul> </li> </ul> </li> </ul> <h3 id="lecture-6-value-function-approximation">Lecture 6: Value Function Approximation</h3> <ul> <li> <p>Large scale RL problems because of the huge state spaces become non-tabular.</p> </li> <li> <p>So far, we had \(V(s)\) or \(Q(s, a)\). But with large scale problems, calculating them takes too much memory and time. Solution: estimate the value function with function approximation:</p> <p>\(v' (s, w) ≈ v_\pi (s)\) or \(q' (s, a, w) ≈ q_\pi (s, a)\)</p> <p>Generalize from seen states to unseen ones. Update parameter \(w\) using MC or TD learning.</p> </li> <li> <p>Incremental Methods:</p> <ul> <li> <p>Table lookup is a special case of linear value function approximation.</p> </li> <li> <p>Target value function for MC is the return \(G_t\) and for TD is the \lambda-return \(G^\lambda _t\)</p> </li> <li> <p>Gradient descent is simple and appealing.</p> </li> </ul> </li> <li> <p>Batch Methods:</p> <ul> <li> <p>GD is not sample efficient.</p> </li> <li> <p>Least squares algorithms find parameter vector \(w\) minimizing sum squared error.</p> </li> <li> <p>Example: experience replay in DQN.</p> </li> </ul> </li> </ul> <h3 id="lecture-7-policy-gradient-methods">Lecture 7: Policy Gradient Methods</h3> <ul> <li> <p>Policy gradient methods optimize the policy directly, instead of the value function.</p> </li> <li> <p>Parametrize the policy: \(\pi_\theta (s, a) = P [a | s, \theta]\)</p> </li> <li> <p>Point of using policy gradient methods is to being able to scale.</p> </li> <li> <p>Nash’s equilibrium is the game theoretic notion of optimality.</p> </li> <li> <p>Finite Difference Policy Gradient:</p> <ul> <li> <p>For each dimension in \(\theta\) parameters, perturbing \(\theta\) by small amount \(\epsilon\). (look at the formula)</p> </li> <li> <p>Uses \(n\) evaluations to compute policy gradient in \(n\) dimension.</p> </li> <li> <p>Simple, noisy, inefficient.</p> </li> <li> <p>Works for arbitrary policies.</p> </li> </ul> </li> <li> <p>Monte-Carlo Policy Gradient:</p> <ul> <li> <p>Score function is \(∇_\theta log \pi_\theta (s, a)\)</p> </li> <li> <p>In continuous action spaces, Gaussian policy should be used.</p> </li> <li> <p>REINFORCE: update parameters by stochastic gradient ascent.</p> </li> <li> <p>Has high variance.</p> </li> </ul> </li> <li> <p>Actor-Critic Policy Gradient:</p> <ul> <li> <p>Use a critic to estimate the action-value function.</p> </li> <li> <p>Critic: updates action-value function parameters \(w\).</p> </li> <li> <p>Actor: updates policy parameters \(\theta\) in direction suggested by critic.</p> </li> <li> <p>Approximating the policy gradient introduces bias.</p> </li> <li> <p>Using baseline function \(B\) to reduce variance. Value function \(V\) could be a good baseline.</p> </li> <li> <p>Using advantage function to reduce the variance: \(Q_\pi (s, a) - V_\pi (s)\)</p> </li> <li> <p>Critic estimates the advantage function.</p> </li> </ul> </li> </ul> <h3 id="lecture-8-integrating-learning-and-planning">Lecture 8: Integrating Learning and Planning</h3> <ul> <li> <p>So far, the course covered model-free RL.</p> </li> <li> <p>Last lecture: learn policy directly from experience; Previous lectures: learn value function directly from experience; This lecture: learn model directly from experience.</p> </li> <li> <p>Use <b>planning</b> to construct a value function or policy.</p> </li> <li>Model-Based RL: <ul> <li>Plan value function (and/or policy) from model.</li> <li>Advantages: <ul> <li>Can learn model by supervised learning methods.</li> <li>Car reason about model uncertainty.</li> </ul> </li> <li>Disadvantages: <ul> <li>First learn a model, then construct a value function: two sources of approximation error.</li> </ul> </li> <li>Model is a parametrized representation of the MDP, i.e. representing state transitions and rewards.</li> <li>Learning \(s, a \rightarrow r\) is a regression problem.</li> <li>Learning \(s, a \rightarrow s'\) is a density estimation problem.</li> <li>Planning the MDP = Solving the MDP, i.e. figure out what’s the best thing to do.</li> <li>Planning algorithms: value iteration, policy iteration, tree search, …</li> </ul> </li> <li>Integrated Architectures: <ul> <li>Put together the best parts of model-free and model-based RL.</li> <li>Two sources of experience: real: sampled from environment (true MDP); simulated: sampled from model (approximate MDP).</li> <li>Dyna: <ul> <li>Learn a model from real experience.</li> <li>Learn and plan value function from real and simulated experience.</li> </ul> </li> </ul> </li> <li>Simulation-Based Search: <ul> <li>Forward search: select best action by lookahead. Doesn’t explore the entire state space. Builds a search tree with current s as the root. Uses a model of MDP to look ahead. Doesn’t solve the whole MDP, just sub-MDP starting from now.</li> <li>Simulation-based search is a forward search paradigm using sample-based planning.</li> <li>Simulates episodes of experience from now with the model.</li> <li>Applies model-free RL to simulated episodes.</li> </ul> </li> </ul> <h3 id="lecture-9-exploration-and-exploitation">Lecture 9: Exploration and Exploitation</h3> <ul> <li> <p>Three methods of exploration and exploitation:</p> <ul> <li> <p>Random exploration: e.g. \(\epsilon\)-greedy</p> </li> <li> <p>Optimism in the face of uncertainty: estimate uncertainty on value, prefer to explore states/actions with highest uncertainty.</p> </li> <li> <p>Information state space: consider agent’s information as part of its state, lookahead to see how information helps reward.</p> </li> </ul> </li> <li> <p>Types of exploration:</p> <ul> <li> <p>State-action exploration: e.g. pick different action \(A\) each time in state \(S\).</p> </li> <li> <p>Parameter exploration: parameterize policy \(\pi (A | S , u)\), e.g. pick different parameters and try for a while.</p> </li> </ul> </li> <li> <p>Multi-Armed Bandit:</p> <ul> <li> <p>One-step decision making problem.</p> </li> <li> <p>No state space, no transition function.</p> </li> <li> <p>\(R (r) = P [ R = r | A = a ]\) is an unknown probability distribution over rewards.</p> </li> <li> <p>Regret is a function of gaps and the counts.</p> </li> <li> <p>\epsilon-greedy has linear total regret. To resolve this, pick a decay schedule for \(\epsilon_1, \epsilon_2\), … . However, it’s not possible to use because \(V^*\) is needed to calculate the gaps.</p> </li> <li> <p>One can transform multi-armed bandit problem into a sequential decision making problem.</p> </li> <li> <p>Define an MDP over information states.</p> </li> <li> <p>At each step, information state \(S'\) summarizes all information accumulated so far.</p> </li> <li> <p>MDP can then be solved by RL.</p> </li> </ul> </li> <li> <p>Contextual Bandits:</p> <ul> <li> <p>\(S = P [ S ]\) is an unknown distribution over states(or “contexts”).</p> </li> <li> <p>\(R (r) = P [ R = r | S = s, A = a ]\) is an unknown probability distribution over rewards.</p> </li> </ul> </li> <li> <p>MDPs:</p> <ul> <li> <p>For unknown or poorly estimated states, replace reward function with \(r_max\). Means to be very optimistic about uncertain states.</p> </li> <li> <p>Augmented MDP: includes information state so that \(S' = (S , I)\).</p> </li> </ul> </li> </ul> <h3 id="lecture-10-classic-games">Lecture 10: Classic Games</h3> <ul> <li> <p>Nash equilibrium is a joint policy for all players, so a way for others to pick actions such that every single player is playing the best response to all other players, i.e. no player would choose from Nash.</p> </li> <li> <p>Single-Agent and Self-Play Reinforcement Learning: Nash equilibrium is fixed-point of self-play RL. Experience is generated by playing games between agents. Each agent learns best response to other players. One player’s policy determines another player’s environment.</p> </li> <li> <p>Two-Player Zero-Sum Games: A two-player game has two (alternating) players: \(R_1 + R_2 = 0\).</p> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[After being excited about RL for more than a year, I should have a concise and satisfying answer to the question, 'What is reinforcement learning?' Here it is gathered briefly.]]></summary></entry></feed>